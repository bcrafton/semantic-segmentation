<!DOCTYPE html>
<html data-rh="lang" lang="en"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8"><script async="" src="Review%20of%20Deep%20Learning%20Algorithms%20for%20Image%20Semantic%20Segmentation_files/branch-latest.js"></script><script async="" src="Review%20of%20Deep%20Learning%20Algorithms%20for%20Image%20Semantic%20Segmentation_files/analytics.js"></script><script>!function(c,f){var t,o,i,e=[],r={passive:!0,capture:!0},n=new Date,a="pointerup",u="pointercancel";function p(n,e){t||(t=e,o=n,i=new Date,w(f),s())}function s(){0<=o&&o<i-n&&(e.forEach(function(n){n(o,t)}),e=[])}function l(n){if(n.cancelable){var e=(1e12<n.timeStamp?new Date:performance.now())-n.timeStamp;"pointerdown"==n.type?function(n,e){function t(){p(n,e),i()}function o(){i()}function i(){f(a,t,r),f(u,o,r)}c(a,t,r),c(u,o,r)}(e,n):p(e,n)}}function w(e){["click","mousedown","keydown","touchstart","pointerdown"].forEach(function(n){e(n,l,r)})}w(c),self.perfMetrics=self.perfMetrics||{},self.perfMetrics.onFirstInputDelay=function(n){e.push(n),s()}}(addEventListener,removeEventListener)</script><title>Review of Deep Learning Algorithms for Image Semantic Segmentation</title><meta data-rh="true" charset="utf-8"><meta data-rh="true" name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1"><meta data-rh="true" name="theme-color" content="#000000"><meta data-rh="true" name="twitter:app:name:iphone" content="Medium"><meta data-rh="true" name="twitter:app:id:iphone" content="828256236"><meta data-rh="true" property="al:ios:app_name" content="Medium"><meta data-rh="true" property="al:ios:app_store_id" content="828256236"><meta data-rh="true" property="al:android:package" content="com.medium.reader"><meta data-rh="true" property="fb:app_id" content="542599432471018"><meta data-rh="true" property="og:site_name" content="Medium"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2018-12-11T11:21:55.064Z"><meta data-rh="true" name="title" content="Review of Deep Learning Algorithms for Image Semantic Segmentation"><meta data-rh="true" property="og:title" content="Review of Deep Learning Algorithms for Image Semantic Segmentation"><meta data-rh="true" property="twitter:title" content="Review of Deep Learning Algorithms for Image Semantic Segmentation"><meta data-rh="true" name="twitter:site" content="@Medium"><meta data-rh="true" name="twitter:app:url:iphone" content="medium://p/509a600f7b57"><meta data-rh="true" property="al:android:url" content="medium://p/509a600f7b57"><meta data-rh="true" property="al:ios:url" content="medium://p/509a600f7b57"><meta data-rh="true" name="apple-itunes-app" content="app-id=828256236,app-argument=medium://p/509a600f7b57"><meta data-rh="true" property="al:android:app_name" content="Medium"><meta data-rh="true" name="description" content="Deep learning algorithms have solved several computer vision tasks with an increasing level of difficulty. In my previous blog posts, I have detailled the well kwown ones: image classification and…"><meta data-rh="true" property="og:description" content="Deep learning algorithms have solved several computer vision tasks with an increasing level of difficulty. In my previous blog posts, I…"><meta data-rh="true" property="twitter:description" content="Deep learning algorithms have solved several computer vision tasks with an increasing level of difficulty. In my previous blog posts, I…"><meta data-rh="true" property="og:url" content="https://medium.com/@arthur_ouaknine/review-of-deep-learning-algorithms-for-image-semantic-segmentation-509a600f7b57"><meta data-rh="true" property="al:web:url" content="https://medium.com/@arthur_ouaknine/review-of-deep-learning-algorithms-for-image-semantic-segmentation-509a600f7b57"><meta data-rh="true" property="og:image" content="https://miro.medium.com/max/1200/1*O6mEzgZITQA80xHI7UJrUQ.png"><meta data-rh="true" name="twitter:image:src" content="https://miro.medium.com/max/1200/1*O6mEzgZITQA80xHI7UJrUQ.png"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="article:author" content="/@arthur_ouaknine"><meta data-rh="true" name="author" content="Arthur Ouaknine"><meta data-rh="true" name="robots" content="index,follow"><meta data-rh="true" name="referrer" content="unsafe-url"><meta data-rh="true" name="twitter:label1" value="Reading time"><meta data-rh="true" name="twitter:data1" value="18 min read"><link data-rh="true" rel="publisher" href="https://plus.google.com/103654360130207659246"><link data-rh="true" rel="search" type="application/opensearchdescription+xml" title="Medium" href="https://medium.com/osd.xml"><link data-rh="true" rel="apple-touch-icon" sizes="152x152" href="https://cdn-images-1.medium.com/fit/c/152/152/1*8I-HPL0bfoIzGied-dzOvA.png"><link data-rh="true" rel="apple-touch-icon" sizes="120x120" href="https://cdn-images-1.medium.com/fit/c/120/120/1*8I-HPL0bfoIzGied-dzOvA.png"><link data-rh="true" rel="apple-touch-icon" sizes="76x76" href="https://cdn-images-1.medium.com/fit/c/76/76/1*8I-HPL0bfoIzGied-dzOvA.png"><link data-rh="true" rel="apple-touch-icon" sizes="60x60" href="https://cdn-images-1.medium.com/fit/c/60/60/1*8I-HPL0bfoIzGied-dzOvA.png"><link data-rh="true" rel="mask-icon" href="https://cdn-static-1.medium.com/_/fp/icons/monogram-mask.KPLCSFEZviQN0jQ7veN2RQ.svg" color="#171717"><link data-rh="true" rel="icon" href="https://cdn-static-1.medium.com/_/fp/icons/favicon-rebrand-medium.3Y6xpZ-0FSdWDnPM3hSBIA.ico"><link data-rh="true" id="glyph_link" rel="stylesheet" type="text/css" href="Review%20of%20Deep%20Learning%20Algorithms%20for%20Image%20Semantic%20Segmentation_files/m2.css"><link data-rh="true" rel="author" href="https://medium.com/@arthur_ouaknine"><link data-rh="true" rel="canonical" href="https://medium.com/@arthur_ouaknine/review-of-deep-learning-algorithms-for-image-semantic-segmentation-509a600f7b57"><script data-rh="true">(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-24232453-2', 'auto');
ga('send', 'pageview');</script><style type="text/css" data-fela-rehydration="359" data-fela-type="STATIC">html{box-sizing:border-box}*, *:before, *:after{box-sizing:inherit}body{margin:0;padding:0;text-rendering:optimizeLegibility;-webkit-font-smoothing:antialiased;color:rgba(0,0,0,0.8);position:relative;min-height:100vh}h1, h2, h3, h4, h5, h6, dl, dd, ol, ul, menu, figure, blockquote, p, pre, form{margin:0}menu, ol, ul{padding:0;list-style:none;list-style-image:none}main{display:block}a{color:inherit;text-decoration:none}a, button, input{-webkit-tap-highlight-color:transparent}img, svg{vertical-align:middle}button{background:transparent;overflow:visible}button, input, optgroup, select, textarea{margin:0}</style><style type="text/css" data-fela-rehydration="359" data-fela-type="KEYFRAME">@-webkit-keyframes k1{0%{transform:scale(1);opacity:1}70%{transform:scale(1.4);opacity:0}100%{opacity:0}}@-moz-keyframes k1{0%{transform:scale(1);opacity:1}70%{transform:scale(1.4);opacity:0}100%{opacity:0}}@keyframes k1{0%{transform:scale(1);opacity:1}70%{transform:scale(1.4);opacity:0}100%{opacity:0}}</style><style type="text/css" data-fela-rehydration="359" data-fela-type="RULE">.a{font-family:medium-content-sans-serif-font, -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Open Sans", "Helvetica Neue", sans-serif}.b{font-weight:400}.c{background-color:rgba(255, 255, 255, 1)}.n{display:block}.o{position:fixed}.p{top:0}.q{left:0}.r{right:0}.s{z-index:500}.t{box-shadow:0 4px 12px 0 rgba(0, 0, 0, 0.05)}.u{transition:transform 300ms ease}.v{will-change:transform}.w{padding-left:24px}.x{padding-right:24px}.y{margin-left:auto}.z{margin-right:auto}.ab{width:100%}.ac{max-width:1080px}.ae{box-sizing:border-box}.af{height:65px}.ag{display:flex}.ah{align-items:center}.ak{flex:1 0 auto}.al{display:none}.am{fill:rgba(0, 0, 0, 0.84)}.ap{flex:0 0 auto}.aq{color:inherit}.ar{fill:inherit}.as{font-size:inherit}.at{border:inherit}.au{font-family:inherit}.av{letter-spacing:inherit}.aw{font-weight:inherit}.ax{padding:0}.ay{margin:0}.az:hover{cursor:pointer}.ba:hover{color:rgba(0, 0, 0, 0.9)}.bb:hover{fill:rgba(0, 0, 0, 0.9)}.bc:focus{outline:none}.bd:disabled{cursor:default}.be:disabled{color:rgba(0, 0, 0, 0.54)}.bf:disabled{fill:rgba(0, 0, 0, 0.54)}.bg{font-family:medium-content-sans-serif-font, "Lucida Grande", "Lucida Sans Unicode", "Lucida Sans", Geneva, Arial, sans-serif}.bh{font-style:normal}.bi{line-height:20px}.bj{font-size:15.8px}.bk{letter-spacing:0px}.bl{color:rgba(0, 0, 0, 0.54)}.bm{fill:rgba(0, 0, 0, 0.54)}.bn{margin-left:16px}.bo{color:rgba(2, 158, 116, 1)}.bp{fill:rgba(3, 168, 124, 1)}.bq:hover{color:rgba(1, 143, 105, 1)}.br:hover{fill:rgba(2, 158, 116, 1)}.bs:disabled{color:rgba(3, 168, 124, 0.5)}.bt:disabled{fill:rgba(3, 168, 124, 0.5)}.bu{padding:8px 16px}.bv{background:0}.bw{border-color:rgba(3, 168, 124, 1)}.bx:hover{border-color:rgba(2, 158, 116, 1)}.by{border-radius:4px}.bz{border-width:1px}.ca{border-style:solid}.cb{display:inline-block}.cc{text-decoration:none}.cd{margin-bottom:0px}.cf{padding-bottom:1px}.cg{margin-top:40px}.ch{max-width:728px}.ci{opacity:0}.cj{pointer-events:none}.ck{will-change:opacity}.cl{transition:opacity 200ms}.cm{width:131px}.cn{left:50%}.co{transform:translateX(-516px)}.cp{top:calc(65px + 54px + 40px)}.cq{flex-direction:column}.cr{padding-top:28px}.cs{margin-bottom:19px}.ct{margin-left:-5px}.cu{margin-right:5px}.cv{position:relative}.cw{outline:0}.cx{border:0}.cy{user-select:none}.cz{cursor:pointer}.da> svg{pointer-events:none}.db:active{border-style:none}.dc{fill:rgba(0, 0, 0, 0.76)}.dd:focus{fill:rgba(0, 0, 0, 0.9)}.de{margin-top:5px}.df button{text-align:left}.dg{font-weight:300}.dh{font-size:16px}.di{clear:both}.dj{justify-content:center}.dk{list-style-type:none}.dl{margin-right:8px}.dm{margin-bottom:8px}.dn{border-radius:3px}.do{padding:5px 10px}.dp{background:rgba(0, 0, 0, 0.05)}.dq{line-height:22px}.dr{font-size:15px}.ds{justify-content:space-between}.dt{margin-top:15px}.du{margin-right:16px}.dv{border:1px solid rgba(0, 0, 0, 0.1)}.dw{border-radius:50%}.dx{height:60px}.dy{transition:border-color 150ms ease}.dz{width:60px}.ea:hover{border-color:rgba(0, 0, 0, 0.84)}.eb::before{background:
      radial-gradient(circle, rgba(0, 0, 0, 0.84) 60%, transparent 70%)
    }.ec::before{border-radius:50%}.ed::before{content:""}.ee::before{display:block}.ef::before{z-index:0}.eg::before{left:0}.eh::before{height:100%}.ei::before{position:absolute}.ej::before{top:0}.ek::before{width:100%}.el:hover::before{animation:k1 2000ms infinite cubic-bezier(.1,.12,.25,1)}.em:active{border-style:solid}.en{background:rgba(255, 255, 255, 1)}.eo{z-index:2}.ep{height:100%}.eq{position:absolute}.er{color:rgba(0, 0, 0, 0.84)}.es{padding-right:8px}.et{margin-top:25px}.eu{margin-bottom:25px}.ev{padding-top:32px}.ew{border-top:solid 1px rgba(0, 0, 0, 0.1)}.ex{margin-bottom:32px}.ey{min-height:80px}.fd{height:80px}.fe{width:80px}.ff{padding-left:102px}.fh{text-transform:uppercase}.fi{letter-spacing:0.05em}.fj{margin-bottom:6px}.fk{font-weight:600}.fl{font-size:28px}.fm{line-height:36px}.fn{padding:4px 12px}.fo{border-color:rgba(0, 0, 0, 0.54)}.fp:hover{color:rgba(0, 0, 0, 0.97)}.fq:hover{fill:rgba(0, 0, 0, 0.97)}.fr:disabled{fill:rgba(0, 0, 0, 0.76)}.fs:disabled{border-color:rgba(0, 0, 0, 0.2)}.ft:disabled{cursor:inherit}.fu:disabled:hover{color:rgba(0, 0, 0, 0.54)}.fv:disabled:hover{fill:rgba(0, 0, 0, 0.76)}.fw:disabled:hover{border-color:rgba(0, 0, 0, 0.2)}.fx{max-width:555px}.fy{max-width:450px}.fz{font-size:18px}.ga{line-height:24px}.gc{padding-top:25px}.gd{border-top:1px solid rgba(0, 0, 0, 0.1)}.ge{padding:20px}.gf{border:1px solid rgba(3, 168, 124, 1)}.gg{text-align:center}.gh{margin-top:64px}.gi{background-color:rgba(0, 0, 0, 0.02)}.gj{top:calc(100vh + 100px)}.gk{bottom:calc(100vh + 100px)}.gl{width:10px}.gm{word-break:break-word}.gn{word-wrap:break-word}.go:after{display:block}.gp:after{content:""}.gq:after{clear:both}.gr{margin:0 auto}.gs{line-height:1.23}.gt{letter-spacing:0}.gu{font-family:medium-content-title-font, Georgia, Cambria, "Times New Roman", Times, serif}.gv{font-size:40px}.hb{margin-bottom:-0.27em}.hc{line-height:48px}.hd{margin-top:32px}.he{height:48px}.hf{width:48px}.hg{margin-left:12px}.hh{margin-bottom:2px}.hj{overflow:hidden}.hk{max-height:20px}.hl{text-overflow:ellipsis}.hm{display:-webkit-box}.hn{-webkit-line-clamp:1}.ho{-webkit-box-orient:vertical}.hp:hover{text-decoration:underline}.hq{margin-left:8px}.hr{padding:0px 8px}.hs{line-height:18px}.hy{max-width:427px}.hz{transition:opacity 100ms 400ms}.ia{transform:translateZ(0)}.ib{margin:auto}.ic{background-color:rgba(0, 0, 0, 0.05)}.id{padding-bottom:109.13348946135832%}.ie{filter:blur(20px)}.if{transform:scale(1.1)}.ig{line-height:1.4}.ih{margin-top:10px}.ij{background-repeat:repeat-x}.ik{background-image:linear-gradient(to right,rgba(0, 0, 0, 0.84) 100%,rgba(0, 0, 0, 0.84) 0);background-image:url('data:image/svg+xml;utf8,<svg preserveAspectRatio="none" viewBox="0 0 1 1" xmlns="http://www.w3.org/2000/svg"><line x1="0" y1="0" x2="1" y2="1" stroke="rgba(0, 0, 0, 0.84)" /></svg>')}.il{background-size:1px 1px}.im{background-position:0 1.05em;background-position:0 calc(1em + 1px)}.in{line-height:1.58}.io{letter-spacing:-0.004em}.ip{font-family:medium-content-serif-font, Georgia, Cambria, "Times New Roman", Times, serif}.ja{margin-bottom:-0.46em}.jb{font-style:italic}.jc{line-height:1.12}.jd{letter-spacing:-0.022em}.jo{margin-bottom:-0.28em}.jp{line-height:1.18}.ka{margin-bottom:-0.31em}.kg{max-width:498px}.kh{padding-bottom:131.52610441767067%}.ki{max-width:1434px}.kj{padding-bottom:47.140864714086476%}.kk{max-width:755px}.kl{padding-bottom:20.264900662251655%}.km{max-width:1179px}.kn{padding-bottom:29.262086513994912%}.kt{font-weight:700}.ku{max-width:456px}.kv{padding-bottom:50.43859649122807%}.kw{max-width:683px}.kx{padding-bottom:24.743777452415813%}.ky{max-width:426px}.kz{padding-bottom:66.4319248826291%}.la{max-width:872px}.lb{padding-bottom:25.573394495412842%}.lc{max-width:580px}.ld{padding-bottom:66.72413793103446%}.le{max-width:269px}.lf{padding-bottom:84.38661710037174%}.lg{max-width:428px}.lh{padding-bottom:58.41121495327103%}.li{max-width:874px}.lj{padding-bottom:29.519450800915333%}.lk{max-width:432px}.ll{padding-bottom:45.833333333333336%}.lm{max-width:552px}.ln{padding-bottom:63.949275362318836%}.lo{max-width:612px}.lp{padding-bottom:50.98039215686275%}.lq{max-width:866px}.lr{padding-bottom:46.76674364896074%}.ls{max-width:796px}.lt{padding-bottom:36.05527638190954%}.lu{max-width:795px}.lv{padding-bottom:25.91194968553459%}.lw{max-width:670px}.lx{padding-bottom:22.23880597014925%}.ly{max-width:718px}.lz{padding-bottom:51.532033426183844%}.ma{max-width:264px}.mb{padding-bottom:59.09090909090909%}.mc{max-width:403px}.md{padding-bottom:45.40942928039702%}.me{max-width:390px}.mf{padding-bottom:51.7948717948718%}.mg{max-width:767px}.mh{padding-bottom:33.89830508474576%}.mi{max-width:419px}.mj{padding-bottom:35.56085918854416%}.mk{max-width:850px}.ml{padding-bottom:28.941176470588236%}.mm{max-width:968px}.mn{padding-bottom:55.68181818181818%}.mo{font-family:medium-content-slab-serif-font, Georgia, Cambria, "Times New Roman", Times, serif}.mp{border:none}.mq{margin-top:30px}.mr:before{content:"..."}.ms:before{letter-spacing:0.6em}.mt:before{text-indent:0.6em}.mu:before{font-style:italic}.mv:before{line-height:1.4}</style><style type="text/css" data-fela-rehydration="359" data-fela-type="RULE" media="all and (max-width: 1256px)">.d{display:none}</style><style type="text/css" data-fela-rehydration="359" data-fela-type="RULE" media="all and (max-width: 1080px)">.e{display:none}.ii{text-align:center}</style><style type="text/css" data-fela-rehydration="359" data-fela-type="RULE" media="all and (max-width: 904px)">.f{display:none}</style><style type="text/css" data-fela-rehydration="359" data-fela-type="RULE" media="all and (max-width: 728px)">.g{display:none}.ai{height:56px}.aj{display:flex}.an{margin-left:-5px}.ao{display:block}.ce{margin-bottom:0px}.ez{margin-bottom:24px}.fa{align-items:center}.fb{width:102px}.fc{position:relative}.fg{padding-left:0}.gb{margin-top:24px}</style><style type="text/css" data-fela-rehydration="359" data-fela-type="RULE" media="all and (max-width: 552px)">.h{display:none}.hi{margin-bottom:0px}</style><style type="text/css" data-fela-rehydration="359" data-fela-type="RULE" media="all and (min-width: 1080px)">.i{display:none}.ha{margin-top:0.78em}.hx{margin-top:56px}.iy{font-size:21px}.iz{margin-top:2em}.jm{font-size:34px}.jn{margin-top:1.95em}.jy{font-size:26px}.jz{margin-top:1.72em}.kf{margin-top:0.86em}.ks{margin-top:1.25em}</style><style type="text/css" data-fela-rehydration="359" data-fela-type="RULE" media="all and (min-width: 904px) and (max-width: 1079.98px)">.j{display:none}.gz{margin-top:0.78em}.hw{margin-top:56px}.iw{font-size:21px}.ix{margin-top:2em}.jk{font-size:34px}.jl{margin-top:1.95em}.jw{font-size:26px}.jx{margin-top:1.72em}.ke{margin-top:0.86em}.kr{margin-top:1.25em}</style><style type="text/css" data-fela-rehydration="359" data-fela-type="RULE" media="all and (min-width: 728px) and (max-width: 903.98px)">.k{display:none}.gy{margin-top:0.78em}.hv{margin-top:56px}.iu{font-size:21px}.iv{margin-top:2em}.ji{font-size:34px}.jj{margin-top:1.95em}.ju{font-size:26px}.jv{margin-top:1.72em}.kd{margin-top:0.86em}.kq{margin-top:1.25em}</style><style type="text/css" data-fela-rehydration="359" data-fela-type="RULE" media="all and (min-width: 552px) and (max-width: 727.98px)">.l{display:none}.gx{margin-top:0.39em}.hu{margin-top:40px}.is{font-size:18px}.it{margin-top:1.56em}.jg{font-size:30px}.jh{margin-top:1.2em}.js{font-size:24px}.jt{margin-top:1.23em}.kc{margin-top:0.67em}.kp{margin-top:0.93em}</style><style type="text/css" data-fela-rehydration="359" data-fela-type="RULE" media="all and (max-width: 551.98px)">.m{display:none}.gw{margin-top:0.39em}.ht{margin-top:40px}.iq{font-size:18px}.ir{margin-top:1.56em}.je{font-size:30px}.jf{margin-top:1.2em}.jq{font-size:24px}.jr{margin-top:1.23em}.kb{margin-top:0.67em}.ko{margin-top:0.93em}</style><script charset="utf-8" src="Review%20of%20Deep%20Learning%20Algorithms%20for%20Image%20Semantic%20Segmentation_files/vendorstracing.js"></script><script charset="utf-8" src="Review%20of%20Deep%20Learning%20Algorithms%20for%20Image%20Semantic%20Segmentation_files/tracing.js"></script><script type="application/ld+json" data-rh="true">{"@context":"http:\u002F\u002Fschema.org","@type":"NewsArticle","image":{"@type":"ImageObject","width":54,"height":60,"url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F59\u002F1*O6mEzgZITQA80xHI7UJrUQ.png"},"thumbnailUrl":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F59\u002F1*O6mEzgZITQA80xHI7UJrUQ.png","url":"https:\u002F\u002Fmedium.com\u002F@arthur_ouaknine\u002Freview-of-deep-learning-algorithms-for-image-semantic-segmentation-509a600f7b57","dateCreated":"2018-12-11T11:21:55.064Z","datePublished":"2018-12-11T11:21:55.064Z","dateModified":"2018-12-11T11:21:55.064Z","headline":"Review of Deep Learning Algorithms for Image Semantic Segmentation","name":"Review of Deep Learning Algorithms for Image Semantic Segmentation","articleId":"509a600f7b57","keywords":["Lite:true","Tag:Machine Learning","Tag:Deep Learning","Tag:Computer Vision","Tag:Object Detection","Tag:Image Classification","Elevated:false","LockedPostSource:LOCKED_POST_SOURCE_NONE","LayerCake:0"],"author":{"@type":"Person","name":"Arthur Ouaknine","url":"https:\u002F\u002Fmedium.com\u002F@arthur_ouaknine"},"creator":["Arthur Ouaknine"],"publisher":{"@type":"Organization","name":"Medium","url":"https:\u002F\u002Fmedium.com\u002F","logo":{"@type":"ImageObject","width":308,"height":60,"url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F336\u002F1*OMF3fSqH8t4xBJ9-6oZDZw.png"}},"mainEntityOfPage":"https:\u002F\u002Fmedium.com\u002F@arthur_ouaknine\u002Freview-of-deep-learning-algorithms-for-image-semantic-segmentation-509a600f7b57"}</script><script type="text/javascript" data-rh="true">(function(b,r,a,n,c,h,_,s,d,k){if(!b[n]||!b[n]._q){for(;s<_.length;)c(h,_[s++]);d=r.createElement(a);d.async=1;d.src="https://cdn.branch.io/branch-latest.min.js";k=r.getElementsByTagName(a)[0];k.parentNode.insertBefore(d,k);b[n]=h}})(window,document,"script","branch",function(b,r){b[r]=function(){b._q.push([r,arguments])}},{_q:[],_v:1},"addListener applyCode autoAppIndex banner closeBanner closeJourney creditHistory credits data deepview deepviewCta first getCode init link logout redeem referrals removeListener sendSMS setBranchViewData setIdentity track validateCode trackCommerceEvent logEvent".split(" "), 0);
  branch.init('key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm', {'no_journeys': true, 'disable_exit_animation': true, 'disable_entry_animation': true, 'tracking_disabled': null}, function(err, data) {});</script></head><body><div id="root"><div class="a b c"><div class="d e f g h i j k l m"></div><nav class="n o p q r c s t u v"><div class="branch-journeys-top"><section class="w x y z ab ac ae n"><div class="af ag ah ai aj"><div class="n ak s"><div class="ag ah"><a href="https://medium.com/?source=post_page---------------------------" aria-label="Homepage"><div class="al g"><svg height="22" width="112" viewBox="0 0 111.5 22" class="am"><path d="M56.3 19.5c0 .4 0 .5.3.7l1.5 1.4v.1h-6.5V19c-.7 1.8-2.4 3-4.3 3-3.3 0-5.8-2.6-5.8-7.5 0-4.5 2.6-7.6 6.3-7.6 1.6-.1 3.1.8 3.8 2.4V3.2c0-.3-.1-.6-.3-.7l-1.4-1.4V1l6.5-.8v19.3zm-4.8-.8V9.5c-.5-.6-1.2-.9-1.9-.9-1.6 0-3.1 1.4-3.1 5.7 0 4 1.3 5.4 3 5.4.8.1 1.6-.3 2-1zm9.1 3.1V9.4c0-.3-.1-.6-.3-.7l-1.4-1.5v-.1h6.5v12.5c0 .4 0 .5.3.7l1.4 1.4v.1h-6.5zm-.2-19.2C60.4 1.2 61.5 0 63 0c1.4 0 2.6 1.2 2.6 2.6S64.4 5.3 63 5.3a2.6 2.6 0 0 1-2.6-2.7zm22.5 16.9c0 .4 0 .5.3.7l1.5 1.4v.1h-6.5v-3.2c-.6 2-2.4 3.4-4.5 3.4-2.9 0-4.4-2.1-4.4-6.2 0-1.9 0-4.1.1-6.5 0-.3-.1-.5-.3-.7L67.7 7v.1H74v8c0 2.6.4 4.4 2 4.4.9-.1 1.7-.6 2.1-1.3V9.5c0-.3-.1-.6-.3-.7l-1.4-1.5v-.2h6.5v12.4zm22 2.3c0-.5.1-6.5.1-7.9 0-2.6-.4-4.5-2.2-4.5-.9 0-1.8.5-2.3 1.3.2.8.3 1.7.3 2.5 0 1.8-.1 4.2-.1 6.5 0 .3.1.5.3.7l1.5 1.4v.1H96c0-.4.1-6.5.1-7.9 0-2.7-.4-4.5-2.2-4.5-.9 0-1.7.5-2.2 1.3v9c0 .4 0 .5.3.7l1.4 1.4v.1h-6.5V9.5c0-.3-.1-.6-.3-.7l-1.4-1.5v-.2h6.5v3.1a4.6 4.6 0 0 1 4.6-3.4c2.2 0 3.6 1.2 4.2 3.5.7-2.1 2.7-3.6 4.9-3.5 2.9 0 4.5 2.2 4.5 6.2 0 1.9-.1 4.2-.1 6.5-.1.3.1.6.3.7l1.4 1.4v.1h-6.6zm-81.4-2l1.9 1.9v.1h-9.8v-.1l2-1.9c.2-.2.3-.4.3-.7V7.3c0-.5 0-1.2.1-1.8L11.4 22h-.1L4.5 6.8c-.1-.4-.2-.4-.3-.6v10c-.1.7 0 1.3.3 1.9l2.7 3.6v.1H0v-.1L2.7 18c.3-.6.4-1.3.3-1.9v-11c0-.5-.1-1.1-.5-1.5L.7 1.1V1h7l5.8 12.9L18.6 1h6.8v.1l-1.9 2.2c-.2.2-.3.5-.3.7v15.2c0 .2.1.5.3.6zm7.6-5.9c0 3.8 1.9 5.3 4.2 5.3 1.9.1 3.6-1 4.4-2.7h.1c-.8 3.7-3.1 5.5-6.5 5.5-3.7 0-7.2-2.2-7.2-7.4 0-5.5 3.5-7.6 7.3-7.6 3.1 0 6.4 1.5 6.4 6.2v.8h-8.7zm0-.8h4.3v-.8c0-3.9-.8-4.9-2-4.9-1.4.1-2.3 1.6-2.3 5.7z"></path></svg></div><div class="n an ao"><svg width="45" height="45" viewBox="0 0 45 45" class="am"><path d="M5 40V5h35v35H5zm8.56-12.63c0 .56-.03.69-.32 1.03L10.8 31.4v.4h6.97v-.4L15.3 28.4c-.29-.34-.34-.5-.34-1.03v-8.95l6.13 13.36h.71l5.26-13.36v10.64c0 .3 0 .35-.19.53l-1.85 1.8v.4h9.2v-.4l-1.83-1.8c-.18-.18-.2-.24-.2-.53V15.94c0-.3.02-.35.2-.53l1.82-1.8v-.4h-6.47l-4.62 11.55-5.2-11.54h-6.8v.4l2.15 2.63c.24.3.29.37.29.77v10.35z"></path></svg></div></a></div></div><div class="n ap s"><div class="ag ah"><div class="ag g"><div><a href="https://medium.com/membership?source=upgrade_membership---nav_full------------------------" class="aq ar as at au av aw ax ay az ba bb bc bd be bf"><span class="bg b bh bi bj bk n bl bm">Become a member</span></a></div><div class="bn n"><span class="bg b bh bi bj bk n bl bm"><a href="https://medium.com/m/signin?operation=login&amp;redirect=https%3A%2F%2Fmedium.com%2F%40arthur_ouaknine%2Freview-of-deep-learning-algorithms-for-image-semantic-segmentation-509a600f7b57&amp;source=post_page--------------------------nav_reg-" class="bo bp as at au av aw ax ay az bq br bc bd bs bt">Sign in</a></span></div></div><div class="bn n"><button class="bu bv bo bp bw bq br bx az by bg b bh bi bj bk bz ca ae cb cc bc">Get started</button></div></div></div></div></section></div></nav><div class="cd af n ce ai"></div><article><div class="cf cg n"><section class="w x y z ab ch ae n"></section></div><span class="n"></span><div><div class="eq q gj gk gl cj"></div><div class="y z ch cv"><div class="n h g f e"><aside class="my eq p" style="width: 458.383px;"></aside></div></div><section class="gm gn go gp gq"><div class="ae gr ab ch w x"><div><div id="1f69" class="gs gt er bh gu b gv gw gx gy gz ha hb"><h1 class="gu b gv hc er">Review of Deep Learning Algorithms for Image Semantic Segmentation</h1></div><div class="hd"><div class="ah ag"><div><a href="https://medium.com/@arthur_ouaknine?source=post_page---------------------------"><img alt="Arthur Ouaknine" src="Review%20of%20Deep%20Learning%20Algorithms%20for%20Image%20Semantic%20Segmentation_files/1aeACCyh-gRb76rWD-vxHxg.jpeg" class="n dw he hf" width="48" height="48"></a></div><div class="hg ab n"><div class="ag"><div style="flex:1"><span class="bg b bh bi bj bk n er am"><div class="hh ag ah hi" data-test-id="postByline"><span class="bg dg dh bi hj hk hl hm hn ho er"><a class="aq ar as at au av aw ax ay az hp bc bd be bf" href="https://medium.com/@arthur_ouaknine?source=post_page---------------------------">Arthur Ouaknine</a></span><div class="hq n ap h"><button class="hr er am bv fo fp fq ea az be fr fs ft fu fv fw by bg b bh hs dr bk bz ca ae cb cc bc">Follow</button></div></div></span></div></div><span class="bg b bh bi bj bk n bl bm"><span class="bg dg dh bi hj hk hl hm hn ho bl"><div><a class="aq ar as at au av aw ax ay az hp bc bd be bf" href="https://medium.com/@arthur_ouaknine/review-of-deep-learning-algorithms-for-image-semantic-segmentation-509a600f7b57?source=post_page---------------------------">Dec 11, 2018</a> <!-- -->·<!-- --> <!-- -->18<!-- --> min read</div></span></span></div></div></div></div><figure class="ht hu hv hw hx di hy y z paragraph-image"><div class="ib n cv ic"><div class="id n"><div class="ci hz eq p q ep ab hj v ia"><img src="Review%20of%20Deep%20Learning%20Algorithms%20for%20Image%20Semantic%20Segmentation_files/1O6mEzgZITQA80xHI7UJrUQ_002.png" class="eq p q ep ab ie if" width="427" height="466"></div><img class="mw mx eq p q ep ab en" src="Review%20of%20Deep%20Learning%20Algorithms%20for%20Image%20Semantic%20Segmentation_files/1O6mEzgZITQA80xHI7UJrUQ.png" width="427" height="466"><noscript><img src="https://miro.medium.com/max/854/1*O6mEzgZITQA80xHI7UJrUQ.png" class="eq p q ep ab" width="427" height="466"/></noscript></div></div><figcaption class="bl dh ig ih gg ch y z ii bg dg" data-selectable-paragraph="">Examples of the COCO dataset for stuff segmentation. Souce: <a href="http://cocodataset.org/?source=post_page---------------------------" class="aq cc ij ik il im">http://cocodataset.org/</a></figcaption></figure><p id="d7dc" class="in io er bh ip b iq ir is it iu iv iw ix iy iz ja" data-selectable-paragraph="">Deep
 learning algorithms have solved several computer vision tasks with an 
increasing level of difficulty. In my previous blog posts, I have 
detailled the well kwown ones: <a class="aq cc ij ik il im" href="https://medium.com/comet-app/review-of-deep-learning-algorithms-for-image-classification-5fdbca4a05e2?source=post_page---------------------------">image classification</a> and <a class="aq cc ij ik il im" href="https://medium.com/comet-app/review-of-deep-learning-algorithms-for-object-detection-c1f3d437b852?source=post_page---------------------------">object detection</a>.
 The image semantic segmentation challenge consists in classifying each 
pixel of an image (or just several ones) into an instance, each instance
 (or category) corresponding to an object or a part of the image (road, 
sky, …). This task is part of the concept of scene understanding: how a 
deep learning model can better learn the global context of a visual 
content ?</p><p id="8d74" class="in io er bh ip b iq ir is it iu iv iw ix iy iz ja" data-selectable-paragraph="">The
 object detection task has exceeded the image classification task in 
term of complexity. It consists in creating bounding boxes around the 
objects contained in an image and classify each one of them. Most of the
 object detection models use anchor boxes and proposals to detect 
bounding box around objects. Unfortunately, just a few models take into 
account the entire context of an image but they only classify a small 
part of the information. Thus, they can’t provide a full comprehension 
of a scene.</p><p id="9ddf" class="in io er bh ip b iq ir is it iu iv iw ix iy iz ja" data-selectable-paragraph=""><mark class="mz na cz">In
 order to understand a scene, each visual information has to be 
associated to an entity while considering the spatial information.</mark>
 Several other challenges have emerged to really understand the actions 
in a image or a video: keypoint detection, action recognition, video 
captioning, visual question answering and so on. A better comprehension 
of the environment will help in many fields. For example, an autonomous 
car needs to delimitate the roadsides with a high precision in order to 
move by itself. In robotics, production machines should understand how 
to grab, turn and put together two different pieces requiring to 
delimitate the exact shape of the object.</p><p id="4431" class="in io er bh ip b iq ir is it iu iv iw ix iy iz ja" data-selectable-paragraph="">In
 this blog post, architecture of a few previous state-of-the-art models 
on image semantic segmentation challenges are detailed. Note that 
researchers test their algorithms using different datasets (PASCAL VOC, 
PASCAL Context, COCO, Cityscapes) which are different between the years 
and use different metrics of evaluation. Thus the cited performances 
cannot be directly compared <em class="jb">per se</em>. Moreover, the 
results depend on the pretrained top network (the backbone), the results
 published in this post correspond to the best scores published in each 
paper with respect to their test dataset.</p><h1 id="c343" class="jc jd er bh bg fk je jf jg jh ji jj jk jl jm jn jo" data-selectable-paragraph="">Datasets and Metrics</h1><h2 id="4e1d" class="jp jd er bh bg fk jq jr js jt ju jv jw jx jy jz ka" data-selectable-paragraph="">PASCAL Visual Object Classes (PASCAL VOC)</h2><p id="8a62" class="in io er bh ip b iq kb is kc iu kd iw ke iy kf ja" data-selectable-paragraph="">The
 PASCAL VOC dataset (2012) is well-known an commonly used for object 
detection and segmentation. More than 11k images compose the train and 
validation datasets while 10k images are dedicated to the test dataset.</p><p id="0631" class="in io er bh ip b iq ir is it iu iv iw ix iy iz ja" data-selectable-paragraph="">The segmentation challenge is evaluated using the <a href="https://www.tensorflow.org/api_docs/python/tf/metrics/mean_iou?source=post_page---------------------------" class="aq cc ij ik il im">mean Intersection over Union (mIoU)</a>
 metric. The Intersection over Union (IoU) is a metric also used in 
object detection to evaluate the relevance of the predicted locations. 
The IoU is the ratio between the area of overlap and the area of union 
between the ground truth and the predicted areas. The mIoU is the 
average between the IoU of the segmented objects over all the images of 
the test dataset.</p><figure class="ht hu hv hw hx di kg y z paragraph-image"><div class="ib n cv ic"><div class="kh n"><div class="mw mx eq p q ep ab hj v ia"><img src="Review%20of%20Deep%20Learning%20Algorithms%20for%20Image%20Semantic%20Segmentation_files/1nIz4PPDt1603WEOEyg7xsQ.png" class="eq p q ep ab ie if" width="498" height="655"></div><img class="ci hz eq p q ep ab en" width="498" height="655"><noscript><img src="https://miro.medium.com/max/996/1*nIz4PPDt1603WEOEyg7xsQ.png" class="eq p q ep ab" width="498" height="655"/></noscript></div></div><figcaption class="bl dh ig ih gg ch y z ii bg dg" data-selectable-paragraph="">Examples of the 2012 PASCAL VOC dataset for image segmentation. Source: <a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html?source=post_page---------------------------" class="aq cc ij ik il im">http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html</a></figcaption></figure><h2 id="7de3" class="jp jd er bh bg fk jq jr js jt ju jv jw jx jy jz ka" data-selectable-paragraph="">PASCAL-Context</h2><p id="c074" class="in io er bh ip b iq kb is kc iu kd iw ke iy kf ja" data-selectable-paragraph="">The
 PASCAL-Context dataset (2014) is an extension of the 2010 PASCAL VOC 
dataset. It contains around 10k images for training, 10k for validation 
and 10k for testing. The specificity of this new release is that the 
entire scene is segmented providing more than 400 categories. Note that 
the images have been annotated during three months by six in-house 
annotators.</p><p id="c40c" class="in io er bh ip b iq ir is it iu iv iw ix iy iz ja" data-selectable-paragraph="">The
 official evaluation metric of the PASCAL-Context challenge is the mIoU.
 Several other metrics are published by researches as the pixel Accuracy
 (pixAcc). Here, the performances will be compared only with the mIoU.</p><figure class="ht hu hv hw hx di ki y z paragraph-image"><div class="ib n cv ic"><div class="kj n"><div class="mw mx eq p q ep ab hj v ia"><img src="Review%20of%20Deep%20Learning%20Algorithms%20for%20Image%20Semantic%20Segmentation_files/1Nb81B38WgOcQVxLIRzsrFg.png" class="eq p q ep ab ie if" width="700" height="330"></div><img class="ci hz eq p q ep ab en" width="700" height="330"><noscript><img src="https://miro.medium.com/max/1400/1*Nb81B38WgOcQVxLIRzsrFg.png" class="eq p q ep ab" width="700" height="330"/></noscript></div></div><figcaption class="bl dh ig ih gg ch y z ii bg dg" data-selectable-paragraph="">Example of the PASCAL-Context dataset. Source: <a href="https://cs.stanford.edu/~roozbeh/pascal-context/*?source=post_page---------------------------" class="aq cc ij ik il im">https://cs.stanford.edu/~roozbeh/pascal-context/</a></figcaption></figure><h2 id="c2c8" class="jp jd er bh bg fk jq jr js jt ju jv jw jx jy jz ka" data-selectable-paragraph="">Common Objects in COntext (COCO)</h2><p id="09cb" class="in io er bh ip b iq kb is kc iu kd iw ke iy kf ja" data-selectable-paragraph="">There
 are two COCO challenges (in 2017 and 2018) for image semantic 
segmentation (“object detection” and “stuff segmentation”). The “object 
detection” task consists in segmenting and categorizing objects into 80 
categories. The “stuff segmentation” task uses data with large segmented
 part of the images (sky, wall, grass), they contain almost the entire 
visual information. In this blog post, only the results of the “object 
detection” task will be compared because too few of the quoted research 
papers have published results on the “stuff segmentation” task.</p><p id="2426" class="in io er bh ip b iq ir is it iu iv iw ix iy iz ja" data-selectable-paragraph="">The
 COCO dataset for object segmentation is composed of more than 200k 
images with over 500k object instance segmented. It contains a training 
dataset, a validation dataset, a test dataset for reseachers (test-dev) 
and a test dataset for the challenge (test-challenge). The annotations 
of both test datasets are not available. These datasets contain 80 
categories and only the corresponding objects are segmented. This 
challenge uses the same metrics than the object detection challenge: the
 Average Precision (AP) and the Average Recall (AR) both using the 
Intersection over Union (IoU).</p><p id="abe6" class="in io er bh ip b iq ir is it iu iv iw ix iy iz ja" data-selectable-paragraph="">Details about IoU and AP metrics are available in my <a class="aq cc ij ik il im" href="https://medium.com/comet-app/review-of-deep-learning-algorithms-for-object-detection-c1f3d437b852?source=post_page---------------------------">previous blog post</a>.
 Such as the AP, the Average Recall is computed using multiple IoU with a
 specific range of overlapping values. For a fixed IoU, the objects with
 the corresponding test / ground truth overlapping are kept. Then the 
Recall metric is computed for the detected objects. The final AR metric 
is the average of the computed Recalls for all the IoU range values. 
Basically the AP and the AR metrics for segmentation works the same way 
with object detection excepting that the IoU is computed pixel-wise with
 a non rectangular shape for semantic segmentation.</p><figure class="ht hu hv hw hx di kk y z paragraph-image"><div class="ib n cv ic"><div class="kl n"><div class="mw mx eq p q ep ab hj v ia"><img src="Review%20of%20Deep%20Learning%20Algorithms%20for%20Image%20Semantic%20Segmentation_files/1ltungS3ZQwFEMbs7lDuI6A.png" class="eq p q ep ab ie if" width="700" height="142"></div><img class="ci hz eq p q ep ab en" width="700" height="142"><noscript><img src="https://miro.medium.com/max/1400/1*ltungS3ZQwFEMbs7lDuI6A.png" class="eq p q ep ab" width="700" height="142"/></noscript></div></div><figcaption class="bl dh ig ih gg ch y z ii bg dg" data-selectable-paragraph="">Example of the COCO dataset for object segmentation. Source: <a href="http://cocodataset.org/*?source=post_page---------------------------" class="aq cc ij ik il im">http://cocodataset.org/</a></figcaption></figure><h2 id="f898" class="jp jd er bh bg fk jq jr js jt ju jv jw jx jy jz ka" data-selectable-paragraph="">Cityscapes</h2><p id="ab2a" class="in io er bh ip b iq kb is kc iu kd iw ke iy kf ja" data-selectable-paragraph="">The
 Cityscapes dataset has been released in 2016 and consists in complex 
segmented urban scenes from 50 cities. It is composed of 23.5k images 
for training and validation (fine and coarse annotations) and 1.5 images
 for testing (only fine annotation). The images are fully segmented such
 as the PASCAL-Context dataset with 29 classes (within 8 super 
categories: flat, human, vehicle, construction, object, nature, sky, 
void). It is often used to evaluate semantic segmentation models because
 of its complexity. It is also well known for its similarity with real 
urban scenes for autonomous driving applications. The performances of 
semantic segmentation models are computed using the mIoU metric such as 
the PASCAL datasets.</p><figure class="ht hu hv hw hx di km y z paragraph-image"><div class="ib n cv ic"><div class="kn n"><div class="mw mx eq p q ep ab hj v ia"><img src="Review%20of%20Deep%20Learning%20Algorithms%20for%20Image%20Semantic%20Segmentation_files/1sa7Q_BFc087-ISgnlAsk4w.png" class="eq p q ep ab ie if" width="700" height="205"></div><img class="ci hz eq p q ep ab en" width="700" height="205"><noscript><img src="https://miro.medium.com/max/1400/1*sa7Q_BFc087-ISgnlAsk4w.png" class="eq p q ep ab" width="700" height="205"/></noscript></div></div><figcaption class="bl dh ig ih gg ch y z ii bg dg" data-selectable-paragraph="">Examples of the Cityscapes dataset. Top: coarse annotations. Bottom: fine annotation. Source: <a href="https://www.cityscapes-dataset.com/*?source=post_page---------------------------" class="aq cc ij ik il im">https://www.cityscapes-dataset.com/</a></figcaption></figure><h1 id="36a0" class="jc jd er bh bg fk je ko jg kp ji kq jk kr jm ks jo" data-selectable-paragraph="">Fully Convolutional Network (FCN)</h1><p id="6037" class="in io er bh ip b iq kb is kc iu kd iw ke iy kf ja" data-selectable-paragraph=""><a href="https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf?source=post_page---------------------------" class="aq cc ij ik il im">J. Long et al. (2015)</a>
 have been the firsts to develop an Fully Convolutional Network (FCN) 
(containing only convolutional layers) trained end-to-end for image 
segmentation.</p><p id="5b8f" class="in io er bh ip b iq ir is it iu iv iw ix iy iz ja" data-selectable-paragraph="">The
 FCN takes an image with an arbitrary size and produces a segmented 
image with the same size. The authors start by modifying well-known 
architectures (AlexNet, VGG16, GoogLeNet) to have a non fixed size input
 while replacing all the fully connected layers by convolutional layers.
 Since the network produces several feature maps with small sizes and 
dense representations, an upsampling is necessary to create an output 
with the same size than the input. Basically, it consists in a 
convolutional layer with a stride inferior to 1. It is commonly called <strong class="ip kt">deconvolution</strong>
 because it creates an output with a larger size than the input. This 
way, the network is trained using a pixel-wise loss. Moreover they have 
added skip connections in the network to combine high level feature map 
representations with more specific and dense ones at the top of the 
network.</p><p id="d7e5" class="in io er bh ip b iq ir is it iu iv iw ix iy iz ja" data-selectable-paragraph="">The
 authors have reached a 62.2% mIoU score on the 2012 PASCAL VOC 
segmentation challenge using pretrained models on the 2012 ImageNet 
dataset. For the 2012 PASCAL VOC object detection challenge, the 
benchmark model called Faster R-CNN has reached 78.8% mIoU. Even if we 
can’t directly compare the two results (different models, different 
datasets and different challenges), it seems that the semantic 
segmentation task is more difficult to solve than the object detection 
task.</p><figure class="ht hu hv hw hx di ku y z paragraph-image"><div class="ib n cv ic"><div class="kv n"><div class="mw mx eq p q ep ab hj v ia"><img src="Review%20of%20Deep%20Learning%20Algorithms%20for%20Image%20Semantic%20Segmentation_files/1pXYT1g8aWtadPBVVv9LuuA.png" class="eq p q ep ab ie if" width="456" height="230"></div><img class="ci hz eq p q ep ab en" width="456" height="230"><noscript><img src="https://miro.medium.com/max/912/1*pXYT1g8aWtadPBVVv9LuuA.png" class="eq p q ep ab" width="456" height="230"/></noscript></div></div><figcaption class="bl dh ig ih gg ch y z ii bg dg" data-selectable-paragraph="">Architecture of the FCN. Note that the skip connections are not drawn here. Souce: <a href="https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf).?source=post_page---------------------------" class="aq cc ij ik il im">J. Long et al. (2015)</a></figcaption></figure><h1 id="1d34" class="jc jd er bh bg fk je ko jg kp ji kq jk kr jm ks jo" data-selectable-paragraph="">ParseNet</h1><p id="18cc" class="in io er bh ip b iq kb is kc iu kd iw ke iy kf ja" data-selectable-paragraph=""><a href="https://arxiv.org/pdf/1506.04579.pdf?source=post_page---------------------------" class="aq cc ij ik il im">W. Liu et al. (2015)</a> have published a paper explaining improvements of the FCN model of <a href="https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf?source=post_page---------------------------" class="aq cc ij ik il im">J. Long et al. (2015)</a>.
 According to the authors, the FCN model loses the global context of the
 image in its deep layers by specializing the generated feature maps. 
The ParseNet is an end-to-end convolutional network predicting values 
for all the pixels at the same time and it avoids taking regions as 
input to keep the global information. The authors use a module taking 
feature maps as input. The first step uses a model to generate feature 
maps which are reduced into a single global feature vector with a 
pooling layer. This context vector is normalised using the <a href="https://en.wikipedia.org/wiki/Norm_(mathematics)?source=post_page---------------------------" class="aq cc ij ik il im">L2 Euclidian Norm</a>
 and it is unpooled (the output is an expanded version of the input) to 
produce new feature maps with the same sizes than the inital ones. The 
second step normalises the entire initial feature maps using the L2 
Euclidian Norm. The last step concatenates the feature maps generated by
 the two previous steps. The normalisation is helpful to scale the 
concatenated feature maps values and it leads to better performances. 
Basically, the ParseNet is a FCN with this module replacing 
convolutional layers. It has obtained a 40.4% mIoU score on the 
PASCAL-Context challenge and a 69.8% mIoU score on the 2012 PASCAL VOC 
segmentation challenge.</p><figure class="ht hu hv hw hx di kw y z paragraph-image"><div class="ib n cv ic"><div class="kx n"><div class="mw mx eq p q ep ab hj v ia"><img src="Review%20of%20Deep%20Learning%20Algorithms%20for%20Image%20Semantic%20Segmentation_files/14LWdZErZ-ocldECLEB42og.png" class="eq p q ep ab ie if" width="683" height="169"></div><img class="ci hz eq p q ep ab en" width="683" height="169"><noscript><img src="https://miro.medium.com/max/1366/1*4LWdZErZ-ocldECLEB42og.png" class="eq p q ep ab" width="683" height="169"/></noscript></div></div><figcaption class="bl dh ig ih gg ch y z ii bg dg" data-selectable-paragraph="">Comparison between the segmentation of the FCN and the ParseNet and architecture of the ParseNet module. Source: <a href="https://arxiv.org/pdf/1506.04579.pdf)?source=post_page---------------------------" class="aq cc ij ik il im">W. Liu et al. (2015)</a></figcaption></figure><h1 id="66ae" class="jc jd er bh bg fk je ko jg kp ji kq jk kr jm ks jo" data-selectable-paragraph="">Convolutional and Deconvolutional Networks</h1><p id="6966" class="in io er bh ip b iq kb is kc iu kd iw ke iy kf ja" data-selectable-paragraph=""><a href="https://arxiv.org/pdf/1505.04366.pdf?source=post_page---------------------------" class="aq cc ij ik il im">H. Noh et al. (2015)</a>
 have released an end-to-end model composed of two linked parts. The 
first part is a convolutional network with a VGG16 architecture. It 
takes as input an instance proposal, for example a bounding box 
generated by an object detection model. The proposal is processed and 
transformed by a convolutional network to generate a vector of features.
 The second part is a <strong class="ip kt">deconvolutional network</strong>
 taking the vector of features as input and generating a map of 
pixel-wise probabilities belonging to each class. The deconvolutional 
network uses <strong class="ip kt">unpooling</strong> targeting the 
maxium activations to keep the location of the information in the maps. 
The second network also uses deconvolution associating a single input to
 multiple feature maps. The deconvolution expands feature maps while 
keeping the information dense.</p><figure class="ht hu hv hw hx di ky y z paragraph-image"><div class="ib n cv ic"><div class="kz n"><div class="mw mx eq p q ep ab hj v ia"><img src="Review%20of%20Deep%20Learning%20Algorithms%20for%20Image%20Semantic%20Segmentation_files/16jJcV8_1J-s7eYl3jnXWYA.png" class="eq p q ep ab ie if" width="426" height="283"></div><img class="ci hz eq p q ep ab en" width="426" height="283"><noscript><img src="https://miro.medium.com/max/852/1*6jJcV8_1J-s7eYl3jnXWYA.png" class="eq p q ep ab" width="426" height="283"/></noscript></div></div><figcaption class="bl dh ig ih gg ch y z ii bg dg" data-selectable-paragraph="">Comparison
 of the convolutional network layers (pooling and convolution) with the 
deconvolutional network layers (unpooling and deconvolution). Source: <a href="https://arxiv.org/pdf/1505.04366.pdf?source=post_page---------------------------" class="aq cc ij ik il im">H. Noh et al. (2015)</a></figcaption></figure><p id="4825" class="in io er bh ip b iq ir is it iu iv iw ix iy iz ja" data-selectable-paragraph="">The
 authors have analysed deconvolution feature maps and they have noted 
that the low-level ones are specific to the shape while the higher-level
 ones help to classify the proposal. Finally, when all the proposals of 
an image are processed by the entire network, the maps are concatenated 
to obtain the fully segmented image. This network has obtained a 72.5% 
mIoU on the 2012 PASCAL VOC segmentation challenge.</p><figure class="ht hu hv hw hx di la y z paragraph-image"><div class="ib n cv ic"><div class="lb n"><div class="mw mx eq p q ep ab hj v ia"><img src="Review%20of%20Deep%20Learning%20Algorithms%20for%20Image%20Semantic%20Segmentation_files/1XPg1eIVN2i4x4Nrqw0gPhA.png" class="eq p q ep ab ie if" width="700" height="179"></div><img class="ci hz eq p q ep ab en" width="700" height="179"><noscript><img src="https://miro.medium.com/max/1400/1*XPg1eIVN2i4x4Nrqw0gPhA.png" class="eq p q ep ab" width="700" height="179"/></noscript></div></div><figcaption class="bl dh ig ih gg ch y z ii bg dg" data-selectable-paragraph="">Architecture
 of the full network. The convolution network is based on the VGG16 
architecture. The deconvolution network uses unpooling and deconvolution
 layers. Source: <a href="https://arxiv.org/pdf/1505.04366.pdf?source=post_page---------------------------" class="aq cc ij ik il im">H. Noh et al. (2015)</a></figcaption></figure><h1 id="91d4" class="jc jd er bh bg fk je ko jg kp ji kq jk kr jm ks jo" data-selectable-paragraph="">U-Net</h1><p id="8767" class="in io er bh ip b iq kb is kc iu kd iw ke iy kf ja" data-selectable-paragraph=""><a href="https://arxiv.org/pdf/1505.04597.pdf?source=post_page---------------------------" class="aq cc ij ik il im">O. Ronneberger et al. (2015)</a> have extended the FCN of <a href="https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf?source=post_page---------------------------" class="aq cc ij ik il im">J. Long et al. (2015)</a>
 for biological microscopy images. The authors have created a network 
called U-net composed in two parts: a contracting part to compute 
features and a expanding part to spatially localise patterns in the 
image. The <strong class="ip kt">downsampling</strong> or contracting part has a FCN-like archicture extracting features with 3x3 convolutions. The <strong class="ip kt">upsampling</strong>
 or expanding part uses up-convolution (or deconvolution) reducing the 
number of feature maps while increasing their height and width. Cropped 
feature maps from the downsampling part of the network are copied within
 the upsampling part to avoid loosing pattern information. Finally, a 
1x1 convolution processes the feature maps to generate a segmentation 
map and thus categorise each pixel of the input image. Since then, the 
U-net architecture has been widely extended in recent works (FPN, 
PSPNet, DeepLabv3 and so on). Note that it doesn’t use any 
fully-connected layer. As consequencies, the number of parameters of the
 model is reduced and it can be trained with a small labelled dataset 
(using appropriate data augmentation). For example, the authors have 
used a public dataset with 30 images for training during their 
experiments.</p><figure class="ht hu hv hw hx di lc y z paragraph-image"><div class="ib n cv ic"><div class="ld n"><div class="mw mx eq p q ep ab hj v ia"><img src="Review%20of%20Deep%20Learning%20Algorithms%20for%20Image%20Semantic%20Segmentation_files/1mi0MoA6CaqTdiYDXMPbhMQ.png" class="eq p q ep ab ie if" width="580" height="387"></div><img class="ci hz eq p q ep ab en" width="580" height="387"><noscript><img src="https://miro.medium.com/max/1160/1*mi0MoA6CaqTdiYDXMPbhMQ.png" class="eq p q ep ab" width="580" height="387"/></noscript></div></div><figcaption class="bl dh ig ih gg ch y z ii bg dg" data-selectable-paragraph="">Architecture
 of the U-net for a given input image. The blue boxes correspond to 
feature maps blocks with their denoted shapes. The white boxes 
correspond to the copied and cropped feature maps. Source: <a href="https://arxiv.org/pdf/1505.04597.pdf?source=post_page---------------------------" class="aq cc ij ik il im">O. Ronneberger et al. (2015)</a></figcaption></figure><h1 id="44f0" class="jc jd er bh bg fk je ko jg kp ji kq jk kr jm ks jo" data-selectable-paragraph="">Feature Pyramid Network (FPN)</h1><p id="5551" class="in io er bh ip b iq kb is kc iu kd iw ke iy kf ja" data-selectable-paragraph="">The Feature Pyramid Network (FPN) has been developped by <a href="https://arxiv.org/pdf/1612.03144.pdf?source=post_page---------------------------" class="aq cc ij ik il im">T.-Y. Lin et al (2016)</a>
 and it is used in object detection or image segmentation frameworks. 
Its architecture is composed of a bottom-up pathway, a top-down pathway 
and lateral connections in order to join low-resolution and 
high-resolution features. The <strong class="ip kt">bottom-up pathway</strong>
 takes an image with an arbitrary size as input. It is processed with 
convolutional layers and downsampled by pooling layers. Note that each 
bunch of feature maps with the same size is called a <strong class="ip kt">stage</strong>, the outputs of the last layer of each stage are the features used for the <strong class="ip kt">pyramid level</strong>. The <strong class="ip kt">top-down pathway</strong>
 consists in upsampling the last feature maps with unpooling while 
enhancing them with feature maps from the same stage of the bottom-up 
pathway using <strong class="ip kt">lateral connections</strong>. These 
connections consist in merging the feature maps of the bottom-up pathway
 processed with a 1x1 convolution (to reduce their dimensions) with the 
feature maps of the top-down pathway.</p><figure class="ht hu hv hw hx di le y z paragraph-image"><div class="ib n cv ic"><div class="lf n"><div class="mw mx eq p q ep ab hj v ia"><img src="Review%20of%20Deep%20Learning%20Algorithms%20for%20Image%20Semantic%20Segmentation_files/1R7r8K9Ke6QOuKyzLbwQHnA.png" class="eq p q ep ab ie if" width="269" height="227"></div><img class="ci hz eq p q ep ab en" width="269" height="227"><noscript><img src="https://miro.medium.com/max/538/1*R7r8K9Ke6QOuKyzLbwQHnA.png" class="eq p q ep ab" width="269" height="227"/></noscript></div></div><figcaption class="bl dh ig ih gg ch y z ii bg dg" data-selectable-paragraph="">Detail of a top-down block process with the lateral connection and the sum of the feature maps. Source: <a href="https://arxiv.org/pdf/1612.03144.pdf?source=post_page---------------------------" class="aq cc ij ik il im">T.-Y. Lin et al (2016)</a></figcaption></figure><p id="fd1b" class="in io er bh ip b iq ir is it iu iv iw ix iy iz ja" data-selectable-paragraph="">The
 concatenated feature maps are then processed by a 3x3 convolution to 
produce the output of the stage. Finally, each stage of the top-down 
pathway generates a prediction to detect an object. For image 
segmentation, the authors uses two Multi-Layer Perceptrons (MLP) to 
generate two masks with different size over the objets. It works 
similarly to Region Proposal Networks with anchor boxes (R-CNN <a href="http://islab.ulsan.ac.kr/files/announcement/513/rcnn_pami.pdf?source=post_page---------------------------" class="aq cc ij ik il im">R. Girshick et al. (2014)</a>, Fast R-CNN <a href="https://arxiv.org/pdf/1504.08083.pdf?source=post_page---------------------------" class="aq cc ij ik il im">R. Girshick et al. (2015)</a>, Faster R-CNN <a href="https://arxiv.org/pdf/1506.01497.pdf?source=post_page---------------------------" class="aq cc ij ik il im">S. Ren et al. (2016)</a> and so on). This method is efficient because it better propagates low information into the network. The FPN based on DeepMask (<a href="https://arxiv.org/pdf/1506.06204.pdf?source=post_page---------------------------" class="aq cc ij ik il im">P. 0. Pinheiro et al. (2015)</a>) and SharpMask (<a href="https://arxiv.org/pdf/1603.08695.pdf?source=post_page---------------------------" class="aq cc ij ik il im">P. 0. Pinheiro et al. (2016)</a>) frameworks achieved a 48.1% Average Recall (AR) score on the 2016 COCO segmentation challenge.</p><figure class="ht hu hv hw hx di lg y z paragraph-image"><div class="ib n cv ic"><div class="lh n"><div class="mw mx eq p q ep ab hj v ia"><img src="Review%20of%20Deep%20Learning%20Algorithms%20for%20Image%20Semantic%20Segmentation_files/1vUHHpEPy2Os1szpb5wDyXg.png" class="eq p q ep ab ie if" width="428" height="250"></div><img class="ci hz eq p q ep ab en" width="428" height="250"><noscript><img src="https://miro.medium.com/max/856/1*vUHHpEPy2Os1szpb5wDyXg.png" class="eq p q ep ab" width="428" height="250"/></noscript></div></div><figcaption class="bl dh ig ih gg ch y z ii bg dg" data-selectable-paragraph="">Comparison
 of architectures. (a): The image is scaled with several sizes and each 
one is processed with convolutions to provide predictions which is 
computationally expansive. (b): The image has a single scale processed 
by a CNN with convolution an pooling layers. © Each step of the CNN is 
used to provide a prediction. (d) Architecture of the FPN with the 
bottom-up part of the left and the top-down part on the right. Source: <a href="https://arxiv.org/pdf/1612.03144.pdf?source=post_page---------------------------" class="aq cc ij ik il im">T.-Y. Lin et al (2016)</a></figcaption></figure><h1 id="cf43" class="jc jd er bh bg fk je ko jg kp ji kq jk kr jm ks jo" data-selectable-paragraph="">Pyramid Scene Parsing Network (PSPNet)</h1><p id="00da" class="in io er bh ip b iq kb is kc iu kd iw ke iy kf ja" data-selectable-paragraph=""><a href="https://arxiv.org/pdf/1612.01105.pdf?source=post_page---------------------------" class="aq cc ij ik il im">H. Zhao et al. (2016)</a>
 have developped the Pyramid Scene Parsing Network (PSPNet) to better 
learn the global context representation of a scene. Patterns are 
extracted from the input image using a feature extractor (ResNet <a href="https://arxiv.org/pdf/1512.03385.pdf?source=post_page---------------------------" class="aq cc ij ik il im">K. He et al. (2015)</a>) with a <strong class="ip kt">dilated network strategy</strong>¹. The feature maps feed a <strong class="ip kt">Pyramid Pooling Module</strong>
 to distinguish patterns with different scales. They are pooled with 
four different scales each one corresponding to a pyramid level and 
processed by a 1x1 convolutional layer to reduce their dimensions. This 
way each pyramid level analyses sub-regions of the image with different 
location. The outputs of the pyramid levels are upsampled and 
concatenated to the inital feature maps to finally contain the local and
 the global context information. Then, they are processed by a 
convolutional layer to generate the pixel-wise predictions. The best 
PSPNet with a pretrained ResNet (using the COCO dataset) has reached a 
85.4% mIoU score on the 2012 PASCAL VOC segmentation challenge.</p><figure class="ht hu hv hw hx di li y z paragraph-image"><div class="ib n cv ic"><div class="lj n"><div class="mw mx eq p q ep ab hj v ia"><img src="Review%20of%20Deep%20Learning%20Algorithms%20for%20Image%20Semantic%20Segmentation_files/1Tp9s_0BrmxeIHIyQbg_vLw.png" class="eq p q ep ab ie if" width="700" height="207"></div><img class="ci hz eq p q ep ab en" width="700" height="207"><noscript><img src="https://miro.medium.com/max/1400/1*Tp9s_0BrmxeIHIyQbg_vLw.png" class="eq p q ep ab" width="700" height="207"/></noscript></div></div><figcaption class="bl dh ig ih gg ch y z ii bg dg" data-selectable-paragraph="">PSPNet
 architecture. The input image (a) is processed by a CNN to generate 
feature maps (b). They feed a Pyramid Pooling Module © and a final 
convolutional layer generates the pixel-wise predictions. Source: <a href="https://arxiv.org/pdf/1612.01105.pdf?source=post_page---------------------------" class="aq cc ij ik il im">H. Zhao et al. (2016)</a></figcaption></figure><h1 id="8793" class="jc jd er bh bg fk je ko jg kp ji kq jk kr jm ks jo" data-selectable-paragraph="">Mask R-CNN</h1><p id="ecfe" class="in io er bh ip b iq kb is kc iu kd iw ke iy kf ja" data-selectable-paragraph=""><a href="https://arxiv.org/pdf/1703.06870.pdf?source=post_page---------------------------" class="aq cc ij ik il im">K. He et al. (2017)</a>
 have released the Mask R-CNN model beating all previous benchmarks on 
many COCO challenges². I have already provided details about Mask R-CNN 
for object detection in my <a class="aq cc ij ik il im" href="https://medium.com/comet-app/review-of-deep-learning-algorithms-for-object-detection-c1f3d437b852?source=post_page---------------------------">previous blog post</a>. As a reminder, the Faster R-CNN (<a href="https://arxiv.org/pdf/1506.01497.pdf?source=post_page---------------------------" class="aq cc ij ik il im">S. Ren et al. (2015)</a>) architecture for object detection uses a <strong class="ip kt">Region Proposal Network</strong> (RPN) to propose bounding box candidates. The RPN extracts <strong class="ip kt">Region of Interest</strong> (RoI) and a <strong class="ip kt">RoIPool</strong>
 layer computes features from these proposals in order to infer the 
bounding box cordinates and the class of the object. The Mask R-CNN is a
 Faster R-CNN with 3 output branches: the first one computes the 
bounding box coordinates, the second one computes the associated class 
and the last one computes the binary mask³ to segment the object. The 
binary mask has a fixed size and it is generated by a FCN for a given 
RoI. It also uses a <strong class="ip kt">RoIAlign</strong> layer 
instead of a RoIPool to avoid misalignments due to the quantization of 
the RoI coordinates. The particularity of the Mask R-CNN model is its <strong class="ip kt">multi-task loss</strong>
 combining the losses of the bounding box coordinates, the predicted 
class and the segmentation mask. The model tries to solve complementary 
tasks leading to better performances on each individual task. The best 
Mask R-CNN uses a ResNeXt (<a href="https://arxiv.org/pdf/1611.05431.pdf?source=post_page---------------------------" class="aq cc ij ik il im">S. Xie et al. (2016)</a>)
 to extract features and a FPN architecture. It has obtained a 37.1% AP 
score on the 2016 COCO segmentation challenge and a 41.8% AP score on 
the 2017 COCO segmentation challenge.</p><figure class="ht hu hv hw hx di lk y z paragraph-image"><div class="ib n cv ic"><div class="ll n"><div class="mw mx eq p q ep ab hj v ia"><img src="Review%20of%20Deep%20Learning%20Algorithms%20for%20Image%20Semantic%20Segmentation_files/1IPB0s9fCiXlGpM_S6Y8rWQ.png" class="eq p q ep ab ie if" width="432" height="198"></div><img class="ci hz eq p q ep ab en" width="432" height="198"><noscript><img src="https://miro.medium.com/max/864/1*IPB0s9fCiXlGpM_S6Y8rWQ.png" class="eq p q ep ab" width="432" height="198"/></noscript></div></div><figcaption class="bl dh ig ih gg ch y z ii bg dg" data-selectable-paragraph="">Mask
 R-CNN achitecture. The first layer is a RPN extracting the RoI. The 
second layer processes the RoI to generate feature maps. They are 
directly used to compute the bounding box coordinates and the predicted 
class. The feature maps are also processed by an FCN (third layer) to 
generate the binary mask. Source: <a href="https://arxiv.org/pdf/1703.06870.pdf?source=post_page---------------------------" class="aq cc ij ik il im">K. He et al. (2017)</a></figcaption></figure><h1 id="3c68" class="jc jd er bh bg fk je ko jg kp ji kq jk kr jm ks jo" data-selectable-paragraph="">DeepLab, DeepLabv3 and DeepLabv3+</h1><h2 id="ebc6" class="jp jd er bh bg fk jq jr js jt ju jv jw jx jy jz ka" data-selectable-paragraph="">DeepLab</h2><p id="9289" class="in io er bh ip b iq kb is kc iu kd iw ke iy kf ja" data-selectable-paragraph="">Inspired by the FPN model of <a href="https://arxiv.org/pdf/1612.03144.pdf?source=post_page---------------------------" class="aq cc ij ik il im">T.-Y. Lin et al (2016)</a>, <a href="https://arxiv.org/pdf/1606.00915.pdf?source=post_page---------------------------" class="aq cc ij ik il im">L.-C. Chen et al. (2017)</a>
 have released DeepLab combining atrous convolution, spatial pyramid 
pooling and fully connected CRFs. The model presented in this paper is 
also called the DeepLabv2 because it is an adjustment of the initial 
DeepLab model (details about the inital one will not be provided to 
avoid redundancy). According to the authors, consecutive max-pooling and
 striding reduces the resolution of the feature maps in deep neural 
networks. They have introduced the <strong class="ip kt">atrous convolution</strong> which is basically the dilated convolution of <a href="https://arxiv.org/pdf/1612.01105.pdf?source=post_page---------------------------" class="aq cc ij ik il im">H. Zhao et al. (2016)</a>.
 It consists of filters targeting sparse pixels with a fixed rate. For 
example, if the rate is equal to 2, the filter targets one pixel over 
two in the input; if the rate equal to 1, the atrous convolution is a 
basic convolution. Atrous convolution permits to capture multiple scale 
of objects. When it is used without max-poolling, it increases the 
resolution of the final output without increasing the number of weights.</p><figure class="ht hu hv hw hx di lm y z paragraph-image"><div class="ib n cv ic"><div class="ln n"><div class="mw mx eq p q ep ab hj v ia"><img src="Review%20of%20Deep%20Learning%20Algorithms%20for%20Image%20Semantic%20Segmentation_files/13UoUqTo_QjrcszzF_LRLfA.png" class="eq p q ep ab ie if" width="552" height="353"></div><img class="ci hz eq p q ep ab en" width="552" height="353"><noscript><img src="https://miro.medium.com/max/1104/1*3UoUqTo_QjrcszzF_LRLfA.png" class="eq p q ep ab" width="552" height="353"/></noscript></div></div><figcaption class="bl dh ig ih gg ch y z ii bg dg" data-selectable-paragraph="">Extraction
 patterns comparison between standard convolution on a low resolution 
input (top) and atrous convolution with a rate of 2 on a high resolution
 input (bottom). Source: <a href="https://arxiv.org/pdf/1606.00915.pdf?source=post_page---------------------------" class="aq cc ij ik il im">L.-C. Chen et al. (2017)</a></figcaption></figure><p id="1c7f" class="in io er bh ip b iq ir is it iu iv iw ix iy iz ja" data-selectable-paragraph="">The <strong class="ip kt">Atrous Spatial Pyramid Pooling</strong>
 consists in applying several atrous convolution of the same input with 
different rate to detect spatial patterns. The features maps are 
processed in separate branches and concatenated using bilinear 
interpolation to recovert the original size of the input. The output 
feeds a fully connected Conditional Random Field (CRF) (<a href="https://arxiv.org/pdf/1210.5644.pdf?source=post_page---------------------------" class="aq cc ij ik il im">Krähenbühl and V. Koltun (2012)</a>) computing edges between the features and long terme dependencies to produce the semantic segmentation.</p><figure class="ht hu hv hw hx di lo y z paragraph-image"><div class="ib n cv ic"><div class="lp n"><div class="mw mx eq p q ep ab hj v ia"><img src="Review%20of%20Deep%20Learning%20Algorithms%20for%20Image%20Semantic%20Segmentation_files/1iKGyx6XR2EmIutDp1Uy5bw.png" class="eq p q ep ab ie if" width="612" height="312"></div><img class="ci hz eq p q ep ab en" width="612" height="312"><noscript><img src="https://miro.medium.com/max/1224/1*iKGyx6XR2EmIutDp1Uy5bw.png" class="eq p q ep ab" width="612" height="312"/></noscript></div></div><figcaption class="bl dh ig ih gg ch y z ii bg dg" data-selectable-paragraph="">Atrous Spatial Pyramid Pooling (ASPP) exploiting multiple scale of objects to classify the pixel in the center. Source: <a href="https://arxiv.org/pdf/1606.00915.pdf?source=post_page---------------------------" class="aq cc ij ik il im">L.-C. Chen et al. (2017)</a></figcaption></figure><p id="19ee" class="in io er bh ip b iq ir is it iu iv iw ix iy iz ja" data-selectable-paragraph="">The
 best DeepLab using a ResNet-101 as backbone has reached a 79.7% mIoU 
score on the 2012 PASCAL VOC challenge, a 45.7% mIoU score on the 
PASCAL-Context challenge and a 70.4% mIoU score on the Cityscapes 
challenge.</p><figure class="ht hu hv hw hx di lq y z paragraph-image"><div class="ib n cv ic"><div class="lr n"><div class="mw mx eq p q ep ab hj v ia"><img src="Review%20of%20Deep%20Learning%20Algorithms%20for%20Image%20Semantic%20Segmentation_files/1fNCnDjexDIJsDdmvf217ew.png" class="eq p q ep ab ie if" width="700" height="327"></div><img class="ci hz eq p q ep ab en" width="700" height="327"><noscript><img src="https://miro.medium.com/max/1400/1*fNCnDjexDIJsDdmvf217ew.png" class="eq p q ep ab" width="700" height="327"/></noscript></div></div><figcaption class="bl dh ig ih gg ch y z ii bg dg" data-selectable-paragraph="">DeepLab framework. Source: <a href="https://arxiv.org/pdf/1606.00915.pdf?source=post_page---------------------------" class="aq cc ij ik il im">L.-C. Chen et al. (2017)</a></figcaption></figure><h2 id="ce1e" class="jp jd er bh bg fk jq jr js jt ju jv jw jx jy jz ka" data-selectable-paragraph="">DeepLabv3</h2><p id="2953" class="in io er bh ip b iq kb is kc iu kd iw ke iy kf ja" data-selectable-paragraph=""><a href="https://arxiv.org/pdf/1706.05587.pdf?source=post_page---------------------------" class="aq cc ij ik il im">L.-C. Chen et al. (2017)</a>
 have revisited the DeepLab framework to create DeepLabv3 combining 
cascaded and parallel modules of atrous convolutions. The authors have 
modified the ResNet architecture to keep high resolution feature maps in
 deep blocks using atrous convolutions.</p><figure class="ht hu hv hw hx di ls y z paragraph-image"><div class="ib n cv ic"><div class="lt n"><div class="mw mx eq p q ep ab hj v ia"><img src="Review%20of%20Deep%20Learning%20Algorithms%20for%20Image%20Semantic%20Segmentation_files/1JtVdwr11eTQnqukZ1YeQnA.png" class="eq p q ep ab ie if" width="700" height="252"></div><img class="ci hz eq p q ep ab en" width="700" height="252"><noscript><img src="https://miro.medium.com/max/1400/1*JtVdwr11eTQnqukZ1YeQnA.png" class="eq p q ep ab" width="700" height="252"/></noscript></div></div><figcaption class="bl dh ig ih gg ch y z ii bg dg" data-selectable-paragraph="">Cascaded modules in the ResNet architecture. Source: <a href="https://arxiv.org/pdf/1706.05587.pdf?source=post_page---------------------------" class="aq cc ij ik il im">L.-C. Chen et al. (2017)</a></figcaption></figure><p id="63d6" class="in io er bh ip b iq ir is it iu iv iw ix iy iz ja" data-selectable-paragraph="">The
 parallel atrous convolution modules are grouped in the Atrous Spatial 
Pyramid Pooling (ASPP). A 1x1 convolution and batch normalisation are 
added in the ASPP. All the outputs are concatenated and processed by 
another 1x1 convolution to create the final output with logits for each 
pixel.</p><figure class="ht hu hv hw hx di lu y z paragraph-image"><div class="ib n cv ic"><div class="lv n"><div class="mw mx eq p q ep ab hj v ia"><img src="Review%20of%20Deep%20Learning%20Algorithms%20for%20Image%20Semantic%20Segmentation_files/1R7tiLxyeHYHMXTGJIanZiA.png" class="eq p q ep ab ie if" width="700" height="181"></div><img class="ci hz eq p q ep ab en" width="700" height="181"><noscript><img src="https://miro.medium.com/max/1400/1*R7tiLxyeHYHMXTGJIanZiA.png" class="eq p q ep ab" width="700" height="181"/></noscript></div></div><figcaption class="bl dh ig ih gg ch y z ii bg dg" data-selectable-paragraph="">Atrous Spatial Pyramid Pooling in the Deeplabv3 framework. Source: <a href="https://arxiv.org/pdf/1706.05587.pdf?source=post_page---------------------------" class="aq cc ij ik il im">L.-C. Chen et al. (2017)</a></figcaption></figure><p id="da3a" class="in io er bh ip b iq ir is it iu iv iw ix iy iz ja" data-selectable-paragraph="">The
 best DeepLabv3 model with a ResNet-101 pretrained on ImageNet and 
JFT-300M datasets has reached 86.9% mIoU score in the 2012 PASCAL VOC 
challenge. It also achieved a 81.3% mIoU score on the Cityscapes 
challenge with a model only trained with the associated training 
dataset.</p><h2 id="c295" class="jp jd er bh bg fk jq jr js jt ju jv jw jx jy jz ka" data-selectable-paragraph="">DeepLabv3+</h2><p id="94d0" class="in io er bh ip b iq kb is kc iu kd iw ke iy kf ja" data-selectable-paragraph=""><a href="https://arxiv.org/pdf/1802.02611.pdf?source=post_page---------------------------" class="aq cc ij ik il im">L.-C. Chen et al. (2018)</a> have finally released the Deeplabv3+ framework using an encoder-decoder structure. The authors have introduced the <strong class="ip kt">atrous separable convolution</strong>
 composed of a depthwise convolution (spatial convolution for each 
channel of the input) and pointwise convolution (1x1 convolution with 
the depthwise convolution as input).</p><figure class="ht hu hv hw hx di lw y z paragraph-image"><div class="ib n cv ic"><div class="lx n"><div class="mw mx eq p q ep ab hj v ia"><img src="Review%20of%20Deep%20Learning%20Algorithms%20for%20Image%20Semantic%20Segmentation_files/1c5rVU1iEJ0jk69DEPyB1-Q.png" class="eq p q ep ab ie if" width="670" height="149"></div><img class="ci hz eq p q ep ab en" width="670" height="149"><noscript><img src="https://miro.medium.com/max/1340/1*c5rVU1iEJ0jk69DEPyB1-Q.png" class="eq p q ep ab" width="670" height="149"/></noscript></div></div><figcaption class="bl dh ig ih gg ch y z ii bg dg" data-selectable-paragraph="">Combinaison
 of Depthwise convolution (a) and Pointwise convolution (b) to create 
Atrous Separable Convolution (with a rate of 2). Source: <a href="https://arxiv.org/pdf/1802.02611.pdf?source=post_page---------------------------" class="aq cc ij ik il im">L.-C. Chen et al. (2018)</a></figcaption></figure><p id="6d49" class="in io er bh ip b iq ir is it iu iv iw ix iy iz ja" data-selectable-paragraph="">They have used the DeepLabv3 framework as encoder. The most performant model has a modified Xception (<a href="https://arxiv.org/abs/1610.02357?source=post_page---------------------------" class="aq cc ij ik il im">F. Chollet (2017)</a>)
 backbone with more layers, atrous depthwise separable convolutions 
instead of max pooling and batch normalization. The outputs of the ASPP 
are processed by a 1x1 convolution and upsampled by a factor of 4. The 
outputs of the encoder backbone CNN are also processed by another 1x1 
convolution and concatenated to the previous ones. The feature maps feed
 two 3x3 convolutional layers and the outputs are upsampled by a factor 
of 4 to create the final segmented image.</p><figure class="ht hu hv hw hx di ly y z paragraph-image"><div class="ib n cv ic"><div class="lz n"><div class="mw mx eq p q ep ab hj v ia"><img src="Review%20of%20Deep%20Learning%20Algorithms%20for%20Image%20Semantic%20Segmentation_files/109azMblllhMGJgZuDeJrzw.png" class="eq p q ep ab ie if" width="700" height="361"></div><img class="ci hz eq p q ep ab en" width="700" height="361"><noscript><img src="https://miro.medium.com/max/1400/1*09azMblllhMGJgZuDeJrzw.png" class="eq p q ep ab" width="700" height="361"/></noscript></div></div><figcaption class="bl dh ig ih gg ch y z ii bg dg" data-selectable-paragraph="">DeepLabv3+
 framework: an encoder with a backbone CNN and an ASPP produces feature 
representations to feed a decoder with 3x3 convolutions producing the 
final predicted image. Source: <a href="https://arxiv.org/pdf/1802.02611.pdf?source=post_page---------------------------" class="aq cc ij ik il im">L.-C. Chen et al. (2018)</a></figcaption></figure><p id="fd5f" class="in io er bh ip b iq ir is it iu iv iw ix iy iz ja" data-selectable-paragraph="">The
 best DeepLabv3+ pretrained on the COCO and the JFT datasets has 
obtained a 89.0% mIoU score on the 2012 PASCAL VOC challenge. The model 
trained on the Cityscapes dataset has reached a 82.1% mIoU score for the
 associated challenge.</p><h1 id="3ef9" class="jc jd er bh bg fk je jf jg jh ji jj jk jl jm jn jo" data-selectable-paragraph="">Path Aggregation Network (PANet)</h1><p id="edf6" class="in io er bh ip b iq kb is kc iu kd iw ke iy kf ja" data-selectable-paragraph=""><a href="https://arxiv.org/pdf/1803.01534.pdf?source=post_page---------------------------" class="aq cc ij ik il im">S. Liu et al. (2018)</a>
 have recently released the Path Aggregation Network (PANet). This 
network is based on the Mask R-CNN and the FPN frameworks while 
enhancing information propagation. The feature extractor of the network 
uses a FPN architecture with a new <strong class="ip kt">augmented bottom-up pathway</strong>
 improving the propagation of low-layer features. Each stage of this 
third pathway takes as input the feature maps of the previous stage and 
processes them with a 3x3 convolutional layer. The output is added to 
the same stage feature maps of the top-down pathway using lateral 
connection and these feature maps feed the next stage.</p><figure class="ht hu hv hw hx di ma y z paragraph-image"><div class="ib n cv ic"><div class="mb n"><div class="mw mx eq p q ep ab hj v ia"><img src="Review%20of%20Deep%20Learning%20Algorithms%20for%20Image%20Semantic%20Segmentation_files/1WmN0JUMwo2WaQnorPQ2zIg.png" class="eq p q ep ab ie if" width="264" height="156"></div><img class="ci hz eq p q ep ab en" width="264" height="156"><noscript><img src="https://miro.medium.com/max/528/1*WmN0JUMwo2WaQnorPQ2zIg.png" class="eq p q ep ab" width="264" height="156"/></noscript></div></div><figcaption class="bl dh ig ih gg ch y z ii bg dg" data-selectable-paragraph="">Lateral connection between the top-down pathway and the augmented bottom-up pathway. Source: <a href="https://arxiv.org/pdf/1803.01534.pdf?source=post_page---------------------------" class="aq cc ij ik il im">S. Liu et al. (2018)</a></figcaption></figure><p id="1797" class="in io er bh ip b iq ir is it iu iv iw ix iy iz ja" data-selectable-paragraph="">The
 feature maps of the augmented bottom-up pathway are pooled with a 
RoIAlign layer to extract proposals from all level features. An <strong class="ip kt">adaptative feature pooling</strong> layer processes the features maps of each stage with a fully connected layer and concatenate all the outputs.</p><figure class="ht hu hv hw hx di mc y z paragraph-image"><div class="ib n cv ic"><div class="md n"><div class="mw mx eq p q ep ab hj v ia"><img src="Review%20of%20Deep%20Learning%20Algorithms%20for%20Image%20Semantic%20Segmentation_files/16qnH-e2HH-gYBaV_vBqzxA.png" class="eq p q ep ab ie if" width="403" height="183"></div><img class="ci hz eq p q ep ab en" width="403" height="183"><noscript><img src="https://miro.medium.com/max/806/1*6qnH-e2HH-gYBaV_vBqzxA.png" class="eq p q ep ab" width="403" height="183"/></noscript></div></div><figcaption class="bl dh ig ih gg ch y z ii bg dg" data-selectable-paragraph="">Adatative feature pooling layer. Source: <a href="https://arxiv.org/pdf/1803.01534.pdf?source=post_page---------------------------" class="aq cc ij ik il im">S. Liu et al. (2018)</a></figcaption></figure><p id="7a1f" class="in io er bh ip b iq ir is it iu iv iw ix iy iz ja" data-selectable-paragraph="">The
 output of the adaptative feature pooling layer feeds three branches 
similarly to the Mask R-CNN. The two first branches uses a fully 
connected layer to generate the predictions of the bounding box 
coordinates and the associated object class. The third branch process 
the RoI with a FCN to <strong class="ip kt">predict a binary pixel-wise mask</strong>
 for the detected object. The authors have added a path processing the 
output of a convolutional layer of the FCN with a fully connected layer 
to improve the localisation of the predicted pixels. Finally the output 
of the parallel path is reshaped and concatenated to the output of the 
FCN generating the binary mask.</p><figure class="ht hu hv hw hx di me y z paragraph-image"><div class="ib n cv ic"><div class="mf n"><div class="mw mx eq p q ep ab hj v ia"><img src="Review%20of%20Deep%20Learning%20Algorithms%20for%20Image%20Semantic%20Segmentation_files/1yHoAq_qBXoJeIDU_fSgtAw.png" class="eq p q ep ab ie if" width="390" height="202"></div><img class="ci hz eq p q ep ab en" width="390" height="202"><noscript><img src="https://miro.medium.com/max/780/1*yHoAq_qBXoJeIDU_fSgtAw.png" class="eq p q ep ab" width="390" height="202"/></noscript></div></div><figcaption class="bl dh ig ih gg ch y z ii bg dg" data-selectable-paragraph="">Branch of the PANet predicting the binary mask using a FCN and a new path with a fully connected layer. Source: <a href="https://arxiv.org/pdf/1803.01534.pdf)*?source=post_page---------------------------" class="aq cc ij ik il im">https://arxiv.org/pdf/1803.01534.pdf</a></figcaption></figure><p id="7424" class="in io er bh ip b iq ir is it iu iv iw ix iy iz ja" data-selectable-paragraph="">The
 PANet has achieved 42.0% AP score on the 2016 COCO segmentation 
challenge using a ResNeXt as feature extractor. They also performed the 
2017 COCO segmentation challenge with an 46.7% AP score using a ensemble
 of seven feature extractors: ResNet (<a href="https://arxiv.org/pdf/1512.03385.pdf?source=post_page---------------------------" class="aq cc ij ik il im">K. He et al. (2015)</a>, ResNeXt (<a href="https://arxiv.org/pdf/1611.05431.pdf?source=post_page---------------------------" class="aq cc ij ik il im">S. Xie et al. (2016)</a>) and SENet (<a href="https://arxiv.org/pdf/1709.01507.pdf?source=post_page---------------------------" class="aq cc ij ik il im">J. Hu et al.(2017)</a>).</p><figure class="ht hu hv hw hx di mg y z paragraph-image"><div class="ib n cv ic"><div class="mh n"><div class="mw mx eq p q ep ab hj v ia"><img src="Review%20of%20Deep%20Learning%20Algorithms%20for%20Image%20Semantic%20Segmentation_files/1hxQWJjNxADU6VlQ5xnSGDw.png" class="eq p q ep ab ie if" width="700" height="237"></div><img class="ci hz eq p q ep ab en" width="700" height="237"><noscript><img src="https://miro.medium.com/max/1400/1*hxQWJjNxADU6VlQ5xnSGDw.png" class="eq p q ep ab" width="700" height="237"/></noscript></div></div><figcaption class="bl dh ig ih gg ch y z ii bg dg" data-selectable-paragraph="">PANet
 Achitecture. (a): Feature extractor using the FPN achitecture. (b): The
 new augmented bottom-up pathway added to the FPN architecture. ©: The 
adaptative feature pooling layer. (d): The two branches predicting the 
bounding box coordinated and the object class. (e): The branch 
predicting the binary mask of the object. The dashed lines correspond to
 links between low-level and high level patterns, the red one is in the 
FPN and consists in more than 100 layers, the green one is a shortcut in
 the PANet consisting of less than 10 layers. Source: <a href="https://arxiv.org/pdf/1803.01534.pdf?source=post_page---------------------------" class="aq cc ij ik il im">S. Liu et al. (2018)</a></figcaption></figure><h1 id="45fe" class="jc jd er bh bg fk je ko jg kp ji kq jk kr jm ks jo" data-selectable-paragraph="">Context Encoding Network (EncNet)</h1><p id="20c0" class="in io er bh ip b iq kb is kc iu kd iw ke iy kf ja" data-selectable-paragraph=""><a href="https://arxiv.org/pdf/1803.08904.pdf?source=post_page---------------------------" class="aq cc ij ik il im">H. Zhang et al. (2018)</a>
 have created a Context Encoding Network (EncNet) capturing global 
information in an image to improve scene segmentation. The model starts 
by using a basic feature extractor (ResNet) and feeds the feature maps 
into a <strong class="ip kt">Context Encoding Module</strong> inspired from the Encoding Layer of <a href="https://arxiv.org/pdf/1612.02844.pdf?source=post_page---------------------------" class="aq cc ij ik il im">H. Zhang et al. (2016)</a>.
 Basically, it learns visual centers and smoothing factors to create an 
embedding taking into account the contextual information while 
highlighting class-dependant feature maps. On top of the module, scaling
 factors for the contextual information are learnt with a feature maps 
attention layer (fully connected layer). In parallel, a <strong class="ip kt">Semantic Encoding Loss</strong>
 (SE-Loss) corresponding to a binary cross-entropy loss regularizes the 
training of the module by detecting presence of object classes (unlike 
the pixel-wise loss). The outputs of the Context Encoding Module are 
reshaped and processed by a dilated convolution strategy while 
minimizing two SE-losses and a final pixel-wise loss. The best EncNet 
has reached 52.6% mIoU and 81.2% pixAcc scores on the PASCAL-Context 
challenge. It has also achieved a 85.9% mIoU score on the 2012 PASCAL 
VOC segmentation challenge.</p><figure class="ht hu hv hw hx di mi y z paragraph-image"><div class="ib n cv ic"><div class="mj n"><div class="mw mx eq p q ep ab hj v ia"><img src="Review%20of%20Deep%20Learning%20Algorithms%20for%20Image%20Semantic%20Segmentation_files/1akweXy62Lay-Qqx7UpiUPA.png" class="eq p q ep ab ie if" width="419" height="149"></div><img class="ci hz eq p q ep ab en" width="419" height="149"><noscript><img src="https://miro.medium.com/max/838/1*akweXy62Lay-Qqx7UpiUPA.png" class="eq p q ep ab" width="419" height="149"/></noscript></div></div><figcaption class="bl dh ig ih gg ch y z ii bg dg" data-selectable-paragraph="">Dilated
 convolution strategy. In blue the convolutional filter with D the 
dilatation rate. The SE-losses (Semantic Encoding Loss) are applied 
after the third and the fourth stages to detect object classes. A final 
Seg-loss (pixel-wise loss) is applied to improve the segmentation. 
Source: <a href="https://arxiv.org/pdf/1803.08904.pdf?source=post_page---------------------------" class="aq cc ij ik il im">H. Zhang et al. (2018)</a></figcaption></figure><figure class="ht hu hv hw hx di mk y z paragraph-image"><div class="ib n cv ic"><div class="ml n"><div class="mw mx eq p q ep ab hj v ia"><img src="Review%20of%20Deep%20Learning%20Algorithms%20for%20Image%20Semantic%20Segmentation_files/1TCsoZHK71wSXy5MbT8JGDw.png" class="eq p q ep ab ie if" width="700" height="203"></div><img class="ci hz eq p q ep ab en" width="700" height="203"><noscript><img src="https://miro.medium.com/max/1400/1*TCsoZHK71wSXy5MbT8JGDw.png" class="eq p q ep ab" width="700" height="203"/></noscript></div></div><figcaption class="bl dh ig ih gg ch y z ii bg dg" data-selectable-paragraph="">Architecture
 of the EncNet. A feature extractor generates feature maps took as input
 of a Context Encoding Module. The module is trained with regularisation
 using the Semantic Encoding Loss. The outputs of the module are 
processed by a dilated convolution strategy to produce the final 
segmention. Source: [<a href="https://arxiv.org/pdf/1803.08904.pdf?source=post_page---------------------------" class="aq cc ij ik il im">H. Zhang et al. (2018)</a></figcaption></figure><h1 id="b45b" class="jc jd er bh bg fk je ko jg kp ji kq jk kr jm ks jo" data-selectable-paragraph="">Conclusion</h1><p id="61c2" class="in io er bh ip b iq kb is kc iu kd iw ke iy kf ja" data-selectable-paragraph="">Image
 semantic segmentation is a challenge recently takled by end-to-end deep
 neural networks. One of the main issue between all the architectures is
 to take into account the global visual context of the input to improve 
the prediction of the segmentation. The state-of-the-art models use 
architectures trying to link different part of the image in order to 
understand the relations between the objects.</p><figure class="ht hu hv hw hx di mm y z paragraph-image"><div class="ib n cv ic"><div class="mn n"><div class="mw mx eq p q ep ab hj v ia"><img src="Review%20of%20Deep%20Learning%20Algorithms%20for%20Image%20Semantic%20Segmentation_files/1MsVAhHvNuFARowPZRiGiJQ.png" class="eq p q ep ab ie if" width="700" height="390"></div><img class="ci hz eq p q ep ab en" width="700" height="390"><noscript><img src="https://miro.medium.com/max/1400/1*MsVAhHvNuFARowPZRiGiJQ.png" class="eq p q ep ab" width="700" height="390"/></noscript></div></div><figcaption class="bl dh ig ih gg ch y z ii bg dg" data-selectable-paragraph="">Overview
 of the scores of the models over the 2012 PASCAL VOC dataset (mIoU), 
the PASCAL-Context dataset (mIoU), the 2016 / 2017 COCO datasets (AP and
 AR) and the Cityscapes dataset (mIoU)</figcaption></figure><p id="f98e" class="in io er bh ip b iq ir is it iu iv iw ix iy iz ja" data-selectable-paragraph="">The
 pixel-wise prediction over an entire image allows a better 
comprehension of the environement with a high precision. Scene 
understanding is also approached with keypoint detection, action 
recognition, video captioning or visual question answering. To my 
opinion, the segmentation task combined with these other issues using 
multi-task loss should help to outperform the global context 
understanding of a scene.</p><p id="256d" class="in io er bh ip b iq ir is it iu iv iw ix iy iz ja" data-selectable-paragraph="">Finally, I would like to thanks <a href="https://ldocao.wordpress.com/?source=post_page---------------------------" class="aq cc ij ik il im">Long Do Cao</a> for helping me with all my posts, you should check his profile if you’re looking for a great senior data scientist ;).</p></div></section><hr class="mo dg fl mp mq gg mr ms mt mu mv"><section class="gm gn go gp gq"><div class="ae gr ab ch w x"><p id="f28d" class="in io er bh ip b iq ir is it iu iv iw ix iy iz ja" data-selectable-paragraph="">¹: The dilated convolutional layer has been released by [F. Yu and V. Koltun (2015)](<a href="https://arxiv.org/pdf/1511.07122.pdf?source=post_page---------------------------" class="aq cc ij ik il im">https://arxiv.org/pdf/1511.07122.pdf</a>).
 It is a convolutional layer with expanded filter (the neurons of the 
filter are no more side-by-side). A dilatation rate fixes the gap 
between two neurons in term of pixel. More details are provided in the 
DeepLab section.</p><p id="aaa2" class="in io er bh ip b iq ir is it iu iv iw ix iy iz ja" data-selectable-paragraph="">²: Object detection, object segmentation and keypoint detection.</p><p id="9c79" class="in io er bh ip b iq ir is it iu iv iw ix iy iz ja" data-selectable-paragraph="">³:
 The Mask R-CNN model compute a binary mask for an object for a 
predicted class (instance-first strategy) instead of classifying each 
pixel into a category (segmentation-first strategy).</p></div></section></div></article><div class="ci cj ck o cl cm cn co cp e" data-test-id="post-sidebar"><div class="ag cq"><div class="cr cs ct ag"><div class="ag ah"><div class="cu n cv"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40arthur_ouaknine%2Freview-of-deep-learning-algorithms-for-image-semantic-segmentation-509a600f7b57&amp;source=post_sidebar-----509a600f7b57---------------------clap_sidebar-" class="aq ar as at au av aw ax ay az ba bb bc bd be bf"><div class="ax cw cx cy cz da db dc dd"><svg width="29" height="29"><g fill-rule="evenodd"><path d="M13.74 1l.76 2.97.76-2.97zM16.82 4.78l1.84-2.56-1.43-.47zM10.38 2.22l1.84 2.56-.41-3.03zM22.38 22.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M9.1 22.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L6.1 15.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L6.4 11.26l-1.18-1.18a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L11.96 14a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L8.43 9.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L20.63 15c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM13 6.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 23 23.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></div></a></div><div class="de n"><div class="df"><h4 class="bg dg dh bi bl"><button class="aq ar as at au av aw ax ay az ba bb bc bd be bf">543 </button></h4></div></div></div></div><div><div class="cb"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40arthur_ouaknine%2Freview-of-deep-learning-algorithms-for-image-semantic-segmentation-509a600f7b57&amp;source=post_sidebar--------------------------bookmark_sidebar-" class="aq ar as at au av aw ax ay az ba bb bc bd be bf"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></a></div></div></div></div><div><div class="cg di ag cq dj"><section class="w x y z ab ch ae n"><ul class="ax ay"><li class="cb dk dl dm"><a href="https://medium.com/tag/machine-learning" class="dn do cc bl n dp dq a b dr">Machine Learning</a></li><li class="cb dk dl dm"><a href="https://medium.com/tag/deep-learning" class="dn do cc bl n dp dq a b dr">Deep Learning</a></li><li class="cb dk dl dm"><a href="https://medium.com/tag/computer-vision" class="dn do cc bl n dp dq a b dr">Computer Vision</a></li><li class="cb dk dl dm"><a href="https://medium.com/tag/object-detection" class="dn do cc bl n dp dq a b dr">Object Detection</a></li><li class="cb dk dl dm"><a href="https://medium.com/tag/image-classification" class="dn do cc bl n dp dq a b dr">Image Classification</a></li></ul><div class="ag ds dt"><div class="ag ah"><div class="du n cv"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40arthur_ouaknine%2Freview-of-deep-learning-algorithms-for-image-semantic-segmentation-509a600f7b57&amp;source=post_actions_footer-----509a600f7b57---------------------clap_footer-" class="aq ar as at au av aw ax ay az ba bb bc bd be bf"><div class="c dv dw ag ah dx cv dy dz ea eb ec ed ee ef eg eh ei ej ek el"><div class="ax cw cx cy cz da em ah en dw ag dc dj eo q ep eq p ab dd"><svg width="33" height="33" viewBox="0 0 33 33"><path d="M28.86 17.34l-3.64-6.4c-.3-.43-.71-.73-1.16-.8a1.12 1.12 0 0 0-.9.21c-.62.5-.73 1.18-.32 2.06l1.22 2.6 1.4 2.45c2.23 4.09 1.51 8-2.15 11.66a9.6 9.6 0 0 1-.8.71 6.53 6.53 0 0 0 4.3-2.1c3.82-3.82 3.57-7.87 2.05-10.39zm-6.25 11.08c3.35-3.35 4-6.78 1.98-10.47L21.2 12c-.3-.43-.71-.72-1.16-.8a1.12 1.12 0 0 0-.9.22c-.62.49-.74 1.18-.32 2.06l1.72 3.63a.5.5 0 0 1-.81.57l-8.91-8.9a1.33 1.33 0 0 0-1.89 1.88l5.3 5.3a.5.5 0 0 1-.71.7l-5.3-5.3-1.49-1.49c-.5-.5-1.38-.5-1.88 0a1.34 1.34 0 0 0 0 1.89l1.49 1.5 5.3 5.28a.5.5 0 0 1-.36.86.5.5 0 0 1-.36-.15l-5.29-5.29a1.34 1.34 0 0 0-1.88 0 1.34 1.34 0 0 0 0 1.89l2.23 2.23L9.3 21.4a.5.5 0 0 1-.36.85.5.5 0 0 1-.35-.14l-3.32-3.33a1.33 1.33 0 0 0-1.89 0 1.32 1.32 0 0 0-.39.95c0 .35.14.69.4.94l6.39 6.4c3.53 3.53 8.86 5.3 12.82 1.35zM12.73 9.26l5.68 5.68-.49-1.04c-.52-1.1-.43-2.13.22-2.89l-3.3-3.3a1.34 1.34 0 0 0-1.88 0 1.33 1.33 0 0 0-.4.94c0 .22.07.42.17.61zm14.79 19.18a7.46 7.46 0 0 1-6.41 2.31 7.92 7.92 0 0 1-3.67.9c-3.05 0-6.12-1.63-8.36-3.88l-6.4-6.4A2.31 2.31 0 0 1 2 19.72a2.33 2.33 0 0 1 1.92-2.3l-.87-.87a2.34 2.34 0 0 1 0-3.3 2.33 2.33 0 0 1 1.24-.64l-.14-.14a2.34 2.34 0 0 1 0-3.3 2.39 2.39 0 0 1 3.3 0l.14.14a2.33 2.33 0 0 1 3.95-1.24l.09.09c.09-.42.29-.83.62-1.16a2.34 2.34 0 0 1 3.3 0l3.38 3.39a2.17 2.17 0 0 1 1.27-.17c.54.08 1.03.35 1.45.76.1-.55.41-1.03.9-1.42a2.12 2.12 0 0 1 1.67-.4 2.8 2.8 0 0 1 1.85 1.25l3.65 6.43c1.7 2.83 2.03 7.37-2.2 11.6zM13.22.48l-1.92.89 2.37 2.83-.45-3.72zm8.48.88L19.78.5l-.44 3.7 2.36-2.84zM16.5 3.3L15.48 0h2.04L16.5 3.3z" fill-rule="evenodd"></path></svg></div></div></a></div><div class="de n"><div class="df"><h4 class="bg dg dh bi er"><button class="aq ar as at au av aw ax ay az ba bb bc bd be bf">543 claps</button></h4></div></div></div><div class="ag ah"><div class="es n ap g"><a href="https://medium.com/p/509a600f7b57/share/twitter?source=follow_footer--------------------------follow_footer-" class="aq ar as at au av aw ax ay az ba bb bc bd be bf"><svg width="29" height="29" class="am"><path d="M22.05 7.54a4.47 4.47 0 0 0-3.3-1.46 4.53 4.53 0 0 0-4.53 4.53c0 .35.04.7.08 1.05A12.9 12.9 0 0 1 5 6.89a5.1 5.1 0 0 0-.65 2.26c.03 1.6.83 2.99 2.02 3.79a4.3 4.3 0 0 1-2.02-.57v.08a4.55 4.55 0 0 0 3.63 4.44c-.4.08-.8.13-1.21.16l-.81-.08a4.54 4.54 0 0 0 4.2 3.15 9.56 9.56 0 0 1-5.66 1.94l-1.05-.08c2 1.27 4.38 2.02 6.94 2.02 8.3 0 12.86-6.9 12.84-12.85.02-.24 0-.43 0-.65a8.68 8.68 0 0 0 2.26-2.34c-.82.38-1.7.62-2.6.72a4.37 4.37 0 0 0 1.95-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></a></div><div class="es n ap g"><a href="https://medium.com/p/509a600f7b57/share/facebook?source=follow_footer--------------------------follow_footer-" class="aq ar as at au av aw ax ay az ba bb bc bd be bf"><svg width="29" height="29" class="am"><path d="M23.2 5H5.8a.8.8 0 0 0-.8.8V23.2c0 .44.35.8.8.8h9.3v-7.13h-2.38V13.9h2.38v-2.38c0-2.45 1.55-3.66 3.74-3.66 1.05 0 1.95.08 2.2.11v2.57h-1.5c-1.2 0-1.48.57-1.48 1.4v1.96h2.97l-.6 2.97h-2.37l.05 7.12h5.1a.8.8 0 0 0 .79-.8V5.8a.8.8 0 0 0-.8-.79"></path></svg></a></div><div class="es al ao"><div class="cb"><button class="aq ar as at au av aw ax ay az ba bb bc bd be bf"><svg width="25" height="25" class="am"><g fill-rule="evenodd"><path d="M15.6 5a.42.42 0 0 0 .17-.3.42.42 0 0 0-.12-.33l-2.8-2.79a.5.5 0 0 0-.7 0l-2.8 2.8a.4.4 0 0 0-.1.32c0 .12.07.23.16.3h.02a.45.45 0 0 0 .57-.04l2-2V10c0 .28.23.5.5.5s.5-.22.5-.5V2.93l2.02 2.02c.08.07.18.12.3.13.11.01.21-.02.3-.08v.01"></path><path d="M18 7h-1.5a.5.5 0 0 0 0 1h1.6c.5 0 .9.4.9.9v10.2c0 .5-.4.9-.9.9H6.9a.9.9 0 0 1-.9-.9V8.9c0-.5.4-.9.9-.9h1.6a.5.5 0 0 0 .35-.15A.5.5 0 0 0 9 7.5a.5.5 0 0 0-.15-.35A.5.5 0 0 0 8.5 7H7a2 2 0 0 0-2 2v10c0 1.1.9 2 2 2h11a2 2 0 0 0 2-2V9a2 2 0 0 0-2-2"></path></g></svg></button></div></div><div class="es n ap"><div><div class="cb"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40arthur_ouaknine%2Freview-of-deep-learning-algorithms-for-image-semantic-segmentation-509a600f7b57&amp;source=post_actions_footer--------------------------bookmark_sidebar-" class="aq ar as at au av aw ax ay az ba bb bc bd be bf"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></a></div></div></div><div class="cb"><div class="n ap"><button class="aq ar as at au av aw ax ay az ba bb bc bd be bf"><svg width="25" height="25" viewBox="-480.5 272.5 21 21" class="am"><path d="M-463 284.6c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5zm-7-.9c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5zm-7-.9c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5z"></path></svg></button></div></div></div></div><div class="et eu ev ew"><div class="ex ey n cv"><span class="n ez aj fa"><div class="n eq fb fc"><a href="https://medium.com/@arthur_ouaknine?source=follow_footer--------------------------follow_footer-"><img alt="Arthur Ouaknine" src="Review%20of%20Deep%20Learning%20Algorithms%20for%20Image%20Semantic%20Segmentation_files/1aeACCyh-gRb76rWD-vxHxg_002.jpeg" class="n dw fd fe" width="80" height="80"></a></div><span class="n"><div class="ff n fg"><p class="bg dg dr bi bl fh fi">Written by</p></div><div class="ff fj ag fg"><div class="ab ag ah ds"><h2 class="bg fk fl fm er"><a class="aq ar as at au av aw ax ay az ba bb bc bd be bf" href="https://medium.com/@arthur_ouaknine?source=follow_footer--------------------------follow_footer-">Arthur Ouaknine</a></h2><div class="n g"><button class="fn er am bv fo fp fq ea az be fr fs ft fu fv fw by bg b bh bi bj bk bz ca ae cb cc bc">Follow</button></div></div></div></span></span><div class="ff fx n fg ao"><div class="fy n"><h4 class="bg dg fz ga bl">PhD Student at Telecom ParisTech and valeo.ai — Deep Learning enthusiast</h4></div><div class="al gb ao"><button class="fn er am bv fo fp fq ea az be fr fs ft fu fv fw by bg b bh bi bj bk bz ca ae cb cc bc">Follow</button></div></div></div></div><div class="gc gd n"><a href="https://medium.com/p/509a600f7b57/responses/show?source=follow_footer--------------------------follow_footer-" class="aq ar as at au av aw ax ay az ba bb bc bd be bf"><div class="ge gf dn n gg ao"><span class="bo">See responses (4)</span></div></a></div></section><div class="gh n gi"><section class="w x y z ab ac ae n"><section class="y hd z ab nb ae ag ds"><section class="ex y z ab nb ae n"><div class="nc nd ex n"><h2 class="bg fk ne nf er">More From Medium</h2></div><div class="ag ng"><div class="nh ni n ak nj"><div class="ab ep"><div class="ex ag cq"><div class="nk n"><h4 class="bg dg dh bi bl">Related reads</h4></div><div class="nl n"><a href="https://towardsdatascience.com/a-keras-pipeline-for-image-segmentation-part-1-6515a421157d?source=post_recirc---------0------------------" class="aq ar as at au av aw ax ay az ba bb bc bd be bf n"><div class="nm cv"><div class="ep eq ab"><div class="nn n no np ep ab nq nr"></div></div></div></a></div><div class="nl n"><a href="https://towardsdatascience.com/a-keras-pipeline-for-image-segmentation-part-1-6515a421157d?source=post_recirc---------0------------------"><h3 class="er am gu ns bh nt nu nv">A Keras Pipeline for Image Segmentation</h3></a></div><div class="ag ah ds"><div class="nw n nx"><div class="ah ag"><div><a href="https://medium.com/@dejavu.chakraborty?source=post_recirc---------0------------------"><img alt="Rwiddhi Chakraborty" src="Review%20of%20Deep%20Learning%20Algorithms%20for%20Image%20Semantic%20Segmentation_files/1EOWEMkJjpeHd9T3zuK0gsQ.jpeg" class="n dw ny nz" width="40" height="40"></a></div><div class="hg ab n"><div class="ag"><div style="flex: 1 1 0%;"><span class="bg b bh bi bj bk n er am"><div class="cd ag ah hi" data-test-id="postByline"><span class="bg dg dh bi hj hk hl hm hn ho er"><a class="aq ar as at au av aw ax ay az hp bc bd be bf" href="https://medium.com/@dejavu.chakraborty?source=post_recirc---------0------------------">Rwiddhi Chakraborty</a><span> in <a href="https://towardsdatascience.com/?source=post_recirc---------0------------------" class="aq ar as at au av aw ax ay az hp bc bd be bf">Towards Data Science</a></span></span></div></span></div></div><span class="bg b bh bi bj bk n bl bm"><span class="bg dg dh bi hj hk hl hm hn ho bl"><div><a href="https://towardsdatascience.com/a-keras-pipeline-for-image-segmentation-part-1-6515a421157d?source=post_recirc---------0------------------" class="aq ar as at au av aw ax ay az hp bc bd be bf">Mar 5</a> · 9 min read<span style="padding-left: 4px;"><svg class="star-15px_svg__svgIcon-use" width="15" height="15" viewBox="0 0 15 15" style="margin-top: -2px;"><path d="M7.44 2.32c.03-.1.09-.1.12 0l1.2 3.53a.29.29 0 0 0 .26.2h3.88c.11 0 .13.04.04.1L9.8 8.33a.27.27 0 0 0-.1.29l1.2 3.53c.03.1-.01.13-.1.07l-3.14-2.18a.3.3 0 0 0-.32 0L4.2 12.22c-.1.06-.14.03-.1-.07l1.2-3.53a.27.27 0 0 0-.1-.3L2.06 6.16c-.1-.06-.07-.12.03-.12h3.89a.29.29 0 0 0 .26-.19l1.2-3.52z"></path></svg></span></div></span></span></div></div></div><div class="ag ah"><div class="ag ah"><div class="cu n cv"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40arthur_ouaknine%2Freview-of-deep-learning-algorithms-for-image-semantic-segmentation-509a600f7b57&amp;source=post_recirc-----6515a421157d----0-----------------clap_preview-" class="aq ar as at au av aw ax ay az ba bb bc bd be bf"><div class="ax cw cx cy cz da db dc dd"><svg width="25" height="25" viewBox="0 0 25 25"><g fill-rule="evenodd"><path d="M11.74 0l.76 2.97.76-2.97zM14.81 3.78l1.84-2.56-1.42-.47zM8.38 1.22l1.84 2.56L9.8.75zM20.38 21.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M7.1 21.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L4.1 14.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L4.4 10.26 3.22 9.08a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L9.96 13a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L6.43 8.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L18.63 14c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM11 5.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 21 22.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></div></a></div><div class="de n"><div class="df"><h4 class="bg dg dh bi bl">229 </h4></div></div></div><div class="oa hg nw ob oc n"></div><div><div class="cb"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40arthur_ouaknine%2Freview-of-deep-learning-algorithms-for-image-semantic-segmentation-509a600f7b57&amp;source=post_recirc---------0-----------------bookmark_sidebar-" class="aq ar as at au av aw ax ay az ba bb bc bd be bf"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></a></div></div></div></div></div></div></div><div class="nh ni n ak nj"><div class="ab ep"><div class="ex ag cq"><div class="nk n"><h4 class="bg dg dh bi bl">Related reads</h4></div><div class="nl n"><a href="https://towardsdatascience.com/review-fpn-feature-pyramid-network-object-detection-262fc7482610?source=post_recirc---------1------------------" class="aq ar as at au av aw ax ay az ba bb bc bd be bf n"><div class="nm cv"><div class="ep eq ab"><div class="od n no np ep ab nq nr"></div></div></div></a></div><div class="nl n"><a href="https://towardsdatascience.com/review-fpn-feature-pyramid-network-object-detection-262fc7482610?source=post_recirc---------1------------------"><h3 class="er am gu ns bh nt nu nv">Review: FPN — Feature Pyramid Network (Object Detection)</h3></a></div><div class="ag ah ds"><div class="nw n nx"><div class="ah ag"><div><a href="https://medium.com/@sh.tsang?source=post_recirc---------1------------------"><img alt="Sik-Ho Tsang" src="Review%20of%20Deep%20Learning%20Algorithms%20for%20Image%20Semantic%20Segmentation_files/1OxjNUHcLFU8-pp-j8su6pg.jpeg" class="n dw ny nz" width="40" height="40"></a></div><div class="hg ab n"><div class="ag"><div style="flex: 1 1 0%;"><span class="bg b bh bi bj bk n er am"><div class="cd ag ah hi" data-test-id="postByline"><span class="bg dg dh bi hj hk hl hm hn ho er"><a class="aq ar as at au av aw ax ay az hp bc bd be bf" href="https://medium.com/@sh.tsang?source=post_recirc---------1------------------">Sik-Ho Tsang</a><span> in <a href="https://towardsdatascience.com/?source=post_recirc---------1------------------" class="aq ar as at au av aw ax ay az hp bc bd be bf">Towards Data Science</a></span></span></div></span></div></div><span class="bg b bh bi bj bk n bl bm"><span class="bg dg dh bi hj hk hl hm hn ho bl"><div><a href="https://towardsdatascience.com/review-fpn-feature-pyramid-network-object-detection-262fc7482610?source=post_recirc---------1------------------" class="aq ar as at au av aw ax ay az hp bc bd be bf">Jan 17</a> · 9 min read</div></span></span></div></div></div><div class="ag ah"><div class="ag ah"><div class="cu n cv"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40arthur_ouaknine%2Freview-of-deep-learning-algorithms-for-image-semantic-segmentation-509a600f7b57&amp;source=post_recirc-----262fc7482610----1-----------------clap_preview-" class="aq ar as at au av aw ax ay az ba bb bc bd be bf"><div class="ax cw cx cy cz da db dc dd"><svg width="25" height="25" viewBox="0 0 25 25"><g fill-rule="evenodd"><path d="M11.74 0l.76 2.97.76-2.97zM14.81 3.78l1.84-2.56-1.42-.47zM8.38 1.22l1.84 2.56L9.8.75zM20.38 21.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M7.1 21.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L4.1 14.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L4.4 10.26 3.22 9.08a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L9.96 13a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L6.43 8.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L18.63 14c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM11 5.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 21 22.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></div></a></div><div class="de n"><div class="df"><h4 class="bg dg dh bi bl">474 </h4></div></div></div><div class="oa hg nw ob oc n"></div><div><div class="cb"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40arthur_ouaknine%2Freview-of-deep-learning-algorithms-for-image-semantic-segmentation-509a600f7b57&amp;source=post_recirc---------1-----------------bookmark_sidebar-" class="aq ar as at au av aw ax ay az ba bb bc bd be bf"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></a></div></div></div></div></div></div></div><div class="oe ni n ak nj"><div class="ab ep"><div class="ex ag cq"><div class="nk n"><h4 class="bg dg dh bi bl">Also tagged Object Detection</h4></div><div class="nl n"><a href="https://towardsdatascience.com/simple-surveillance-system-with-the-tensorflow-object-detection-api-125e04d36446?source=post_recirc---------2------------------" class="aq ar as at au av aw ax ay az ba bb bc bd be bf n"><div class="nm cv"><div class="ep eq ab"><div class="of n no np ep ab nq nr"></div></div></div></a></div><div class="nl n"><a href="https://towardsdatascience.com/simple-surveillance-system-with-the-tensorflow-object-detection-api-125e04d36446?source=post_recirc---------2------------------"><h3 class="er am gu ns bh nt nu nv">Simple Surveillance System with the Tensorflow Object Detection API</h3></a></div><div class="ag ah ds"><div class="nw n nx"><div class="ah ag"><div><a href="https://medium.com/@gilberttanner?source=post_recirc---------2------------------"><div class="cv nz ny"><svg width="46" height="50" viewBox="0 0 46 50" class="bp eq og oh oi oj cj"><path d="M1.45 15.22C5.43 7.07 13.59 1.5 23 1.5v-1C13.18.5 4.69 6.32.55 14.78l.9.44zM23 1.5c9.4 0 17.57 5.57 21.55 13.72l.9-.44C41.3 6.32 32.82.5 23 .5v1zm21.55 33.28C40.57 42.93 32.41 48.5 23 48.5v1c9.82 0 18.31-5.82 22.45-14.28l-.9-.44zM23 48.5c-9.4 0-17.57-5.57-21.55-13.72l-.9.44C4.7 43.68 13.18 49.5 23 49.5v-1z"></path></svg><img alt="Gilbert Tanner" src="Review%20of%20Deep%20Learning%20Algorithms%20for%20Image%20Semantic%20Segmentation_files/1-za8S8j0FQi1qTsRI8MvXA.jpeg" class="n dw ny nz" width="40" height="40"></div></a></div><div class="hg ab n"><div class="ag"><div style="flex: 1 1 0%;"><span class="bg b bh bi bj bk n er am"><div class="cd ag ah hi" data-test-id="postByline"><span class="bg dg dh bi hj hk hl hm hn ho er"><a class="aq ar as at au av aw ax ay az hp bc bd be bf" href="https://medium.com/@gilberttanner?source=post_recirc---------2------------------">Gilbert Tanner</a><span> in <a href="https://towardsdatascience.com/?source=post_recirc---------2------------------" class="aq ar as at au av aw ax ay az hp bc bd be bf">Towards Data Science</a></span></span></div></span></div></div><span class="bg b bh bi bj bk n bl bm"><span class="bg dg dh bi hj hk hl hm hn ho bl"><div><a href="https://towardsdatascience.com/simple-surveillance-system-with-the-tensorflow-object-detection-api-125e04d36446?source=post_recirc---------2------------------" class="aq ar as at au av aw ax ay az hp bc bd be bf">Jul 20</a> · 5 min read<span style="padding-left: 4px;"><svg class="star-15px_svg__svgIcon-use" width="15" height="15" viewBox="0 0 15 15" style="margin-top: -2px;"><path d="M7.44 2.32c.03-.1.09-.1.12 0l1.2 3.53a.29.29 0 0 0 .26.2h3.88c.11 0 .13.04.04.1L9.8 8.33a.27.27 0 0 0-.1.29l1.2 3.53c.03.1-.01.13-.1.07l-3.14-2.18a.3.3 0 0 0-.32 0L4.2 12.22c-.1.06-.14.03-.1-.07l1.2-3.53a.27.27 0 0 0-.1-.3L2.06 6.16c-.1-.06-.07-.12.03-.12h3.89a.29.29 0 0 0 .26-.19l1.2-3.52z"></path></svg></span></div></span></span></div></div></div><div class="ag ah"><div class="ag ah"><div class="cu n cv"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40arthur_ouaknine%2Freview-of-deep-learning-algorithms-for-image-semantic-segmentation-509a600f7b57&amp;source=post_recirc-----125e04d36446----2-----------------clap_preview-" class="aq ar as at au av aw ax ay az ba bb bc bd be bf"><div class="ax cw cx cy cz da db dc dd"><svg width="25" height="25" viewBox="0 0 25 25"><g fill-rule="evenodd"><path d="M11.74 0l.76 2.97.76-2.97zM14.81 3.78l1.84-2.56-1.42-.47zM8.38 1.22l1.84 2.56L9.8.75zM20.38 21.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M7.1 21.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L4.1 14.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L4.4 10.26 3.22 9.08a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L9.96 13a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L6.43 8.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L18.63 14c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM11 5.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 21 22.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></div></a></div><div class="de n"><div class="df"><h4 class="bg dg dh bi bl">61 </h4></div></div></div><div class="oa hg nw ob oc n"></div><div><div class="cb"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40arthur_ouaknine%2Freview-of-deep-learning-algorithms-for-image-semantic-segmentation-509a600f7b57&amp;source=post_recirc---------2-----------------bookmark_sidebar-" class="aq ar as at au av aw ax ay az ba bb bc bd be bf"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></a></div></div></div></div></div></div></div></div></section></section></section></div></div></div><script>window.PARSELY = window.PARSELY || {autotrack: false}</script></div></div><script>window.__BUILD_ID__ = "development"</script><script>window.__GRAPHQL_URI__ = "https://medium.com/_/graphql"</script><script>window.__PRELOADED_STATE__ = {"config":{"nodeEnv":"production","version":"master-20190725-172748-a8311c1bcc","productName":"Medium","publicUrl":"https:\u002F\u002Fcdn-client.medium.com\u002Flite","authDomain":"medium.com","authGoogleClientId":"216296035834-k1k6qe060s2tp2a2jam4ljdcms00sttg.apps.googleusercontent.com","favicon":"production","glyphUrl":"https:\u002F\u002Fglyph.medium.com","iTunesAppId":"828256236","branchKey":"key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm","lightStep":{"name":"lite-web","host":"collector-medium.lightstep.com","token":"ce5be895bef60919541332990ac9fef2","appVersion":"master-20190725-172748-a8311c1bcc"},"algolia":{"appId":"MQ57UUUQZ2","apiKeySearch":"394474ced050e3911ae2249ecc774921","indexPrefix":"medium_","host":"-dsn.algolia.net"},"recaptchaKey":"6LdAokEUAAAAAC7seICd4vtC8chDb3jIXDQulyUJ","sentry":{"dsn":"https:\u002F\u002F589e367c28ca47b195ce200d1507d18b@sentry.io\u002F1423575","environment":"production"},"isAmp":false,"googleAnalyticsCode":"UA-24232453-2","signInWallCustomDomainCollectionIds":["3a8144eabfe3","336d898217ee","61061eb0c96b","138adf9c44c","819cc2aaeee0"]},"debug":{"requestId":"26ffe96c-95bb-4ba2-a8bf-9683509b0ca2","originalSpanCarrier":{"ot-tracer-spanid":"1588dcdf3e530e54","ot-tracer-traceid":"715e24e27bdb0ef9","ot-tracer-sampled":"true"}},"session":{"user":{"id":"lo_u1EUALNPlpAr"},"xsrf":""},"stats":{"itemCount":0,"sending":false,"timeout":null,"backup":{}},"navigation":{"showBranchBanner":null,"hideGoogleOneTap":false,"currentLocation":"https:\u002F\u002Fmedium.com\u002F@arthur_ouaknine\u002Freview-of-deep-learning-algorithms-for-image-semantic-segmentation-509a600f7b57","host":"medium.com","hostname":"medium.com","referrer":"https:\u002F\u002Fwww.google.com\u002F","currentHash":""},"client":{"isBot":false,"isDnt":false,"isEu":false,"isNativeMedium":false,"isCustomDomain":false},"multiVote":{"clapsPerPost":{}},"metadata":{"faviconImageId":null}}</script><script>window.__APOLLO_STATE__ = {"ROOT_QUERY":{"viewer":null,"variantFlags":[{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.0","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.1","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.2","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.3","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.4","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.5","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.6","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.7","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.8","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.9","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.10","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.11","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.12","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.13","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.14","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.15","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.16","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.17","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.18","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.19","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.20","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.21","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.22","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.23","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.24","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.25","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.26","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.27","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.28","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.29","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.30","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.31","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.32","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.33","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.34","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.35","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.36","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.37","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.38","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.39","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.40","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.41","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.42","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.43","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.44","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.45","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.46","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.47","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.48","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.49","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.50","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.51","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.52","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.53","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.54","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.55","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.56","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.57","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.58","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.59","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.60","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.61","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.62","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.63","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.64","typename":"VariantFlag"}],"meterPost({\"postId\":\"509a600f7b57\",\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}})":{"type":"id","generated":false,"id":"MeteringInfo:singleton","typename":"MeteringInfo"},"postResult({\"id\":\"509a600f7b57\"})":{"type":"id","generated":false,"id":"Post:509a600f7b57","typename":"Post"}},"ROOT_QUERY.variantFlags.0":{"name":"allow_access","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.0.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.0.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.1":{"name":"allow_signup","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.1.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.1.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.2":{"name":"allow_test_auth","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.2.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.2.valueType":{"__typename":"VariantFlagString","value":"disallow"},"ROOT_QUERY.variantFlags.3":{"name":"signin_services","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.3.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.3.valueType":{"__typename":"VariantFlagString","value":"twitter,facebook,google,email,google-fastidv,google-one-tap"},"ROOT_QUERY.variantFlags.4":{"name":"signup_services","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.4.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.4.valueType":{"__typename":"VariantFlagString","value":"twitter,facebook,google,email,google-fastidv,google-one-tap"},"ROOT_QUERY.variantFlags.5":{"name":"google_sign_in_android","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.5.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.5.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.6":{"name":"browsable_stream_config_bucket","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.6.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.6.valueType":{"__typename":"VariantFlagString","value":"curated-topics"},"ROOT_QUERY.variantFlags.7":{"name":"enable_dedicated_series_tab_api_ios","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.7.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.7.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.8":{"name":"enable_post_import","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.8.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.8.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.9":{"name":"available_monthly_plan","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.9.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.9.valueType":{"__typename":"VariantFlagString","value":"60e220181034"},"ROOT_QUERY.variantFlags.10":{"name":"available_annual_plan","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.10.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.10.valueType":{"__typename":"VariantFlagString","value":"2c754bcc2995"},"ROOT_QUERY.variantFlags.11":{"name":"disable_ios_resume_reading_toast","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.11.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.11.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.12":{"name":"is_not_medium_subscriber","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.12.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.12.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.13":{"name":"glyph_font_set","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.13.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.13.valueType":{"__typename":"VariantFlagString","value":"m2"},"ROOT_QUERY.variantFlags.14":{"name":"enable_branding","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.14.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.14.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.15":{"name":"enable_branding_fonts","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.15.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.15.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.16":{"name":"enable_automated_mission_control_triggers","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.16.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.16.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.17":{"name":"enable_lite_profile","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.17.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.17.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.18":{"name":"enable_gosocial_aurora_shadow_reads","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.18.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.18.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.19":{"name":"enable_marketing_emails","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.19.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.19.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.20":{"name":"enable_parsely","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.20.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.20.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.21":{"name":"enable_branch_io","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.21.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.21.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.22":{"name":"enable_ios_post_stats","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.22.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.22.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.23":{"name":"enable_lite_topics","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.23.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.23.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.24":{"name":"enable_lite_stories","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.24.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.24.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.25":{"name":"redis_read_write_splitting","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.25.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.25.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.26":{"name":"enable_tipalti_onboarding","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.26.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.26.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.27":{"name":"enable_annual_renewal_reminder_email","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.27.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.27.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.28":{"name":"enable_janky_spam_rules","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.28.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.28.valueType":{"__typename":"VariantFlagString","value":"users,posts"},"ROOT_QUERY.variantFlags.29":{"name":"enable_new_collaborative_filtering_data","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.29.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.29.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.30":{"name":"enable_google_one_tap","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.30.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.30.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.31":{"name":"enable_email_sign_in_captcha","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.31.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.31.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.32":{"name":"enable_primary_topic_for_mobile","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.32.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.32.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.33":{"name":"enable_lite_post","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.33.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.33.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.34":{"name":"enable_logged_out_homepage_signup","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.34.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.34.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.35":{"name":"use_new_admin_topic_backend","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.35.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.35.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.36":{"name":"enable_quarantine_rules","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.36.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.36.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.37":{"name":"enable_patronus_on_kubernetes","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.37.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.37.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.38":{"name":"pub_sidebar","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.38.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.38.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.39":{"name":"disable_mobile_featured_chunk","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.39.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.39.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.40":{"name":"enable_embedding_based_diversification","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.40.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.40.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.41":{"name":"enable_lite_pub_header_menu","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.41.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.41.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.42":{"name":"enable_lite_claps","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.42.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.42.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.43":{"name":"enable_live_user_post_scoring","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.43.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.43.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.44":{"name":"enable_lite_post_highlights","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.44.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.44.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.45":{"name":"enable_lite_post_highlights_view_only","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.45.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.45.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.46":{"name":"enable_tick_landing_page","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.46.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.46.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.47":{"name":"enable_lite_post_cd","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.47.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.47.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.48":{"name":"enable_lite_private_notes","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.48.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.48.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.49":{"name":"enable_lite_private_notes_li_100","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.49.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.49.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.50":{"name":"enable_trumpland_landing_page","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.50.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.50.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.51":{"name":"enable_lite_email_sign_in_flow","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.51.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.51.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.52":{"name":"enable_lite_paywall_alert","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.52.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.52.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.53":{"name":"enable_edit_alt_text","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.53.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.53.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.54":{"name":"enable_serve_recs_from_ml_rank_homepage","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.54.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.54.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.55":{"name":"enable_serve_recs_from_ml_rank_digest","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.55.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.55.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.56":{"name":"enable_serve_recs_from_ml_rank_app_highlights","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.56.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.56.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.57":{"name":"enable_lite_thanks_to","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.57.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.57.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.58":{"name":"enable_lite_google_captcha","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.58.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.58.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.59":{"name":"enable_lite_branch_io","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.59.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.59.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.60":{"name":"enable_lite_notifications","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.60.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.60.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.61":{"name":"enable_lite_audio_upsells","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.61.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.61.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.62":{"name":"enable_lite_verify_email_butter_bar","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.62.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.62.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.63":{"name":"remove_social_proof_on_digest","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.63.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.63.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.64":{"name":"enable_lite_unread_notification_count","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.64.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.64.valueType":{"__typename":"VariantFlagBoolean","value":true},"MeteringInfo:singleton":{"__typename":"MeteringInfo","postIds":{"type":"json","json":[]},"maxUnlockCount":3,"unlocksRemaining":3},"Post:509a600f7b57":{"__typename":"Post","creator":{"type":"id","generated":false,"id":"User:9690d2dcec12","typename":"User"},"isLocked":false,"lockedSource":"LOCKED_POST_SOURCE_NONE","id":"509a600f7b57","collection":null,"sequence":null,"firstPublishedAt":1544527315064,"isPublished":true,"title":"Review of Deep Learning Algorithms for Image Semantic Segmentation","canonicalUrl":"","layerCake":0,"primaryTopic":null,"content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}})":{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}})","typename":"PostContent"},"highlights":[{"type":"id","generated":false,"id":"Quote:anon_a808118a3505","typename":"Quote"}],"latestPublishedVersion":"1d09ac354e16","mediumUrl":"https:\u002F\u002Fmedium.com\u002F@arthur_ouaknine\u002Freview-of-deep-learning-algorithms-for-image-semantic-segmentation-509a600f7b57","readingTime":17.244339622641508,"statusForCollection":null,"visibility":"PUBLIC","allowResponses":true,"tags":[{"type":"id","generated":false,"id":"Tag:machine-learning","typename":"Tag"},{"type":"id","generated":false,"id":"Tag:deep-learning","typename":"Tag"},{"type":"id","generated":false,"id":"Tag:computer-vision","typename":"Tag"},{"type":"id","generated":false,"id":"Tag:object-detection","typename":"Tag"},{"type":"id","generated":false,"id":"Tag:image-classification","typename":"Tag"}],"viewerClapCount":null,"readingList":"READING_LIST_NONE","clapCount":543,"voterCount":101,"recommenders":[],"pendingCollection":null,"responsesCount":4,"collaborators":[],"inResponseToPostResult":null,"inResponseToMediaResource":null,"curationEligibleAt":0,"audioVersionUrl":null,"socialTitle":"","socialDek":"","metaDescription":"","latestPublishedAt":1544527315064,"previewContent":{"type":"id","generated":true,"id":"$Post:509a600f7b57.previewContent","typename":"PreviewContent"},"previewImage":{"type":"id","generated":false,"id":"ImageMetadata:1*O6mEzgZITQA80xHI7UJrUQ.png","typename":"ImageMetadata"},"updatedAt":1544527315064,"topics":[],"isSuspended":false},"User:9690d2dcec12":{"id":"9690d2dcec12","__typename":"User","isSuspended":false,"allowNotes":true,"name":"Arthur Ouaknine","isFollowing":false,"username":"arthur_ouaknine","bio":"PhD Student at Telecom ParisTech and valeo.ai — Deep Learning enthusiast","imageId":"1*aeACCyh-gRb76rWD-vxHxg.jpeg","mediumMemberAt":0,"isBlocking":false,"isPartnerProgramEnrolled":false,"twitterScreenName":""},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}})":{"isLockedPreviewOnly":false,"__typename":"PostContent","bodyModel":{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel","typename":"RichText"}},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.sections.0":{"name":"823a","startIndex":0,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null,"__typename":"Section"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.sections.1":{"name":"a5f5","startIndex":88,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null,"__typename":"Section"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel":{"sections":[{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.sections.0","typename":"Section"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.sections.1","typename":"Section"}],"paragraphs":[{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.0","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.1","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.2","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.3","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.4","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.5","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.6","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.7","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.8","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.9","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.10","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.11","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.12","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.13","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.14","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.15","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.16","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.17","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.18","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.19","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.20","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.21","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.22","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.23","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.24","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.25","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.26","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.27","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.28","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.29","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.30","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.31","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.32","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.33","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.34","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.35","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.36","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.37","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.38","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.39","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.40","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.41","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.42","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.43","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.44","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.45","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.46","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.47","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.48","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.49","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.50","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.51","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.52","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.53","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.54","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.55","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.56","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.57","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.58","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.59","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.60","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.61","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.62","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.63","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.64","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.65","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.66","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.67","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.68","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.69","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.70","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.71","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.72","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.73","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.74","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.75","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.76","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.77","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.78","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.79","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.80","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.81","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.82","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.83","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.84","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.85","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.86","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.87","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.88","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.89","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.90","typename":"Paragraph"}],"__typename":"RichText"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.0":{"name":"1f69","__typename":"Paragraph","type":"H3","href":null,"layout":null,"metadata":null,"text":"Review of Deep Learning Algorithms for Image Semantic Segmentation","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.1":{"name":"ee2f","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*O6mEzgZITQA80xHI7UJrUQ.png","typename":"ImageMetadata"},"text":"Examples of the COCO dataset for stuff segmentation. Souce: http:\u002F\u002Fcocodataset.org\u002F","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.1.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*O6mEzgZITQA80xHI7UJrUQ.png":{"id":"1*O6mEzgZITQA80xHI7UJrUQ.png","originalHeight":466,"originalWidth":427,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.1.markups.0":{"type":"A","start":60,"end":83,"href":"http:\u002F\u002Fcocodataset.org\u002F","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.2":{"name":"d7dc","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Deep learning algorithms have solved several computer vision tasks with an increasing level of difficulty. In my previous blog posts, I have detailled the well kwown ones: image classification and object detection. The image semantic segmentation challenge consists in classifying each pixel of an image (or just several ones) into an instance, each instance (or category) corresponding to an object or a part of the image (road, sky, …). This task is part of the concept of scene understanding: how a deep learning model can better learn the global context of a visual content ?","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.2.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.2.markups.1","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.2.markups.0":{"type":"A","start":172,"end":192,"href":"https:\u002F\u002Fmedium.com\u002Fcomet-app\u002Freview-of-deep-learning-algorithms-for-image-classification-5fdbca4a05e2","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.2.markups.1":{"type":"A","start":197,"end":213,"href":"https:\u002F\u002Fmedium.com\u002Fcomet-app\u002Freview-of-deep-learning-algorithms-for-object-detection-c1f3d437b852","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.3":{"name":"8d74","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"The object detection task has exceeded the image classification task in term of complexity. It consists in creating bounding boxes around the objects contained in an image and classify each one of them. Most of the object detection models use anchor boxes and proposals to detect bounding box around objects. Unfortunately, just a few models take into account the entire context of an image but they only classify a small part of the information. Thus, they can’t provide a full comprehension of a scene.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.4":{"name":"9ddf","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"In order to understand a scene, each visual information has to be associated to an entity while considering the spatial information. Several other challenges have emerged to really understand the actions in a image or a video: keypoint detection, action recognition, video captioning, visual question answering and so on. A better comprehension of the environment will help in many fields. For example, an autonomous car needs to delimitate the roadsides with a high precision in order to move by itself. In robotics, production machines should understand how to grab, turn and put together two different pieces requiring to delimitate the exact shape of the object.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.5":{"name":"4431","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"In this blog post, architecture of a few previous state-of-the-art models on image semantic segmentation challenges are detailed. Note that researchers test their algorithms using different datasets (PASCAL VOC, PASCAL Context, COCO, Cityscapes) which are different between the years and use different metrics of evaluation. Thus the cited performances cannot be directly compared per se. Moreover, the results depend on the pretrained top network (the backbone), the results published in this post correspond to the best scores published in each paper with respect to their test dataset.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.5.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.5.markups.0":{"type":"EM","start":381,"end":387,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.6":{"name":"c343","__typename":"Paragraph","type":"H3","href":null,"layout":null,"metadata":null,"text":"Datasets and Metrics","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.7":{"name":"4e1d","__typename":"Paragraph","type":"H4","href":null,"layout":null,"metadata":null,"text":"PASCAL Visual Object Classes (PASCAL VOC)","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.8":{"name":"8a62","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"The PASCAL VOC dataset (2012) is well-known an commonly used for object detection and segmentation. More than 11k images compose the train and validation datasets while 10k images are dedicated to the test dataset.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.9":{"name":"0631","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"The segmentation challenge is evaluated using the mean Intersection over Union (mIoU) metric. The Intersection over Union (IoU) is a metric also used in object detection to evaluate the relevance of the predicted locations. The IoU is the ratio between the area of overlap and the area of union between the ground truth and the predicted areas. The mIoU is the average between the IoU of the segmented objects over all the images of the test dataset.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.9.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.9.markups.0":{"type":"A","start":50,"end":85,"href":"https:\u002F\u002Fwww.tensorflow.org\u002Fapi_docs\u002Fpython\u002Ftf\u002Fmetrics\u002Fmean_iou","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.10":{"name":"3326","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*nIz4PPDt1603WEOEyg7xsQ.png","typename":"ImageMetadata"},"text":"Examples of the 2012 PASCAL VOC dataset for image segmentation. Source: http:\u002F\u002Fhost.robots.ox.ac.uk\u002Fpascal\u002FVOC\u002Fvoc2012\u002Findex.html","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.10.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*nIz4PPDt1603WEOEyg7xsQ.png":{"id":"1*nIz4PPDt1603WEOEyg7xsQ.png","originalHeight":655,"originalWidth":498,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.10.markups.0":{"type":"A","start":72,"end":129,"href":"http:\u002F\u002Fhost.robots.ox.ac.uk\u002Fpascal\u002FVOC\u002Fvoc2012\u002Findex.html","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.11":{"name":"7de3","__typename":"Paragraph","type":"H4","href":null,"layout":null,"metadata":null,"text":"PASCAL-Context","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.12":{"name":"c074","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"The PASCAL-Context dataset (2014) is an extension of the 2010 PASCAL VOC dataset. It contains around 10k images for training, 10k for validation and 10k for testing. The specificity of this new release is that the entire scene is segmented providing more than 400 categories. Note that the images have been annotated during three months by six in-house annotators.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.13":{"name":"c40c","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"The official evaluation metric of the PASCAL-Context challenge is the mIoU. Several other metrics are published by researches as the pixel Accuracy (pixAcc). Here, the performances will be compared only with the mIoU.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.14":{"name":"9456","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*Nb81B38WgOcQVxLIRzsrFg.png","typename":"ImageMetadata"},"text":"Example of the PASCAL-Context dataset. Source: https:\u002F\u002Fcs.stanford.edu\u002F~roozbeh\u002Fpascal-context\u002F","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.14.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*Nb81B38WgOcQVxLIRzsrFg.png":{"id":"1*Nb81B38WgOcQVxLIRzsrFg.png","originalHeight":676,"originalWidth":1434,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.14.markups.0":{"type":"A","start":47,"end":95,"href":"https:\u002F\u002Fcs.stanford.edu\u002F~roozbeh\u002Fpascal-context\u002F*","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.15":{"name":"c2c8","__typename":"Paragraph","type":"H4","href":null,"layout":null,"metadata":null,"text":"Common Objects in COntext (COCO)","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.16":{"name":"09cb","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"There are two COCO challenges (in 2017 and 2018) for image semantic segmentation (“object detection” and “stuff segmentation”). The “object detection” task consists in segmenting and categorizing objects into 80 categories. The “stuff segmentation” task uses data with large segmented part of the images (sky, wall, grass), they contain almost the entire visual information. In this blog post, only the results of the “object detection” task will be compared because too few of the quoted research papers have published results on the “stuff segmentation” task.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.17":{"name":"2426","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"The COCO dataset for object segmentation is composed of more than 200k images with over 500k object instance segmented. It contains a training dataset, a validation dataset, a test dataset for reseachers (test-dev) and a test dataset for the challenge (test-challenge). The annotations of both test datasets are not available. These datasets contain 80 categories and only the corresponding objects are segmented. This challenge uses the same metrics than the object detection challenge: the Average Precision (AP) and the Average Recall (AR) both using the Intersection over Union (IoU).","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.18":{"name":"abe6","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Details about IoU and AP metrics are available in my previous blog post. Such as the AP, the Average Recall is computed using multiple IoU with a specific range of overlapping values. For a fixed IoU, the objects with the corresponding test \u002F ground truth overlapping are kept. Then the Recall metric is computed for the detected objects. The final AR metric is the average of the computed Recalls for all the IoU range values. Basically the AP and the AR metrics for segmentation works the same way with object detection excepting that the IoU is computed pixel-wise with a non rectangular shape for semantic segmentation.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.18.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.18.markups.0":{"type":"A","start":53,"end":71,"href":"https:\u002F\u002Fmedium.com\u002Fcomet-app\u002Freview-of-deep-learning-algorithms-for-object-detection-c1f3d437b852","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.19":{"name":"087a","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*ltungS3ZQwFEMbs7lDuI6A.png","typename":"ImageMetadata"},"text":"Example of the COCO dataset for object segmentation. Source: http:\u002F\u002Fcocodataset.org\u002F","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.19.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*ltungS3ZQwFEMbs7lDuI6A.png":{"id":"1*ltungS3ZQwFEMbs7lDuI6A.png","originalHeight":153,"originalWidth":755,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.19.markups.0":{"type":"A","start":61,"end":84,"href":"http:\u002F\u002Fcocodataset.org\u002F*","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.20":{"name":"f898","__typename":"Paragraph","type":"H4","href":null,"layout":null,"metadata":null,"text":"Cityscapes","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.21":{"name":"ab2a","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"The Cityscapes dataset has been released in 2016 and consists in complex segmented urban scenes from 50 cities. It is composed of 23.5k images for training and validation (fine and coarse annotations) and 1.5 images for testing (only fine annotation). The images are fully segmented such as the PASCAL-Context dataset with 29 classes (within 8 super categories: flat, human, vehicle, construction, object, nature, sky, void). It is often used to evaluate semantic segmentation models because of its complexity. It is also well known for its similarity with real urban scenes for autonomous driving applications. The performances of semantic segmentation models are computed using the mIoU metric such as the PASCAL datasets.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.22":{"name":"784f","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*sa7Q_BFc087-ISgnlAsk4w.png","typename":"ImageMetadata"},"text":"Examples of the Cityscapes dataset. Top: coarse annotations. Bottom: fine annotation. Source: https:\u002F\u002Fwww.cityscapes-dataset.com\u002F","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.22.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*sa7Q_BFc087-ISgnlAsk4w.png":{"id":"1*sa7Q_BFc087-ISgnlAsk4w.png","originalHeight":345,"originalWidth":1179,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.22.markups.0":{"type":"A","start":94,"end":129,"href":"https:\u002F\u002Fwww.cityscapes-dataset.com\u002F*","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.23":{"name":"36a0","__typename":"Paragraph","type":"H3","href":null,"layout":null,"metadata":null,"text":"Fully Convolutional Network (FCN)","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.24":{"name":"6037","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"J. Long et al. (2015) have been the firsts to develop an Fully Convolutional Network (FCN) (containing only convolutional layers) trained end-to-end for image segmentation.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.24.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.24.markups.0":{"type":"A","start":0,"end":21,"href":"https:\u002F\u002Fpeople.eecs.berkeley.edu\u002F~jonlong\u002Flong_shelhamer_fcn.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.25":{"name":"5b8f","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"The FCN takes an image with an arbitrary size and produces a segmented image with the same size. The authors start by modifying well-known architectures (AlexNet, VGG16, GoogLeNet) to have a non fixed size input while replacing all the fully connected layers by convolutional layers. Since the network produces several feature maps with small sizes and dense representations, an upsampling is necessary to create an output with the same size than the input. Basically, it consists in a convolutional layer with a stride inferior to 1. It is commonly called deconvolution because it creates an output with a larger size than the input. This way, the network is trained using a pixel-wise loss. Moreover they have added skip connections in the network to combine high level feature map representations with more specific and dense ones at the top of the network.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.25.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.25.markups.0":{"type":"STRONG","start":557,"end":570,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.26":{"name":"d7e5","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"The authors have reached a 62.2% mIoU score on the 2012 PASCAL VOC segmentation challenge using pretrained models on the 2012 ImageNet dataset. For the 2012 PASCAL VOC object detection challenge, the benchmark model called Faster R-CNN has reached 78.8% mIoU. Even if we can’t directly compare the two results (different models, different datasets and different challenges), it seems that the semantic segmentation task is more difficult to solve than the object detection task.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.27":{"name":"d862","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*pXYT1g8aWtadPBVVv9LuuA.png","typename":"ImageMetadata"},"text":"Architecture of the FCN. Note that the skip connections are not drawn here. Souce: J. Long et al. (2015)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.27.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*pXYT1g8aWtadPBVVv9LuuA.png":{"id":"1*pXYT1g8aWtadPBVVv9LuuA.png","originalHeight":230,"originalWidth":456,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.27.markups.0":{"type":"A","start":83,"end":104,"href":"https:\u002F\u002Fpeople.eecs.berkeley.edu\u002F~jonlong\u002Flong_shelhamer_fcn.pdf).","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.28":{"name":"1d34","__typename":"Paragraph","type":"H3","href":null,"layout":null,"metadata":null,"text":"ParseNet","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.29":{"name":"18cc","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"W. Liu et al. (2015) have published a paper explaining improvements of the FCN model of J. Long et al. (2015). According to the authors, the FCN model loses the global context of the image in its deep layers by specializing the generated feature maps. The ParseNet is an end-to-end convolutional network predicting values for all the pixels at the same time and it avoids taking regions as input to keep the global information. The authors use a module taking feature maps as input. The first step uses a model to generate feature maps which are reduced into a single global feature vector with a pooling layer. This context vector is normalised using the L2 Euclidian Norm and it is unpooled (the output is an expanded version of the input) to produce new feature maps with the same sizes than the inital ones. The second step normalises the entire initial feature maps using the L2 Euclidian Norm. The last step concatenates the feature maps generated by the two previous steps. The normalisation is helpful to scale the concatenated feature maps values and it leads to better performances. Basically, the ParseNet is a FCN with this module replacing convolutional layers. It has obtained a 40.4% mIoU score on the PASCAL-Context challenge and a 69.8% mIoU score on the 2012 PASCAL VOC segmentation challenge.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.29.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.29.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.29.markups.2","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.29.markups.0":{"type":"A","start":0,"end":20,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1506.04579.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.29.markups.1":{"type":"A","start":88,"end":109,"href":"https:\u002F\u002Fpeople.eecs.berkeley.edu\u002F~jonlong\u002Flong_shelhamer_fcn.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.29.markups.2":{"type":"A","start":656,"end":673,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FNorm_(mathematics)","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.30":{"name":"4cff","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*4LWdZErZ-ocldECLEB42og.png","typename":"ImageMetadata"},"text":"Comparison between the segmentation of the FCN and the ParseNet and architecture of the ParseNet module. Source: W. Liu et al. (2015)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.30.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*4LWdZErZ-ocldECLEB42og.png":{"id":"1*4LWdZErZ-ocldECLEB42og.png","originalHeight":169,"originalWidth":683,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.30.markups.0":{"type":"A","start":113,"end":133,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1506.04579.pdf)","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.31":{"name":"66ae","__typename":"Paragraph","type":"H3","href":null,"layout":null,"metadata":null,"text":"Convolutional and Deconvolutional Networks","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.32":{"name":"6966","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"H. Noh et al. (2015) have released an end-to-end model composed of two linked parts. The first part is a convolutional network with a VGG16 architecture. It takes as input an instance proposal, for example a bounding box generated by an object detection model. The proposal is processed and transformed by a convolutional network to generate a vector of features. The second part is a deconvolutional network taking the vector of features as input and generating a map of pixel-wise probabilities belonging to each class. The deconvolutional network uses unpooling targeting the maxium activations to keep the location of the information in the maps. The second network also uses deconvolution associating a single input to multiple feature maps. The deconvolution expands feature maps while keeping the information dense.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.32.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.32.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.32.markups.2","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.32.markups.0":{"type":"A","start":0,"end":20,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1505.04366.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.32.markups.1":{"type":"STRONG","start":385,"end":408,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.32.markups.2":{"type":"STRONG","start":555,"end":564,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.33":{"name":"8c5a","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*6jJcV8_1J-s7eYl3jnXWYA.png","typename":"ImageMetadata"},"text":"Comparison of the convolutional network layers (pooling and convolution) with the deconvolutional network layers (unpooling and deconvolution). Source: H. Noh et al. (2015)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.33.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*6jJcV8_1J-s7eYl3jnXWYA.png":{"id":"1*6jJcV8_1J-s7eYl3jnXWYA.png","originalHeight":283,"originalWidth":426,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.33.markups.0":{"type":"A","start":152,"end":172,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1505.04366.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.34":{"name":"4825","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"The authors have analysed deconvolution feature maps and they have noted that the low-level ones are specific to the shape while the higher-level ones help to classify the proposal. Finally, when all the proposals of an image are processed by the entire network, the maps are concatenated to obtain the fully segmented image. This network has obtained a 72.5% mIoU on the 2012 PASCAL VOC segmentation challenge.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.35":{"name":"c023","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*XPg1eIVN2i4x4Nrqw0gPhA.png","typename":"ImageMetadata"},"text":"Architecture of the full network. The convolution network is based on the VGG16 architecture. The deconvolution network uses unpooling and deconvolution layers. Source: H. Noh et al. (2015)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.35.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*XPg1eIVN2i4x4Nrqw0gPhA.png":{"id":"1*XPg1eIVN2i4x4Nrqw0gPhA.png","originalHeight":223,"originalWidth":872,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.35.markups.0":{"type":"A","start":169,"end":189,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1505.04366.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.36":{"name":"91d4","__typename":"Paragraph","type":"H3","href":null,"layout":null,"metadata":null,"text":"U-Net","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.37":{"name":"8767","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"O. Ronneberger et al. (2015) have extended the FCN of J. Long et al. (2015) for biological microscopy images. The authors have created a network called U-net composed in two parts: a contracting part to compute features and a expanding part to spatially localise patterns in the image. The downsampling or contracting part has a FCN-like archicture extracting features with 3x3 convolutions. The upsampling or expanding part uses up-convolution (or deconvolution) reducing the number of feature maps while increasing their height and width. Cropped feature maps from the downsampling part of the network are copied within the upsampling part to avoid loosing pattern information. Finally, a 1x1 convolution processes the feature maps to generate a segmentation map and thus categorise each pixel of the input image. Since then, the U-net architecture has been widely extended in recent works (FPN, PSPNet, DeepLabv3 and so on). Note that it doesn’t use any fully-connected layer. As consequencies, the number of parameters of the model is reduced and it can be trained with a small labelled dataset (using appropriate data augmentation). For example, the authors have used a public dataset with 30 images for training during their experiments.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.37.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.37.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.37.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.37.markups.3","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.37.markups.0":{"type":"A","start":0,"end":28,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1505.04597.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.37.markups.1":{"type":"A","start":54,"end":75,"href":"https:\u002F\u002Fpeople.eecs.berkeley.edu\u002F~jonlong\u002Flong_shelhamer_fcn.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.37.markups.2":{"type":"STRONG","start":290,"end":302,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.37.markups.3":{"type":"STRONG","start":396,"end":406,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.38":{"name":"8198","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*mi0MoA6CaqTdiYDXMPbhMQ.png","typename":"ImageMetadata"},"text":"Architecture of the U-net for a given input image. The blue boxes correspond to feature maps blocks with their denoted shapes. The white boxes correspond to the copied and cropped feature maps. Source: O. Ronneberger et al. (2015)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.38.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*mi0MoA6CaqTdiYDXMPbhMQ.png":{"id":"1*mi0MoA6CaqTdiYDXMPbhMQ.png","originalHeight":387,"originalWidth":580,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.38.markups.0":{"type":"A","start":202,"end":230,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1505.04597.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.39":{"name":"44f0","__typename":"Paragraph","type":"H3","href":null,"layout":null,"metadata":null,"text":"Feature Pyramid Network (FPN)","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.40":{"name":"5551","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"The Feature Pyramid Network (FPN) has been developped by T.-Y. Lin et al (2016) and it is used in object detection or image segmentation frameworks. Its architecture is composed of a bottom-up pathway, a top-down pathway and lateral connections in order to join low-resolution and high-resolution features. The bottom-up pathway takes an image with an arbitrary size as input. It is processed with convolutional layers and downsampled by pooling layers. Note that each bunch of feature maps with the same size is called a stage, the outputs of the last layer of each stage are the features used for the pyramid level. The top-down pathway consists in upsampling the last feature maps with unpooling while enhancing them with feature maps from the same stage of the bottom-up pathway using lateral connections. These connections consist in merging the feature maps of the bottom-up pathway processed with a 1x1 convolution (to reduce their dimensions) with the feature maps of the top-down pathway.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.40.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.40.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.40.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.40.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.40.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.40.markups.5","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.40.markups.0":{"type":"A","start":57,"end":79,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1612.03144.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.40.markups.1":{"type":"STRONG","start":311,"end":328,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.40.markups.2":{"type":"STRONG","start":522,"end":527,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.40.markups.3":{"type":"STRONG","start":603,"end":616,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.40.markups.4":{"type":"STRONG","start":622,"end":638,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.40.markups.5":{"type":"STRONG","start":789,"end":808,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.41":{"name":"0a30","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*R7r8K9Ke6QOuKyzLbwQHnA.png","typename":"ImageMetadata"},"text":"Detail of a top-down block process with the lateral connection and the sum of the feature maps. Source: T.-Y. Lin et al (2016)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.41.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*R7r8K9Ke6QOuKyzLbwQHnA.png":{"id":"1*R7r8K9Ke6QOuKyzLbwQHnA.png","originalHeight":227,"originalWidth":269,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.41.markups.0":{"type":"A","start":104,"end":126,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1612.03144.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.42":{"name":"fd1b","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"The concatenated feature maps are then processed by a 3x3 convolution to produce the output of the stage. Finally, each stage of the top-down pathway generates a prediction to detect an object. For image segmentation, the authors uses two Multi-Layer Perceptrons (MLP) to generate two masks with different size over the objets. It works similarly to Region Proposal Networks with anchor boxes (R-CNN R. Girshick et al. (2014), Fast R-CNN R. Girshick et al. (2015), Faster R-CNN S. Ren et al. (2016) and so on). This method is efficient because it better propagates low information into the network. The FPN based on DeepMask (P. 0. Pinheiro et al. (2015)) and SharpMask (P. 0. Pinheiro et al. (2016)) frameworks achieved a 48.1% Average Recall (AR) score on the 2016 COCO segmentation challenge.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.42.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.42.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.42.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.42.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.42.markups.4","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.42.markups.0":{"type":"A","start":400,"end":425,"href":"http:\u002F\u002Fislab.ulsan.ac.kr\u002Ffiles\u002Fannouncement\u002F513\u002Frcnn_pami.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.42.markups.1":{"type":"A","start":438,"end":463,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1504.08083.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.42.markups.2":{"type":"A","start":478,"end":498,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1506.01497.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.42.markups.3":{"type":"A","start":626,"end":654,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1506.06204.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.42.markups.4":{"type":"A","start":671,"end":699,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1603.08695.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.43":{"name":"947e","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*vUHHpEPy2Os1szpb5wDyXg.png","typename":"ImageMetadata"},"text":"Comparison of architectures. (a): The image is scaled with several sizes and each one is processed with convolutions to provide predictions which is computationally expansive. (b): The image has a single scale processed by a CNN with convolution an pooling layers. © Each step of the CNN is used to provide a prediction. (d) Architecture of the FPN with the bottom-up part of the left and the top-down part on the right. Source: T.-Y. Lin et al (2016)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.43.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*vUHHpEPy2Os1szpb5wDyXg.png":{"id":"1*vUHHpEPy2Os1szpb5wDyXg.png","originalHeight":250,"originalWidth":428,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.43.markups.0":{"type":"A","start":429,"end":451,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1612.03144.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.44":{"name":"cf43","__typename":"Paragraph","type":"H3","href":null,"layout":null,"metadata":null,"text":"Pyramid Scene Parsing Network (PSPNet)","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.45":{"name":"00da","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"H. Zhao et al. (2016) have developped the Pyramid Scene Parsing Network (PSPNet) to better learn the global context representation of a scene. Patterns are extracted from the input image using a feature extractor (ResNet K. He et al. (2015)) with a dilated network strategy¹. The feature maps feed a Pyramid Pooling Module to distinguish patterns with different scales. They are pooled with four different scales each one corresponding to a pyramid level and processed by a 1x1 convolutional layer to reduce their dimensions. This way each pyramid level analyses sub-regions of the image with different location. The outputs of the pyramid levels are upsampled and concatenated to the inital feature maps to finally contain the local and the global context information. Then, they are processed by a convolutional layer to generate the pixel-wise predictions. The best PSPNet with a pretrained ResNet (using the COCO dataset) has reached a 85.4% mIoU score on the 2012 PASCAL VOC segmentation challenge.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.45.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.45.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.45.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.45.markups.3","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.45.markups.0":{"type":"A","start":0,"end":21,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1612.01105.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.45.markups.1":{"type":"A","start":221,"end":240,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1512.03385.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.45.markups.2":{"type":"STRONG","start":249,"end":273,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.45.markups.3":{"type":"STRONG","start":300,"end":322,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.46":{"name":"c800","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*Tp9s_0BrmxeIHIyQbg_vLw.png","typename":"ImageMetadata"},"text":"PSPNet architecture. The input image (a) is processed by a CNN to generate feature maps (b). They feed a Pyramid Pooling Module © and a final convolutional layer generates the pixel-wise predictions. Source: H. Zhao et al. (2016)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.46.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*Tp9s_0BrmxeIHIyQbg_vLw.png":{"id":"1*Tp9s_0BrmxeIHIyQbg_vLw.png","originalHeight":258,"originalWidth":874,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.46.markups.0":{"type":"A","start":208,"end":229,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1612.01105.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.47":{"name":"8793","__typename":"Paragraph","type":"H3","href":null,"layout":null,"metadata":null,"text":"Mask R-CNN","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.48":{"name":"ecfe","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"K. He et al. (2017) have released the Mask R-CNN model beating all previous benchmarks on many COCO challenges². I have already provided details about Mask R-CNN for object detection in my previous blog post. As a reminder, the Faster R-CNN (S. Ren et al. (2015)) architecture for object detection uses a Region Proposal Network (RPN) to propose bounding box candidates. The RPN extracts Region of Interest (RoI) and a RoIPool layer computes features from these proposals in order to infer the bounding box cordinates and the class of the object. The Mask R-CNN is a Faster R-CNN with 3 output branches: the first one computes the bounding box coordinates, the second one computes the associated class and the last one computes the binary mask³ to segment the object. The binary mask has a fixed size and it is generated by a FCN for a given RoI. It also uses a RoIAlign layer instead of a RoIPool to avoid misalignments due to the quantization of the RoI coordinates. The particularity of the Mask R-CNN model is its multi-task loss combining the losses of the bounding box coordinates, the predicted class and the segmentation mask. The model tries to solve complementary tasks leading to better performances on each individual task. The best Mask R-CNN uses a ResNeXt (S. Xie et al. (2016)) to extract features and a FPN architecture. It has obtained a 37.1% AP score on the 2016 COCO segmentation challenge and a 41.8% AP score on the 2017 COCO segmentation challenge.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.48.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.48.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.48.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.48.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.48.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.48.markups.5","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.48.markups.6","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.48.markups.7","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.48.markups.8","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.48.markups.0":{"type":"A","start":0,"end":19,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1703.06870.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.48.markups.1":{"type":"A","start":189,"end":207,"href":"https:\u002F\u002Fmedium.com\u002Fcomet-app\u002Freview-of-deep-learning-algorithms-for-object-detection-c1f3d437b852","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.48.markups.2":{"type":"A","start":242,"end":262,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1506.01497.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.48.markups.3":{"type":"A","start":1272,"end":1292,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1611.05431.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.48.markups.4":{"type":"STRONG","start":305,"end":328,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.48.markups.5":{"type":"STRONG","start":388,"end":406,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.48.markups.6":{"type":"STRONG","start":419,"end":426,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.48.markups.7":{"type":"STRONG","start":862,"end":870,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.48.markups.8":{"type":"STRONG","start":1018,"end":1033,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.49":{"name":"2379","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*IPB0s9fCiXlGpM_S6Y8rWQ.png","typename":"ImageMetadata"},"text":"Mask R-CNN achitecture. The first layer is a RPN extracting the RoI. The second layer processes the RoI to generate feature maps. They are directly used to compute the bounding box coordinates and the predicted class. The feature maps are also processed by an FCN (third layer) to generate the binary mask. Source: K. He et al. (2017)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.49.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*IPB0s9fCiXlGpM_S6Y8rWQ.png":{"id":"1*IPB0s9fCiXlGpM_S6Y8rWQ.png","originalHeight":198,"originalWidth":432,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.49.markups.0":{"type":"A","start":315,"end":334,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1703.06870.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.50":{"name":"3c68","__typename":"Paragraph","type":"H3","href":null,"layout":null,"metadata":null,"text":"DeepLab, DeepLabv3 and DeepLabv3+","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.51":{"name":"ebc6","__typename":"Paragraph","type":"H4","href":null,"layout":null,"metadata":null,"text":"DeepLab","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.52":{"name":"9289","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Inspired by the FPN model of T.-Y. Lin et al (2016), L.-C. Chen et al. (2017) have released DeepLab combining atrous convolution, spatial pyramid pooling and fully connected CRFs. The model presented in this paper is also called the DeepLabv2 because it is an adjustment of the initial DeepLab model (details about the inital one will not be provided to avoid redundancy). According to the authors, consecutive max-pooling and striding reduces the resolution of the feature maps in deep neural networks. They have introduced the atrous convolution which is basically the dilated convolution of H. Zhao et al. (2016). It consists of filters targeting sparse pixels with a fixed rate. For example, if the rate is equal to 2, the filter targets one pixel over two in the input; if the rate equal to 1, the atrous convolution is a basic convolution. Atrous convolution permits to capture multiple scale of objects. When it is used without max-poolling, it increases the resolution of the final output without increasing the number of weights.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.52.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.52.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.52.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.52.markups.3","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.52.markups.0":{"type":"A","start":29,"end":51,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1612.03144.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.52.markups.1":{"type":"A","start":53,"end":77,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1606.00915.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.52.markups.2":{"type":"A","start":594,"end":615,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1612.01105.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.52.markups.3":{"type":"STRONG","start":529,"end":547,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.53":{"name":"c397","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*3UoUqTo_QjrcszzF_LRLfA.png","typename":"ImageMetadata"},"text":"Extraction patterns comparison between standard convolution on a low resolution input (top) and atrous convolution with a rate of 2 on a high resolution input (bottom). Source: L.-C. Chen et al. (2017)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.53.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*3UoUqTo_QjrcszzF_LRLfA.png":{"id":"1*3UoUqTo_QjrcszzF_LRLfA.png","originalHeight":353,"originalWidth":552,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.53.markups.0":{"type":"A","start":177,"end":201,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1606.00915.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.54":{"name":"1c7f","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"The Atrous Spatial Pyramid Pooling consists in applying several atrous convolution of the same input with different rate to detect spatial patterns. The features maps are processed in separate branches and concatenated using bilinear interpolation to recovert the original size of the input. The output feeds a fully connected Conditional Random Field (CRF) (Krähenbühl and V. Koltun (2012)) computing edges between the features and long terme dependencies to produce the semantic segmentation.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.54.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.54.markups.1","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.54.markups.0":{"type":"A","start":359,"end":390,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1210.5644.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.54.markups.1":{"type":"STRONG","start":4,"end":34,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.55":{"name":"ff33","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*iKGyx6XR2EmIutDp1Uy5bw.png","typename":"ImageMetadata"},"text":"Atrous Spatial Pyramid Pooling (ASPP) exploiting multiple scale of objects to classify the pixel in the center. Source: L.-C. Chen et al. (2017)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.55.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*iKGyx6XR2EmIutDp1Uy5bw.png":{"id":"1*iKGyx6XR2EmIutDp1Uy5bw.png","originalHeight":312,"originalWidth":612,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.55.markups.0":{"type":"A","start":120,"end":144,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1606.00915.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.56":{"name":"19ee","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"The best DeepLab using a ResNet-101 as backbone has reached a 79.7% mIoU score on the 2012 PASCAL VOC challenge, a 45.7% mIoU score on the PASCAL-Context challenge and a 70.4% mIoU score on the Cityscapes challenge.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.57":{"name":"d956","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*fNCnDjexDIJsDdmvf217ew.png","typename":"ImageMetadata"},"text":"DeepLab framework. Source: L.-C. Chen et al. (2017)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.57.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*fNCnDjexDIJsDdmvf217ew.png":{"id":"1*fNCnDjexDIJsDdmvf217ew.png","originalHeight":405,"originalWidth":866,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.57.markups.0":{"type":"A","start":27,"end":51,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1606.00915.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.58":{"name":"ce1e","__typename":"Paragraph","type":"H4","href":null,"layout":null,"metadata":null,"text":"DeepLabv3","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.59":{"name":"2953","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"L.-C. Chen et al. (2017) have revisited the DeepLab framework to create DeepLabv3 combining cascaded and parallel modules of atrous convolutions. The authors have modified the ResNet architecture to keep high resolution feature maps in deep blocks using atrous convolutions.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.59.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.59.markups.0":{"type":"A","start":0,"end":24,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1706.05587.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.60":{"name":"df4a","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*JtVdwr11eTQnqukZ1YeQnA.png","typename":"ImageMetadata"},"text":"Cascaded modules in the ResNet architecture. Source: L.-C. Chen et al. (2017)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.60.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*JtVdwr11eTQnqukZ1YeQnA.png":{"id":"1*JtVdwr11eTQnqukZ1YeQnA.png","originalHeight":287,"originalWidth":796,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.60.markups.0":{"type":"A","start":53,"end":77,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1706.05587.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.61":{"name":"63d6","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"The parallel atrous convolution modules are grouped in the Atrous Spatial Pyramid Pooling (ASPP). A 1x1 convolution and batch normalisation are added in the ASPP. All the outputs are concatenated and processed by another 1x1 convolution to create the final output with logits for each pixel.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.62":{"name":"02bb","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*R7tiLxyeHYHMXTGJIanZiA.png","typename":"ImageMetadata"},"text":"Atrous Spatial Pyramid Pooling in the Deeplabv3 framework. Source: L.-C. Chen et al. (2017)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.62.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*R7tiLxyeHYHMXTGJIanZiA.png":{"id":"1*R7tiLxyeHYHMXTGJIanZiA.png","originalHeight":206,"originalWidth":795,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.62.markups.0":{"type":"A","start":67,"end":91,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1706.05587.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.63":{"name":"da3a","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"The best DeepLabv3 model with a ResNet-101 pretrained on ImageNet and JFT-300M datasets has reached 86.9% mIoU score in the 2012 PASCAL VOC challenge. It also achieved a 81.3% mIoU score on the Cityscapes challenge with a model only trained with the associated training dataset.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.64":{"name":"c295","__typename":"Paragraph","type":"H4","href":null,"layout":null,"metadata":null,"text":"DeepLabv3+","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.65":{"name":"94d0","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"L.-C. Chen et al. (2018) have finally released the Deeplabv3+ framework using an encoder-decoder structure. The authors have introduced the atrous separable convolution composed of a depthwise convolution (spatial convolution for each channel of the input) and pointwise convolution (1x1 convolution with the depthwise convolution as input).","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.65.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.65.markups.1","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.65.markups.0":{"type":"A","start":0,"end":24,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1802.02611.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.65.markups.1":{"type":"STRONG","start":140,"end":168,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.66":{"name":"3f26","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*c5rVU1iEJ0jk69DEPyB1-Q.png","typename":"ImageMetadata"},"text":"Combinaison of Depthwise convolution (a) and Pointwise convolution (b) to create Atrous Separable Convolution (with a rate of 2). Source: L.-C. Chen et al. (2018)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.66.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*c5rVU1iEJ0jk69DEPyB1-Q.png":{"id":"1*c5rVU1iEJ0jk69DEPyB1-Q.png","originalHeight":149,"originalWidth":670,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.66.markups.0":{"type":"A","start":138,"end":162,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1802.02611.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.67":{"name":"6d49","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"They have used the DeepLabv3 framework as encoder. The most performant model has a modified Xception (F. Chollet (2017)) backbone with more layers, atrous depthwise separable convolutions instead of max pooling and batch normalization. The outputs of the ASPP are processed by a 1x1 convolution and upsampled by a factor of 4. The outputs of the encoder backbone CNN are also processed by another 1x1 convolution and concatenated to the previous ones. The feature maps feed two 3x3 convolutional layers and the outputs are upsampled by a factor of 4 to create the final segmented image.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.67.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.67.markups.0":{"type":"A","start":102,"end":119,"href":"https:\u002F\u002Farxiv.org\u002Fabs\u002F1610.02357","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.68":{"name":"c0f8","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*09azMblllhMGJgZuDeJrzw.png","typename":"ImageMetadata"},"text":"DeepLabv3+ framework: an encoder with a backbone CNN and an ASPP produces feature representations to feed a decoder with 3x3 convolutions producing the final predicted image. Source: L.-C. Chen et al. (2018)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.68.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*09azMblllhMGJgZuDeJrzw.png":{"id":"1*09azMblllhMGJgZuDeJrzw.png","originalHeight":370,"originalWidth":718,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.68.markups.0":{"type":"A","start":183,"end":207,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1802.02611.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.69":{"name":"fd5f","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"The best DeepLabv3+ pretrained on the COCO and the JFT datasets has obtained a 89.0% mIoU score on the 2012 PASCAL VOC challenge. The model trained on the Cityscapes dataset has reached a 82.1% mIoU score for the associated challenge.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.70":{"name":"3ef9","__typename":"Paragraph","type":"H3","href":null,"layout":null,"metadata":null,"text":"Path Aggregation Network (PANet)","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.71":{"name":"edf6","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"S. Liu et al. (2018) have recently released the Path Aggregation Network (PANet). This network is based on the Mask R-CNN and the FPN frameworks while enhancing information propagation. The feature extractor of the network uses a FPN architecture with a new augmented bottom-up pathway improving the propagation of low-layer features. Each stage of this third pathway takes as input the feature maps of the previous stage and processes them with a 3x3 convolutional layer. The output is added to the same stage feature maps of the top-down pathway using lateral connection and these feature maps feed the next stage.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.71.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.71.markups.1","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.71.markups.0":{"type":"A","start":0,"end":20,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1803.01534.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.71.markups.1":{"type":"STRONG","start":258,"end":285,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.72":{"name":"b39b","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*WmN0JUMwo2WaQnorPQ2zIg.png","typename":"ImageMetadata"},"text":"Lateral connection between the top-down pathway and the augmented bottom-up pathway. Source: S. Liu et al. (2018)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.72.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*WmN0JUMwo2WaQnorPQ2zIg.png":{"id":"1*WmN0JUMwo2WaQnorPQ2zIg.png","originalHeight":156,"originalWidth":264,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.72.markups.0":{"type":"A","start":93,"end":113,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1803.01534.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.73":{"name":"1797","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"The feature maps of the augmented bottom-up pathway are pooled with a RoIAlign layer to extract proposals from all level features. An adaptative feature pooling layer processes the features maps of each stage with a fully connected layer and concatenate all the outputs.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.73.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.73.markups.0":{"type":"STRONG","start":134,"end":160,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.74":{"name":"86f1","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*6qnH-e2HH-gYBaV_vBqzxA.png","typename":"ImageMetadata"},"text":"Adatative feature pooling layer. Source: S. Liu et al. (2018)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.74.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*6qnH-e2HH-gYBaV_vBqzxA.png":{"id":"1*6qnH-e2HH-gYBaV_vBqzxA.png","originalHeight":183,"originalWidth":403,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.74.markups.0":{"type":"A","start":41,"end":61,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1803.01534.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.75":{"name":"7a1f","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"The output of the adaptative feature pooling layer feeds three branches similarly to the Mask R-CNN. The two first branches uses a fully connected layer to generate the predictions of the bounding box coordinates and the associated object class. The third branch process the RoI with a FCN to predict a binary pixel-wise mask for the detected object. The authors have added a path processing the output of a convolutional layer of the FCN with a fully connected layer to improve the localisation of the predicted pixels. Finally the output of the parallel path is reshaped and concatenated to the output of the FCN generating the binary mask.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.75.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.75.markups.0":{"type":"STRONG","start":293,"end":325,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.76":{"name":"ae6b","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*yHoAq_qBXoJeIDU_fSgtAw.png","typename":"ImageMetadata"},"text":"Branch of the PANet predicting the binary mask using a FCN and a new path with a fully connected layer. Source: https:\u002F\u002Farxiv.org\u002Fpdf\u002F1803.01534.pdf","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.76.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*yHoAq_qBXoJeIDU_fSgtAw.png":{"id":"1*yHoAq_qBXoJeIDU_fSgtAw.png","originalHeight":202,"originalWidth":390,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.76.markups.0":{"type":"A","start":112,"end":148,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1803.01534.pdf)*","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.77":{"name":"7424","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"The PANet has achieved 42.0% AP score on the 2016 COCO segmentation challenge using a ResNeXt as feature extractor. They also performed the 2017 COCO segmentation challenge with an 46.7% AP score using a ensemble of seven feature extractors: ResNet (K. He et al. (2015), ResNeXt (S. Xie et al. (2016)) and SENet (J. Hu et al.(2017)).","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.77.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.77.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.77.markups.2","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.77.markups.0":{"type":"A","start":250,"end":269,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1512.03385.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.77.markups.1":{"type":"A","start":280,"end":300,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1611.05431.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.77.markups.2":{"type":"A","start":313,"end":331,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1709.01507.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.78":{"name":"36e1","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*hxQWJjNxADU6VlQ5xnSGDw.png","typename":"ImageMetadata"},"text":"PANet Achitecture. (a): Feature extractor using the FPN achitecture. (b): The new augmented bottom-up pathway added to the FPN architecture. ©: The adaptative feature pooling layer. (d): The two branches predicting the bounding box coordinated and the object class. (e): The branch predicting the binary mask of the object. The dashed lines correspond to links between low-level and high level patterns, the red one is in the FPN and consists in more than 100 layers, the green one is a shortcut in the PANet consisting of less than 10 layers. Source: S. Liu et al. (2018)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.78.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*hxQWJjNxADU6VlQ5xnSGDw.png":{"id":"1*hxQWJjNxADU6VlQ5xnSGDw.png","originalHeight":260,"originalWidth":767,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.78.markups.0":{"type":"A","start":552,"end":572,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1803.01534.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.79":{"name":"45fe","__typename":"Paragraph","type":"H3","href":null,"layout":null,"metadata":null,"text":"Context Encoding Network (EncNet)","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.80":{"name":"20c0","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"H. Zhang et al. (2018) have created a Context Encoding Network (EncNet) capturing global information in an image to improve scene segmentation. The model starts by using a basic feature extractor (ResNet) and feeds the feature maps into a Context Encoding Module inspired from the Encoding Layer of H. Zhang et al. (2016). Basically, it learns visual centers and smoothing factors to create an embedding taking into account the contextual information while highlighting class-dependant feature maps. On top of the module, scaling factors for the contextual information are learnt with a feature maps attention layer (fully connected layer). In parallel, a Semantic Encoding Loss (SE-Loss) corresponding to a binary cross-entropy loss regularizes the training of the module by detecting presence of object classes (unlike the pixel-wise loss). The outputs of the Context Encoding Module are reshaped and processed by a dilated convolution strategy while minimizing two SE-losses and a final pixel-wise loss. The best EncNet has reached 52.6% mIoU and 81.2% pixAcc scores on the PASCAL-Context challenge. It has also achieved a 85.9% mIoU score on the 2012 PASCAL VOC segmentation challenge.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.80.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.80.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.80.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.80.markups.3","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.80.markups.0":{"type":"A","start":0,"end":22,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1803.08904.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.80.markups.1":{"type":"A","start":299,"end":321,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1612.02844.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.80.markups.2":{"type":"STRONG","start":239,"end":262,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.80.markups.3":{"type":"STRONG","start":656,"end":678,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.81":{"name":"e99c","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*akweXy62Lay-Qqx7UpiUPA.png","typename":"ImageMetadata"},"text":"Dilated convolution strategy. In blue the convolutional filter with D the dilatation rate. The SE-losses (Semantic Encoding Loss) are applied after the third and the fourth stages to detect object classes. A final Seg-loss (pixel-wise loss) is applied to improve the segmentation. Source: H. Zhang et al. (2018)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.81.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*akweXy62Lay-Qqx7UpiUPA.png":{"id":"1*akweXy62Lay-Qqx7UpiUPA.png","originalHeight":149,"originalWidth":419,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.81.markups.0":{"type":"A","start":289,"end":311,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1803.08904.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.82":{"name":"eed2","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*TCsoZHK71wSXy5MbT8JGDw.png","typename":"ImageMetadata"},"text":"Architecture of the EncNet. A feature extractor generates feature maps took as input of a Context Encoding Module. The module is trained with regularisation using the Semantic Encoding Loss. The outputs of the module are processed by a dilated convolution strategy to produce the final segmention. Source: [H. Zhang et al. (2018)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.82.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*TCsoZHK71wSXy5MbT8JGDw.png":{"id":"1*TCsoZHK71wSXy5MbT8JGDw.png","originalHeight":246,"originalWidth":850,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.82.markups.0":{"type":"A","start":307,"end":329,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1803.08904.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.83":{"name":"b45b","__typename":"Paragraph","type":"H3","href":null,"layout":null,"metadata":null,"text":"Conclusion","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.84":{"name":"61c2","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Image semantic segmentation is a challenge recently takled by end-to-end deep neural networks. One of the main issue between all the architectures is to take into account the global visual context of the input to improve the prediction of the segmentation. The state-of-the-art models use architectures trying to link different part of the image in order to understand the relations between the objects.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.85":{"name":"96af","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*MsVAhHvNuFARowPZRiGiJQ.png","typename":"ImageMetadata"},"text":"Overview of the scores of the models over the 2012 PASCAL VOC dataset (mIoU), the PASCAL-Context dataset (mIoU), the 2016 \u002F 2017 COCO datasets (AP and AR) and the Cityscapes dataset (mIoU)","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*MsVAhHvNuFARowPZRiGiJQ.png":{"id":"1*MsVAhHvNuFARowPZRiGiJQ.png","originalHeight":539,"originalWidth":968,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.86":{"name":"f98e","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"The pixel-wise prediction over an entire image allows a better comprehension of the environement with a high precision. Scene understanding is also approached with keypoint detection, action recognition, video captioning or visual question answering. To my opinion, the segmentation task combined with these other issues using multi-task loss should help to outperform the global context understanding of a scene.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.87":{"name":"256d","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Finally, I would like to thanks Long Do Cao for helping me with all my posts, you should check his profile if you’re looking for a great senior data scientist ;).","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.87.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.87.markups.0":{"type":"A","start":32,"end":43,"href":"https:\u002F\u002Fldocao.wordpress.com\u002F","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.88":{"name":"f28d","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"¹: The dilated convolutional layer has been released by [F. Yu and V. Koltun (2015)](https:\u002F\u002Farxiv.org\u002Fpdf\u002F1511.07122.pdf). It is a convolutional layer with expanded filter (the neurons of the filter are no more side-by-side). A dilatation rate fixes the gap between two neurons in term of pixel. More details are provided in the DeepLab section.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.88.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.88.markups.0":{"type":"A","start":85,"end":121,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1511.07122.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.89":{"name":"aaa2","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"²: Object detection, object segmentation and keypoint detection.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:509a600f7b57.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.paragraphs.90":{"name":"9c79","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"³: The Mask R-CNN model compute a binary mask for an object for a predicted class (instance-first strategy) instead of classifying each pixel into a category (segmentation-first strategy).","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Quote:anon_a808118a3505":{"id":"anon_a808118a3505","paragraphs":[{"type":"id","generated":true,"id":"Quote:anon_a808118a3505.paragraphs.0","typename":"Paragraph"}],"__typename":"Quote","userId":"anon","startOffset":0,"endOffset":132,"user":null},"Quote:anon_a808118a3505.paragraphs.0":{"name":"9ddf","__typename":"Paragraph"},"Tag:machine-learning":{"id":"machine-learning","displayTitle":"Machine Learning","__typename":"Tag"},"Tag:deep-learning":{"id":"deep-learning","displayTitle":"Deep Learning","__typename":"Tag"},"Tag:computer-vision":{"id":"computer-vision","displayTitle":"Computer Vision","__typename":"Tag"},"Tag:object-detection":{"id":"object-detection","displayTitle":"Object Detection","__typename":"Tag"},"Tag:image-classification":{"id":"image-classification","displayTitle":"Image Classification","__typename":"Tag"},"$Post:509a600f7b57.previewContent":{"subtitle":"Deep learning algorithms have solved several computer vision tasks with an increasing level of difficulty. In my previous blog posts, I…","__typename":"PreviewContent"}}</script><script src="Review%20of%20Deep%20Learning%20Algorithms%20for%20Image%20Semantic%20Segmentation_files/manifest.js"></script><script src="Review%20of%20Deep%20Learning%20Algorithms%20for%20Image%20Semantic%20Segmentation_files/vendorsmain.js"></script><script src="Review%20of%20Deep%20Learning%20Algorithms%20for%20Image%20Semantic%20Segmentation_files/main.js"></script><script src="Review%20of%20Deep%20Learning%20Algorithms%20for%20Image%20Semantic%20Segmentation_files/vendorsscreen.js"></script>
<script src="Review%20of%20Deep%20Learning%20Algorithms%20for%20Image%20Semantic%20Segmentation_files/screen.js"></script>
<script src="Review%20of%20Deep%20Learning%20Algorithms%20for%20Image%20Semantic%20Segmentation_files/screen_003.js"></script>
<script src="Review%20of%20Deep%20Learning%20Algorithms%20for%20Image%20Semantic%20Segmentation_files/screen_004.js"></script>
<script src="Review%20of%20Deep%20Learning%20Algorithms%20for%20Image%20Semantic%20Segmentation_files/screen_002.js"></script><script>window.main();</script><script src="Review%20of%20Deep%20Learning%20Algorithms%20for%20Image%20Semantic%20Segmentation_files/p.js" async="" id="parsely-cf"></script><script src="Review%20of%20Deep%20Learning%20Algorithms%20for%20Image%20Semantic%20Segmentation_files/client.js" async=""></script><iframe src="Review%20of%20Deep%20Learning%20Algorithms%20for%20Image%20Semantic%20Segmentation_files/request.html" style="border: medium none; position: fixed; z-index: 9999; top: 0px; right: 0px; width: 400px; height: 207px;"></iframe></body></html>