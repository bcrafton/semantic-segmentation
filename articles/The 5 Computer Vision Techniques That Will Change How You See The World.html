<!DOCTYPE html>
<html data-rh="lang" lang="en"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8"><script async="" src="The%205%20Computer%20Vision%20Techniques%20That%20Will%20Change%20How%20You%20See%20The%20World_files/branch-latest.js"></script><script async="" src="The%205%20Computer%20Vision%20Techniques%20That%20Will%20Change%20How%20You%20See%20The%20World_files/analytics.js"></script><script>!function(c,f){var t,o,i,e=[],r={passive:!0,capture:!0},n=new Date,a="pointerup",u="pointercancel";function p(n,e){t||(t=e,o=n,i=new Date,w(f),s())}function s(){0<=o&&o<i-n&&(e.forEach(function(n){n(o,t)}),e=[])}function l(n){if(n.cancelable){var e=(1e12<n.timeStamp?new Date:performance.now())-n.timeStamp;"pointerdown"==n.type?function(n,e){function t(){p(n,e),i()}function o(){i()}function i(){f(a,t,r),f(u,o,r)}c(a,t,r),c(u,o,r)}(e,n):p(e,n)}}function w(e){["click","mousedown","keydown","touchstart","pointerdown"].forEach(function(n){e(n,l,r)})}w(c),self.perfMetrics=self.perfMetrics||{},self.perfMetrics.onFirstInputDelay=function(n){e.push(n),s()}}(addEventListener,removeEventListener)</script><title>The 5 Computer Vision Techniques That Will Change How You See The World</title><meta data-rh="true" charset="utf-8"><meta data-rh="true" name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1"><meta data-rh="true" name="theme-color" content="#000000"><meta data-rh="true" name="twitter:app:name:iphone" content="Medium"><meta data-rh="true" name="twitter:app:id:iphone" content="828256236"><meta data-rh="true" property="al:ios:app_name" content="Medium"><meta data-rh="true" property="al:ios:app_store_id" content="828256236"><meta data-rh="true" property="al:android:package" content="com.medium.reader"><meta data-rh="true" property="fb:app_id" content="542599432471018"><meta data-rh="true" property="og:site_name" content="Medium"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2019-05-24T13:14:31.694Z"><meta data-rh="true" name="title" content="The 5 Computer Vision Techniques That Will Change How You See The World"><meta data-rh="true" property="og:title" content="The 5 Computer Vision Techniques That Will Change How You See The World"><meta data-rh="true" property="twitter:title" content="The 5 Computer Vision Techniques That Will Change How You See The World"><meta data-rh="true" name="twitter:site" content="@fritzlabs"><meta data-rh="true" name="twitter:app:url:iphone" content="medium://p/1ee19334354b"><meta data-rh="true" property="al:android:url" content="medium://p/1ee19334354b"><meta data-rh="true" property="al:ios:url" content="medium://p/1ee19334354b"><meta data-rh="true" name="apple-itunes-app" content="app-id=828256236,app-argument=medium://p/1ee19334354b"><meta data-rh="true" property="al:android:app_name" content="Medium"><meta data-rh="true" name="description" content="In-depth overviews of common Computer Vision techniques: Image classification, object detection, object tracking, semantic segmentation, and instance segmentation"><meta data-rh="true" property="og:description" content="In-depth overviews of common Computer Vision techniques: Image classification, object detection, object tracking, semantic segmentation, and instance segmentation"><meta data-rh="true" property="twitter:description" content="In-depth overviews of common Computer Vision techniques: Image classification, object detection, object tracking, semantic segmentation, and instance segmentation"><meta data-rh="true" property="og:url" content="https://heartbeat.fritz.ai/the-5-computer-vision-techniques-that-will-change-how-you-see-the-world-1ee19334354b"><meta data-rh="true" property="al:web:url" content="https://heartbeat.fritz.ai/the-5-computer-vision-techniques-that-will-change-how-you-see-the-world-1ee19334354b"><meta data-rh="true" property="og:image" content="https://miro.medium.com/max/1200/1*4zHA9YtHIzKTm5Ix98rKZg.png"><meta data-rh="true" name="twitter:image:src" content="https://miro.medium.com/max/1200/1*4zHA9YtHIzKTm5Ix98rKZg.png"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="article:author" content="/@james_aka_yale"><meta data-rh="true" name="twitter:creator" content="@james_aka_yale"><meta data-rh="true" name="author" content="James Le"><meta data-rh="true" name="robots" content="index,follow"><meta data-rh="true" name="referrer" content="unsafe-url"><meta data-rh="true" name="twitter:label1" value="Reading time"><meta data-rh="true" name="twitter:data1" value="16 min read"><link data-rh="true" rel="publisher" href="https://plus.google.com/103654360130207659246"><link data-rh="true" rel="search" type="application/opensearchdescription+xml" title="Medium" href="https://heartbeat.fritz.ai/osd.xml"><link data-rh="true" rel="apple-touch-icon" sizes="152x152" href="https://cdn-images-1.medium.com/fit/c/152/152/1*8I-HPL0bfoIzGied-dzOvA.png"><link data-rh="true" rel="apple-touch-icon" sizes="120x120" href="https://cdn-images-1.medium.com/fit/c/120/120/1*8I-HPL0bfoIzGied-dzOvA.png"><link data-rh="true" rel="apple-touch-icon" sizes="76x76" href="https://cdn-images-1.medium.com/fit/c/76/76/1*8I-HPL0bfoIzGied-dzOvA.png"><link data-rh="true" rel="apple-touch-icon" sizes="60x60" href="https://cdn-images-1.medium.com/fit/c/60/60/1*8I-HPL0bfoIzGied-dzOvA.png"><link data-rh="true" rel="mask-icon" href="https://cdn-static-1.medium.com/_/fp/icons/monogram-mask.KPLCSFEZviQN0jQ7veN2RQ.svg" color="#171717"><link data-rh="true" id="glyph_link" rel="stylesheet" type="text/css" href="The%205%20Computer%20Vision%20Techniques%20That%20Will%20Change%20How%20You%20See%20The%20World_files/m2.css"><link data-rh="true" rel="author" href="https://heartbeat.fritz.ai/@james_aka_yale"><link data-rh="true" rel="canonical" href="https://heartbeat.fritz.ai/the-5-computer-vision-techniques-that-will-change-how-you-see-the-world-1ee19334354b"><script data-rh="true">(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-24232453-2', 'auto');
ga('send', 'pageview');</script><style type="text/css" data-fela-rehydration="417" data-fela-type="STATIC">html{box-sizing:border-box}*, *:before, *:after{box-sizing:inherit}body{margin:0;padding:0;text-rendering:optimizeLegibility;-webkit-font-smoothing:antialiased;color:rgba(0,0,0,0.8);position:relative;min-height:100vh}h1, h2, h3, h4, h5, h6, dl, dd, ol, ul, menu, figure, blockquote, p, pre, form{margin:0}menu, ol, ul{padding:0;list-style:none;list-style-image:none}main{display:block}a{color:inherit;text-decoration:none}a, button, input{-webkit-tap-highlight-color:transparent}img, svg{vertical-align:middle}button{background:transparent;overflow:visible}button, input, optgroup, select, textarea{margin:0}</style><style type="text/css" data-fela-rehydration="417" data-fela-type="KEYFRAME">@-webkit-keyframes k1{0%{transform:scale(1);opacity:1}70%{transform:scale(1.4);opacity:0}100%{opacity:0}}@-moz-keyframes k1{0%{transform:scale(1);opacity:1}70%{transform:scale(1.4);opacity:0}100%{opacity:0}}@keyframes k1{0%{transform:scale(1);opacity:1}70%{transform:scale(1.4);opacity:0}100%{opacity:0}}@-webkit-keyframes k2{0%{transform:matrix(0.97, 0, 0, 1, 0, 12);opacity:0}20%{transform:matrix(0.99, 0, 0, 1, 0, 2);opacity:0.7}40%{transform:matrix(1, 0, 0, 1, 0, -1);opacity:1}70%{transform:matrix(1, 0, 0, 1, 0, 0);opacity:1}100%{transform:matrix(1, 0, 0, 1, 0, 0);opacity:1}}@-moz-keyframes k2{0%{transform:matrix(0.97, 0, 0, 1, 0, 12);opacity:0}20%{transform:matrix(0.99, 0, 0, 1, 0, 2);opacity:0.7}40%{transform:matrix(1, 0, 0, 1, 0, -1);opacity:1}70%{transform:matrix(1, 0, 0, 1, 0, 0);opacity:1}100%{transform:matrix(1, 0, 0, 1, 0, 0);opacity:1}}@keyframes k2{0%{transform:matrix(0.97, 0, 0, 1, 0, 12);opacity:0}20%{transform:matrix(0.99, 0, 0, 1, 0, 2);opacity:0.7}40%{transform:matrix(1, 0, 0, 1, 0, -1);opacity:1}70%{transform:matrix(1, 0, 0, 1, 0, 0);opacity:1}100%{transform:matrix(1, 0, 0, 1, 0, 0);opacity:1}}</style><style type="text/css" data-fela-rehydration="417" data-fela-type="RULE">.a{font-family:medium-content-sans-serif-font, -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Open Sans", "Helvetica Neue", sans-serif}.b{font-weight:400}.c{background-color:rgba(255, 255, 255, 1)}.n{display:block}.o{position:fixed}.p{top:0}.q{left:0}.r{right:0}.s{z-index:500}.t{box-shadow:0 4px 12px 0 rgba(0, 0, 0, 0.05)}.u{transition:transform 300ms ease}.v{will-change:transform}.w{padding-left:24px}.x{padding-right:24px}.y{margin-left:auto}.z{margin-right:auto}.ab{height:65px}.ac{width:100%}.ae{max-width:1080px}.af{box-sizing:border-box}.ag{display:flex}.ah{align-items:center}.ak{flex:1 0 auto}.al{margin-left:-6px}.am{fill:rgba(0, 0, 0, 0.84)}.an{flex:0 0 auto}.ao{margin-left:16px}.ap{font-family:medium-content-sans-serif-font, "Lucida Grande", "Lucida Sans Unicode", "Lucida Sans", Geneva, Arial, sans-serif}.aq{font-style:normal}.ar{line-height:20px}.as{font-size:15.8px}.at{letter-spacing:0px}.au{color:rgba(0, 0, 0, 0.54)}.av{fill:rgba(0, 0, 0, 0.54)}.aw{color:rgba(115, 113, 113, 1)}.ax{fill:rgba(134, 132, 132, 1)}.ay{font-size:inherit}.az{border:inherit}.ba{font-family:inherit}.bb{letter-spacing:inherit}.bc{font-weight:inherit}.bd{padding:0}.be{margin:0}.bf:hover{cursor:pointer}.bg:hover{color:rgba(105, 104, 103, 1)}.bh:hover{fill:rgba(115, 113, 113, 1)}.bi:focus{outline:none}.bj:disabled{cursor:default}.bk:disabled{color:rgba(3, 168, 124, 0.5)}.bl:disabled{fill:rgba(3, 168, 124, 0.5)}.bm{padding:8px 16px}.bn{background:0}.bo{border-color:rgba(134, 132, 132, 1)}.bp:hover{border-color:rgba(115, 113, 113, 1)}.bq{border-radius:4px}.br{border-width:1px}.bs{border-style:solid}.bt{display:inline-block}.bu{text-decoration:none}.bv{border-top:none}.bw{background-color:rgba(0, 0, 0, 1)}.by{height:54px}.bz{overflow:hidden}.ca{margin-right:40px}.cb{width:196px}.cc{height:36px}.cd{overflow:auto}.ce{flex:0 1 auto}.cf{list-style-type:none}.cg{line-height:40px}.ch{white-space:nowrap}.ci{overflow-x:auto}.cj{align-items:flex-start}.ck{margin-top:20px}.cl{padding-top:20px}.cm{height:80px}.cn{height:20px}.co{margin-right:15px}.cp{margin-left:15px}.cq:first-child{margin-left:0}.cr{min-width:1px}.cs{background-color:rgba(177, 174, 174, 1)}.ct{font-weight:300}.cu{font-size:15px}.cv{line-height:21px}.cw{color:rgba(177, 174, 174, 1)}.cx{text-transform:uppercase}.cy{letter-spacing:1px}.cz{color:inherit}.da{fill:inherit}.db:hover{color:rgba(236, 233, 233, 1)}.dc:hover{fill:rgba(217, 214, 214, 1)}.dd:disabled{color:rgba(123, 121, 121, 1)}.de:disabled{fill:rgba(123, 121, 121, 1)}.df{margin-bottom:0px}.dg{height:119px}.dj{padding-bottom:1px}.dk{margin-top:40px}.dl{max-width:728px}.dm{opacity:0}.dn{pointer-events:none}.do{will-change:opacity}.dp{transition:opacity 200ms}.dq{width:131px}.dr{left:50%}.ds{transform:translateX(-516px)}.dt{top:calc(65px + 54px + 40px)}.du{flex-direction:column}.dv{padding-bottom:28px}.dw{border-bottom:1px solid rgba(0, 0, 0, 0.1)}.dx:hover{color:rgba(0, 0, 0, 0.9)}.dy:hover{fill:rgba(0, 0, 0, 0.9)}.dz:disabled{color:rgba(0, 0, 0, 0.54)}.ea:disabled{fill:rgba(0, 0, 0, 0.54)}.eb{font-weight:600}.ec{font-size:18px}.ed{color:rgba(0, 0, 0, 0.84)}.ee{padding-bottom:20px}.ef{padding-top:2px}.eg{font-size:16px}.eh{max-height:120px}.ei{text-overflow:ellipsis}.ej{display:-webkit-box}.ek{-webkit-line-clamp:6}.el{-webkit-box-orient:vertical}.em{padding:4px 12px}.en{padding-top:28px}.eo{margin-bottom:19px}.ep{margin-left:-5px}.eq{margin-right:5px}.er{position:relative}.es{outline:0}.et{border:0}.eu{user-select:none}.ev{cursor:pointer}.ew> svg{pointer-events:none}.ex:active{border-style:none}.ey:focus{fill:rgba(115, 113, 113, 1)}.ez{margin-top:5px}.fa button{text-align:left}.fb{clear:both}.fc{justify-content:center}.fd{margin-right:8px}.fe{margin-bottom:8px}.ff{border-radius:3px}.fg{padding:5px 10px}.fh{background:rgba(0, 0, 0, 0.05)}.fi{line-height:22px}.fj{justify-content:space-between}.fk{margin-top:15px}.fl{margin-right:16px}.fm{border:1px solid rgba(0, 0, 0, 0.1)}.fn{border-radius:50%}.fo{height:60px}.fp{transition:border-color 150ms ease}.fq{width:60px}.fr::before{background:
      radial-gradient(circle, rgba(115, 113, 113, 1) 60%, transparent 70%)
    }.fs::before{border-radius:50%}.ft::before{content:""}.fu::before{display:block}.fv::before{z-index:0}.fw::before{left:0}.fx::before{height:100%}.fy::before{position:absolute}.fz::before{top:0}.ga::before{width:100%}.gb:hover::before{animation:k1 2000ms infinite cubic-bezier(.1,.12,.25,1)}.gc:active{border-style:solid}.gd{background:rgba(255, 255, 255, 1)}.ge{z-index:2}.gf{height:100%}.gg{position:absolute}.gh{padding-right:8px}.gi{display:none}.gj{margin-top:25px}.gk{margin-bottom:25px}.gl{padding-top:32px}.gm{border-top:solid 1px rgba(0, 0, 0, 0.1)}.gn{margin-bottom:32px}.go{min-height:80px}.gt{width:80px}.gu{fill:rgba(3, 168, 124, 1)}.gv{width:calc(100% + 10px)}.gw{height:calc(100% + 10px)}.gx{top:-5px}.gy{left:-5px}.gz{padding-left:102px}.hb{letter-spacing:0.05em}.hc{margin-bottom:6px}.hd{font-size:28px}.he{line-height:36px}.hf{border-color:rgba(0, 0, 0, 0.54)}.hg:hover{color:rgba(0, 0, 0, 0.97)}.hh:hover{fill:rgba(0, 0, 0, 0.97)}.hi:hover{border-color:rgba(0, 0, 0, 0.84)}.hj:disabled{fill:rgba(0, 0, 0, 0.76)}.hk:disabled{border-color:rgba(0, 0, 0, 0.2)}.hl:disabled{cursor:inherit}.hm:disabled:hover{color:rgba(0, 0, 0, 0.54)}.hn:disabled:hover{fill:rgba(0, 0, 0, 0.76)}.ho:disabled:hover{border-color:rgba(0, 0, 0, 0.2)}.hp{max-width:555px}.hq{max-width:450px}.hr{line-height:24px}.ht{max-width:550px}.hu{padding-top:25px}.hv{border-top:1px solid rgba(0, 0, 0, 0.1)}.hw{padding:20px}.hx{border:1px solid rgba(134, 132, 132, 1)}.hy{text-align:center}.hz{margin-top:64px}.ia{background-color:rgba(0, 0, 0, 0.02)}.ib{top:calc(100vh + 100px)}.ic{bottom:calc(100vh + 100px)}.id{width:10px}.ie{word-break:break-word}.if{word-wrap:break-word}.ig:after{display:block}.ih:after{content:""}.ii:after{clear:both}.ij{margin-top:0px}.ik{transition:opacity 100ms 400ms}.il{transform:translateZ(0)}.im{margin:auto}.in{background-color:rgba(0, 0, 0, 0.05)}.io{padding-bottom:58.89101338432123%}.ip{filter:blur(20px)}.iq{transform:scale(1.1)}.ir{line-height:1.4}.is{margin-top:10px}.iu{margin:0 auto}.iv{line-height:1.23}.iw{letter-spacing:0}.ix{font-family:medium-content-title-font, Georgia, Cambria, "Times New Roman", Times, serif}.iy{font-size:40px}.je{margin-bottom:-0.27em}.jf{line-height:48px}.jg{margin-top:32px}.jh{width:48px}.ji{height:48px}.jj{width:calc(100% + 8px)}.jk{height:calc(100% + 8px)}.jl{top:-4px}.jm{left:-4px}.jn{margin-left:12px}.jo{margin-bottom:2px}.jq{max-height:20px}.jr{-webkit-line-clamp:1}.js:hover{text-decoration:underline}.jt{margin-left:8px}.ju{padding:0px 8px}.jv{line-height:18px}.jw{line-height:1.58}.jx{letter-spacing:-0.004em}.jy{font-family:medium-content-serif-font, Georgia, Cambria, "Times New Roman", Times, serif}.kj{margin-bottom:-0.46em}.kk{background-repeat:repeat-x}.kl{background-image:linear-gradient(to right,rgba(0, 0, 0, 0.84) 100%,rgba(0, 0, 0, 0.84) 0);background-image:url('data:image/svg+xml;utf8,<svg preserveAspectRatio="none" viewBox="0 0 1 1" xmlns="http://www.w3.org/2000/svg"><line x1="0" y1="0" x2="1" y2="1" stroke="rgba(0, 0, 0, 0.84)" /></svg>')}.km{background-size:1px 1px}.kn{background-position:0 1.05em;background-position:0 calc(1em + 1px)}.ko{line-height:1.18}.kp{letter-spacing:-0.022em}.la{margin-bottom:-0.31em}.lg{list-style-type:disc}.lh{margin-left:30px}.li{padding-left:0px}.lo{line-height:1.12}.lz{margin-bottom:-0.28em}.mf{padding-bottom:32.65%}.mg{font-style:italic}.mh{font-weight:700}.mi{max-width:1000px}.mj{padding-bottom:34.1%}.mk{max-width:6000px}.ml{padding-bottom:56.49999999999999%}.mm{padding-bottom:66.66666666666667%}.mn{border-width:2px}.mo{border-color:rgba(255, 255, 255, 1)}.mp{float:left}.mq{margin-left:-150px}.mr{margin-right:30px}.ms{width:75%}.mt{padding-bottom:10px}.my{margin-bottom:16px}.mz{max-width:1503px}.na{padding-bottom:63.60612109115104%}.nb{max-width:1334px}.nc{padding-bottom:63.86806596701649%}.nd{max-width:758px}.ne{padding-bottom:89.70976253298153%}.nf{font-family:medium-content-slab-serif-font, Georgia, Cambria, "Times New Roman", Times, serif}.ng{border:none}.nh{margin-top:30px}.ni:before{content:"..."}.nj:before{letter-spacing:0.6em}.nk:before{text-indent:0.6em}.nl:before{font-style:italic}.nm:before{line-height:1.4}.nn{padding-left:30px}.no{line-height:1.48}.np{letter-spacing:-0.014em}.nq{color:rgba(0, 0, 0, 0.76)}.nr{font-size:24px}.nx{font-size:30px}.od{padding-bottom:46.94244604316546%}.oe{max-width:1020px}.of{padding-bottom:112.54901960784315%}.og{padding-bottom:28.333333333333332%}.oh{max-width:2408px}.oi{padding-bottom:41.4451827242525%}.oj{padding-bottom:56.25%}.ok{padding-bottom:51.86274509803922%}.ol{max-width:1600px}.om{padding-bottom:27.875%}.on{padding-bottom:57.25190839694657%}.oo{max-width:558px}.op{padding-bottom:75.62724014336918%}.oq{max-width:692px}.or{padding-bottom:43.35260115606936%}.os{max-width:2000px}.ot{margin-left:24px}.ou{margin-right:24px}.ov{padding-bottom:53.85%}.pb{padding-bottom:50%}</style><style type="text/css" data-fela-rehydration="417" data-fela-type="RULE" media="all and (max-width: 1256px)">.d{display:none}</style><style type="text/css" data-fela-rehydration="417" data-fela-type="RULE" media="all and (max-width: 1080px)">.e{display:none}.it{text-align:center}</style><style type="text/css" data-fela-rehydration="417" data-fela-type="RULE" media="all and (max-width: 904px)">.f{display:none}.mu{float:none}.mv{margin-left:0}.mw{margin-right:0}.mx{width:100%}</style><style type="text/css" data-fela-rehydration="417" data-fela-type="RULE" media="all and (max-width: 728px)">.g{display:none}.ai{height:56px}.aj{display:flex}.bx{display:block}.dh{margin-bottom:0px}.di{height:110px}.gp{margin-bottom:24px}.gq{align-items:center}.gr{width:102px}.gs{position:relative}.ha{padding-left:0}.hs{margin-top:24px}</style><style type="text/css" data-fela-rehydration="417" data-fela-type="RULE" media="all and (max-width: 552px)">.h{display:none}.jp{margin-bottom:0px}</style><style type="text/css" data-fela-rehydration="417" data-fela-type="RULE" media="all and (min-width: 1080px)">.i{display:none}.jd{margin-top:0.78em}.kh{font-size:21px}.ki{margin-top:2em}.ky{font-size:26px}.kz{margin-top:1.72em}.lf{margin-top:0.86em}.ln{margin-top:1.05em}.lx{font-size:34px}.ly{margin-top:1.95em}.me{margin-top:56px}.nw{margin-top:1.75em}.oc{margin-top:1.25em}.pa{margin-top:3.14em}</style><style type="text/css" data-fela-rehydration="417" data-fela-type="RULE" media="all and (min-width: 904px) and (max-width: 1079.98px)">.j{display:none}.jc{margin-top:0.78em}.kf{font-size:21px}.kg{margin-top:2em}.kw{font-size:26px}.kx{margin-top:1.72em}.le{margin-top:0.86em}.lm{margin-top:1.05em}.lv{font-size:34px}.lw{margin-top:1.95em}.md{margin-top:56px}.nv{margin-top:1.75em}.ob{margin-top:1.25em}.oz{margin-top:3.14em}</style><style type="text/css" data-fela-rehydration="417" data-fela-type="RULE" media="all and (min-width: 728px) and (max-width: 903.98px)">.k{display:none}.jb{margin-top:0.78em}.kd{font-size:21px}.ke{margin-top:2em}.ku{font-size:26px}.kv{margin-top:1.72em}.ld{margin-top:0.86em}.ll{margin-top:1.05em}.lt{font-size:34px}.lu{margin-top:1.95em}.mc{margin-top:56px}.nu{margin-top:1.75em}.oa{margin-top:1.25em}.oy{margin-top:3.14em}</style><style type="text/css" data-fela-rehydration="417" data-fela-type="RULE" media="all and (min-width: 552px) and (max-width: 727.98px)">.l{display:none}.ja{margin-top:0.39em}.kb{font-size:18px}.kc{margin-top:1.56em}.ks{font-size:24px}.kt{margin-top:1.23em}.lc{margin-top:0.67em}.lk{margin-top:1.34em}.lr{font-size:30px}.ls{margin-top:1.2em}.mb{margin-top:40px}.nt{margin-top:1.08em}.nz{margin-top:0.93em}.ox{margin-top:2em}</style><style type="text/css" data-fela-rehydration="417" data-fela-type="RULE" media="all and (max-width: 551.98px)">.m{display:none}.iz{margin-top:0.39em}.jz{font-size:18px}.ka{margin-top:1.56em}.kq{font-size:24px}.kr{margin-top:1.23em}.lb{margin-top:0.67em}.lj{margin-top:1.34em}.lp{font-size:30px}.lq{margin-top:1.2em}.ma{margin-top:40px}.ns{margin-top:1.08em}.ny{margin-top:0.93em}.ow{margin-top:2em}</style><script charset="utf-8" src="The%205%20Computer%20Vision%20Techniques%20That%20Will%20Change%20How%20You%20See%20The%20World_files/vendorstracing.js"></script><script charset="utf-8" src="The%205%20Computer%20Vision%20Techniques%20That%20Will%20Change%20How%20You%20See%20The%20World_files/tracing.js"></script><link rel="icon" href="https://miro.medium.com/fit/c/128/128/1*KpWOVdzvNX_R3ewGUwLmYA.png" data-rh="true"><script type="application/ld+json" data-rh="true">{"@context":"http:\u002F\u002Fschema.org","@type":"NewsArticle","image":{"@type":"ImageObject","width":101,"height":60,"url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F101\u002F1*4zHA9YtHIzKTm5Ix98rKZg.png"},"thumbnailUrl":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F101\u002F1*4zHA9YtHIzKTm5Ix98rKZg.png","url":"https:\u002F\u002Fheartbeat.fritz.ai\u002Fthe-5-computer-vision-techniques-that-will-change-how-you-see-the-world-1ee19334354b","dateCreated":"2018-04-12T13:46:01.956Z","datePublished":"2018-04-12T13:46:01.956Z","dateModified":"2019-05-24T13:14:31.930Z","headline":"The 5 Computer Vision Techniques That Will Change How You See The World","name":"The 5 Computer Vision Techniques That Will Change How You See The World","articleId":"1ee19334354b","keywords":["Lite:true","Tag:Machine Learning","Tag:Deep Learning","Tag:Computer Vision","Tag:Heartbeat","Tag:Machine Learning Tools","Topic:Data Science","Publication:fritzheartbeat","Elevated:false","LockedPostSource:LOCKED_POST_SOURCE_NONE","LayerCake:3"],"author":{"@type":"Person","name":"James Le","url":"https:\u002F\u002Fheartbeat.fritz.ai\u002F@james_aka_yale"},"creator":["James Le"],"publisher":{"@type":"Organization","name":"Heartbeat","url":"heartbeat.fritz.ai","logo":{"@type":"ImageObject","width":323,"height":60,"url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F323\u002F1*XdqEfc3Jep6vKtS-KQXicw.png"}},"mainEntityOfPage":"https:\u002F\u002Fheartbeat.fritz.ai\u002Fthe-5-computer-vision-techniques-that-will-change-how-you-see-the-world-1ee19334354b"}</script><script type="text/javascript" data-rh="true">(function(b,r,a,n,c,h,_,s,d,k){if(!b[n]||!b[n]._q){for(;s<_.length;)c(h,_[s++]);d=r.createElement(a);d.async=1;d.src="https://cdn.branch.io/branch-latest.min.js";k=r.getElementsByTagName(a)[0];k.parentNode.insertBefore(d,k);b[n]=h}})(window,document,"script","branch",function(b,r){b[r]=function(){b._q.push([r,arguments])}},{_q:[],_v:1},"addListener applyCode autoAppIndex banner closeBanner closeJourney creditHistory credits data deepview deepviewCta first getCode init link logout redeem referrals removeListener sendSMS setBranchViewData setIdentity track validateCode trackCommerceEvent logEvent".split(" "), 0);
  branch.init('key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm', {'no_journeys': true, 'disable_exit_animation': true, 'disable_entry_animation': true, 'tracking_disabled': null}, function(err, data) {});</script></head><body><div id="root"><div class="a b c"><div class="d e f g h i j k l m"></div><nav class="n o p q r c s t u v"><div class="branch-journeys-top"><div class="n c"><section class="w x y z ab ac ae af ag ah ai aj"><div class="ag ah ak s"><div class="al n"><a href="https://medium.com/?source=post_page---------------------------" aria-label="Homepage"><svg width="45" height="45" viewBox="0 0 45 45" class="am"><path d="M5 40V5h35v35H5zm8.56-12.63c0 .56-.03.69-.32 1.03L10.8 31.4v.4h6.97v-.4L15.3 28.4c-.29-.34-.34-.5-.34-1.03v-8.95l6.13 13.36h.71l5.26-13.36v10.64c0 .3 0 .35-.19.53l-1.85 1.8v.4h9.2v-.4l-1.83-1.8c-.18-.18-.2-.24-.2-.53V15.94c0-.3.02-.35.2-.53l1.82-1.8v-.4h-6.47l-4.62 11.55-5.2-11.54h-6.8v.4l2.15 2.63c.24.3.29.37.29.77v10.35z"></path></svg></a></div></div><div class="n an s"><div class="ag ah"><div class="ag g"><div class="ao n"><span class="ap b aq ar as at n au av"><a href="https://medium.com/m/signin?operation=login&amp;redirect=https%3A%2F%2Fheartbeat.fritz.ai%2Fthe-5-computer-vision-techniques-that-will-change-how-you-see-the-world-1ee19334354b&amp;source=post_page--------------------------nav_reg-" class="aw ax ay az ba bb bc bd be bf bg bh bi bj bk bl">Sign in</a></span></div></div><div class="ao n"><button class="bm bn aw ax bo bg bh bp bf bq ap b aq ar as at br bs af bt bu bi">Get started</button></div></div></div></section></div><div class="bv n bw bx"><section class="w x y z by ac ae af bz ag ah"><div class="ca n an"><a href="https://heartbeat.fritz.ai/?source=post_page---------------------------"><div class="cb cc"><img alt="Heartbeat" src="The%205%20Computer%20Vision%20Techniques%20That%20Will%20Change%20How%20You%20See%20The%20World_files/1XdqEfc3Jep6vKtS-KQXicw.png" class="" width="196" height="36"></div></a></div><div class="cd n ce"><ul class="cf be cg ch ci ag cj g ck cl cm"><li class="ag ah cn co cp cq"><span class="ap ct cu cv cw cx cy"><a href="https://heartbeat.fritz.ai/mobile-ml/home?source=post_page---------------------------" class="cz da ay az ba bb bc bd be bf db dc bi bj dd de">MOBILE</a></span></li><li class="ag ah cn co cp cq"><span class="ap ct cu cv cw cx cy"><a href="https://heartbeat.fritz.ai/machine-learning/home?source=post_page---------------------------" class="cz da ay az ba bb bc bd be bf db dc bi bj dd de">MACHINE LEARNING</a></span></li><li class="ag ah cn co cp cq"><span class="ap ct cu cv cw cx cy"><a href="https://heartbeat.fritz.ai/heartbeat-fritz-ai-newsletter/home?source=post_page---------------------------" class="cz da ay az ba bb bc bd be bf db dc bi bj dd de">NEWSLETTER</a></span></li><li class="ag ah cn co cp cq"><span class="ap ct cu cv cw cx cy"><a href="https://heartbeat.fritz.ai/heartbeat-fritz-ai-community/home?source=post_page---------------------------" class="cz da ay az ba bb bc bd be bf db dc bi bj dd de">COMMUNITY</a></span></li><li class="ag ah cn co cp cq"><span class="ap ct cu cv cw cx cy"><a class="cz da ay az ba bb bc bd be bf db dc bi bj dd de" href="https://heartbeat.fritz.ai/call-for-contributors-october-2018-update-fee7f5b80f3e?source=post_page---------------------------">CONTRIBUTE</a></span></li><li class="ag ah cn co cp cq"><span class="ap ct cu cv cw cx cy"><a href="https://heartbeat.fritz.ai/archive?source=post_page---------------------------" class="cz da ay az ba bb bc bd be bf db dc bi bj dd de">ALL</a></span></li><span class="cn cr cs"></span><li class="ag ah cn co cp cq"><span class="ap ct cu cv cw cx cy"><a href="http://fritz.ai/?source=post_page---------------------------" class="cz da ay az ba bb bc bd be bf db dc bi bj dd de">ABOUT FRITZ</a></span></li></ul></div></section></div></div></nav><div class="df dg n dh di"></div><article><div class="dj dk n"><section class="w x y z ac dl af n"></section></div><span class="n"></span><div><div class="gg q ib ic id dn"></div><div class="y z dl er"><div class="n h g f e"><aside class="pe gg p" style="width: 896px;"></aside></div></div><section class="ie if ig ih ii"><div class="ac"><figure class="ij fb ac paragraph-image"><div class="im n er in"><div class="io n"><div class="dm ik gg p q gf ac bz v il"><img src="The%205%20Computer%20Vision%20Techniques%20That%20Will%20Change%20How%20You%20See%20The%20World_files/14zHA9YtHIzKTm5Ix98rKZg_002.png" class="gg p q gf ac ip iq" width="1046" height="616"></div><img class="pc pd gg p q gf ac gd" src="The%205%20Computer%20Vision%20Techniques%20That%20Will%20Change%20How%20You%20See%20The%20World_files/14zHA9YtHIzKTm5Ix98rKZg.png" width="1046" height="616"><noscript><img src="https://miro.medium.com/max/2092/1*4zHA9YtHIzKTm5Ix98rKZg.png" class="gg p q gf ac" width="1046" height="616"/></noscript></div></div><figcaption class="au eg ir is hy dl y z it ap ct" data-selectable-paragraph="">Digital Eye</figcaption></figure></div><div class="af iu ac dl w x"><div><div id="bde7" class="iv iw ed aq ix b iy iz ja jb jc jd je"><h1 class="ix b iy jf ed">The 5 Computer Vision Techniques That Will Change How You See The World</h1></div><div class="jg"><div class="ah ag"><div><a href="https://heartbeat.fritz.ai/@james_aka_yale?source=post_page---------------------------"><div class="er jh ji"><svg width="52" height="58" viewBox="0 0 52 58" class="gu gg jj jk jl jm dn"><path d="M1.49 16.25A27.53 27.53 0 0 1 26 1.55V.45A28.63 28.63 0 0 0 .51 15.75l.98.5zM26 1.55a27.53 27.53 0 0 1 24.51 14.7l.98-.5A28.63 28.63 0 0 0 26 .45v1.1zm24.51 40.2A27.53 27.53 0 0 1 26 56.45v1.1a28.63 28.63 0 0 0 25.49-15.3l-.98-.5zM26 56.45a27.53 27.53 0 0 1-24.51-14.7l-.98.5A28.63 28.63 0 0 0 26 57.55v-1.1z"></path></svg><img alt="James Le" src="The%205%20Computer%20Vision%20Techniques%20That%20Will%20Change%20How%20You%20See%20The%20World_files/1kbXSc2-EEtk9ekKq36woIQ.jpeg" class="n fn ji jh" width="48" height="48"></div></a></div><div class="jn ac n"><div class="ag"><div style="flex:1"><span class="ap b aq ar as at n ed am"><div class="jo ag ah jp" data-test-id="postByline"><span class="ap ct eg ar bz jq ei ej jr el ed"><a href="https://heartbeat.fritz.ai/@james_aka_yale?source=post_page---------------------------" class="cz da ay az ba bb bc bd be bf js bi bj dz ea">James Le</a></span><div class="jt n an h"><button class="ju ed am bn hf hg hh hi bf dz hj hk hl hm hn ho bq ap b aq jv cu at br bs af bt bu bi">Follow</button></div></div></span></div></div><span class="ap b aq ar as at n au av"><span class="ap ct eg ar bz jq ei ej jr el au"><div><a class="cz da ay az ba bb bc bd be bf js bi bj dz ea" href="https://heartbeat.fritz.ai/the-5-computer-vision-techniques-that-will-change-how-you-see-the-world-1ee19334354b?source=post_page---------------------------">Apr 12, 2018</a> <!-- -->·<!-- --> <!-- -->16<!-- --> min read</div></span></span></div></div></div></div><p id="7594" class="jw jx ed aq jy b jz ka kb kc kd ke kf kg kh ki kj" data-selectable-paragraph=""><a class="cz bu kk kl km kn" href="https://heartbeat.fritz.ai/the-5-trends-that-dominated-computer-vision-in-2018-de38fbb9bd86?source=post_page---------------------------">Computer Vision</a>
 is one of the hottest research fields within Deep Learning at the 
moment. It sits at the intersection of many academic subjects, such as 
Computer Science (Graphics, Algorithms, Theory, Systems, Architecture), 
Mathematics (Information Retrieval, Machine Learning), Engineering 
(Robotics, Speech, NLP, Image Processing), Physics (Optics), Biology 
(Neuroscience), and Psychology (Cognitive Science).</p><p id="aec3" class="jw jx ed aq jy b jz ka kb kc kd ke kf kg kh ki kj" data-selectable-paragraph="">As
 Computer Vision represents a relative understanding of visual 
environments and their contexts, many scientists believe the field paves
 the way towards Artificial General Intelligence due to its cross-domain
 mastery.</p><h2 id="a7b3" class="ko kp ed aq ap eb kq kr ks kt ku kv kw kx ky kz la" data-selectable-paragraph="">So <strong class="bc">what is Computer Vision?</strong></h2><p id="854e" class="jw jx ed aq jy b jz lb kb lc kd ld kf le kh lf kj" data-selectable-paragraph="">Here are a couple of formal textbook definitions:</p><ul class=""><li id="d661" class="jw jx ed aq jy b jz ka kb kc kd ke kf kg kh ki kj lg lh li" data-selectable-paragraph="">“the construction of explicit, meaningful descriptions of physical objects from images” (<a href="https://www.amazon.com/Computer-Vision-Dana-H-Ballard/dp/0131653164?source=post_page---------------------------" class="cz bu kk kl km kn">Ballard &amp; Brown</a>, 1982)</li><li id="cf11" class="jw jx ed aq jy b jz lj kb lk kd ll kf lm kh ln kj lg lh li" data-selectable-paragraph="">“computing properties of the 3D world from one or more digital images” (<a href="https://www.amazon.com/Introductory-Techniques-3-D-Computer-Vision/dp/0132611082?source=post_page---------------------------" class="cz bu kk kl km kn">Trucco &amp; Verri</a>, 1998)</li><li id="acca" class="jw jx ed aq jy b jz lj kb lk kd ll kf lm kh ln kj lg lh li" data-selectable-paragraph="">“to make useful decisions about real physical objects and scenes based on sensed images” (<a href="https://www.amazon.com/Computer-Vision-Linda-G-Shapiro/dp/0130307963?source=post_page---------------------------" class="cz bu kk kl km kn">Sockman &amp; Shapiro</a>, 2001)</li></ul><h2 id="825d" class="ko kp ed aq ap eb kq kr ks kt ku kv kw kx ky kz la" data-selectable-paragraph=""><strong class="bc">Why study Computer Vision?</strong></h2><p id="4b5a" class="jw jx ed aq jy b jz lb kb lc kd ld kf le kh lf kj" data-selectable-paragraph="">The
 most obvious answer is that there’s a fast-growing collection of useful
 applications derived from this field of study. Here are just a handful 
of them:</p><ul class=""><li id="dd43" class="jw jx ed aq jy b jz ka kb kc kd ke kf kg kh ki kj lg lh li" data-selectable-paragraph="">Face recognition: Snapchat and Facebook use <a class="cz bu kk kl km kn" href="https://heartbeat.fritz.ai/building-a-real-time-face-detector-in-android-with-ml-kit-f930eb7b36d9?source=post_page---------------------------">face-detection</a> algorithms to apply filters and recognize you in pictures.</li><li id="bc7a" class="jw jx ed aq jy b jz lj kb lk kd ll kf lm kh ln kj lg lh li" data-selectable-paragraph="">Image
 retrieval: Google Images uses content-based queries to search relevant 
images. The algorithms analyze the content in the query image and return
 results based on best-matched content.</li><li id="ffb7" class="jw jx ed aq jy b jz lj kb lk kd ll kf lm kh ln kj lg lh li" data-selectable-paragraph="">Gaming and controls: A great commercial product in gaming that uses stereo vision is Microsoft Kinect.</li><li id="a75f" class="jw jx ed aq jy b jz lj kb lk kd ll kf lm kh ln kj lg lh li" data-selectable-paragraph="">Surveillance: Surveillance cameras are ubiquitous at public locations and are used to detect suspicious behaviors.</li><li id="73d1" class="jw jx ed aq jy b jz lj kb lk kd ll kf lm kh ln kj lg lh li" data-selectable-paragraph="">Biometrics: Fingerprint, iris and <a href="https://support.apple.com/en-us/HT208108?source=post_page---------------------------" class="cz bu kk kl km kn">face matching</a> remains some common methods in biometric identification.</li><li id="325c" class="jw jx ed aq jy b jz lj kb lk kd ll kf lm kh ln kj lg lh li" data-selectable-paragraph="">Smart cars: Vision remains the main source of information to detect traffic signs and lights and other visual features.</li></ul><p id="06c6" class="jw jx ed aq jy b jz ka kb kc kd ke kf kg kh ki kj" data-selectable-paragraph="">I recently finished Stanford’s wonderful <a href="http://cs231n.stanford.edu/?source=post_page---------------------------" class="cz bu kk kl km kn">CS231n course</a> on using <a class="cz bu kk kl km kn" href="https://heartbeat.fritz.ai/a-beginners-guide-to-convolutional-neural-networks-cnn-cf26c5ee17ed?source=post_page---------------------------">Convolutional Neural Networks</a> for visual recognition. <mark class="pf pg ev">Visual recognition tasks such as image classification, </mark><a class="cz bu kk kl km kn" href="https://heartbeat.fritz.ai/gentle-guide-on-how-yolo-object-localization-works-with-keras-part-2-65fe59ac12d?source=post_page---------------------------"><mark class="pf pg ev">localization</mark></a><mark class="pf pg ev">, and </mark><a class="cz bu kk kl km kn" href="https://heartbeat.fritz.ai/detecting-objects-in-videos-and-camera-feeds-using-keras-opencv-and-imageai-c869fe1ebcdb?source=post_page---------------------------"><mark class="pf pg ev">detection</mark></a><mark class="pf pg ev"> are key components of Computer vision.</mark></p><p id="f7b7" class="jw jx ed aq jy b jz ka kb kc kd ke kf kg kh ki kj" data-selectable-paragraph="">Recent
 developments in neural networks and deep learning approaches have 
greatly advanced the performance of these state-of-the-art visual 
recognition systems. The course is a phenomenal resource that taught me 
the details of deep learning architectures being used in cutting-edge 
computer vision research. In this article, I want to share the 5 major 
computer vision techniques I’ve learned as well as major deep learning 
models and applications using each of them.</p><h1 id="f4aa" class="lo kp ed aq ap eb lp lq lr ls lt lu lv lw lx ly lz" data-selectable-paragraph=""><strong class="bc">1 — Image Classification</strong></h1></div><div class="ac"><figure class="ma mb mc md me fb ac paragraph-image"><div class="im n er in"><div class="mf n"><div class="dm ik gg p q gf ac bz v il"><img src="The%205%20Computer%20Vision%20Techniques%20That%20Will%20Change%20How%20You%20See%20The%20World_files/1TaXXuvQ6kBn1nCcLVlhpAA.png" class="gg p q gf ac ip iq" width="2000" height="653"></div><img class="pc pd gg p q gf ac gd" src="The%205%20Computer%20Vision%20Techniques%20That%20Will%20Change%20How%20You%20See%20The%20World_files/1TaXXuvQ6kBn1nCcLVlhpAA_002.png" width="2000" height="653"><noscript><img src="https://miro.medium.com/max/4000/1*TaXXuvQ6kBn1nCcLVlhpAA.jpeg" class="gg p q gf ac" width="2000" height="653"/></noscript></div></div></figure></div><div class="af iu ac dl w x"><p id="c398" class="jw jx ed aq jy b jz ka kb kc kd ke kf kg kh ki kj" data-selectable-paragraph="">The problem of <a class="cz bu kk kl km kn" href="https://heartbeat.fritz.ai/basics-of-image-classification-with-pytorch-2f8973c51864?source=post_page---------------------------">image classification</a>
 goes like this: Given a set of images that are all labeled with a 
single category, we’re asked to predict these categories for a novel set
 of test images and measure the accuracy of the predictions. There are a
 variety of challenges associated with this task, including viewpoint 
variation, scale variation, intra-class variation, image deformation, 
image occlusion, illumination conditions, and background clutter.</p><p id="62b7" class="jw jx ed aq jy b jz ka kb kc kd ke kf kg kh ki kj" data-selectable-paragraph="">How
 might we go about writing an algorithm that can classify images into 
distinct categories? Computer Vision researchers have come up with a 
data-driven approach to solve this. Instead of trying to specify what 
every one of the image categories of interest look like directly in 
code, they provide the computer with many examples of each image class 
and then develop learning algorithms that look at these examples and 
learn about the visual appearance of each class.</p><p id="44d4" class="jw jx ed aq jy b jz ka kb kc kd ke kf kg kh ki kj" data-selectable-paragraph="">In
 other words, they first accumulate a training dataset of labeled 
images, then feed it to the computer to process the data. Given that 
fact, the complete image classification pipeline can be formalized as 
follows:</p><ul class=""><li id="c2ad" class="jw jx ed aq jy b jz ka kb kc kd ke kf kg kh ki kj lg lh li" data-selectable-paragraph="">Our input is a training dataset that consists of <em class="mg">N</em> images, each labeled with one of <em class="mg">K</em> different classes.</li><li id="a619" class="jw jx ed aq jy b jz lj kb lk kd ll kf lm kh ln kj lg lh li" data-selectable-paragraph="">Then, we use this training set to <a class="cz bu kk kl km kn" href="https://heartbeat.fritz.ai/training-a-core-ml-model-with-turi-create-to-classify-dog-breeds-d10009bd30b6?source=post_page---------------------------">train a classifier</a> to learn what every one of the classes looks like.</li><li id="3c97" class="jw jx ed aq jy b jz lj kb lk kd ll kf lm kh ln kj lg lh li" data-selectable-paragraph="">In
 the end, we evaluate the quality of the classifier by asking it to 
predict labels for a new set of images that it’s never seen before. 
We’ll then compare the true labels of these images to the ones predicted
 by the classifier.</li></ul><p id="91cd" class="jw jx ed aq jy b jz ka kb kc kd ke kf kg kh ki kj" data-selectable-paragraph="">The most popular architecture used for image classification is <strong class="jy mh">Convolutional Neural Networks (CNNs). </strong>A
 typical use case for CNNs is where you feed the network images and the 
network classifies the data. CNNs tend to start with an input “scanner” 
which isn’t intended to parse all the training data at once. For 
example, to input an image of 100 x 100 pixels, you wouldn’t want a 
layer with 10,000 nodes.</p><p id="7544" class="jw jx ed aq jy b jz ka kb kc kd ke kf kg kh ki kj" data-selectable-paragraph="">Rather,
 you create a scanning input layer of say 10 x 10 which you feed the 
first 10 x 10 pixels of the image. Once you passed that input, you feed 
it the next 10 x 10 pixels by moving the scanner one pixel to the right.
 This technique is known as <strong class="jy mh">sliding windows</strong>.</p><figure class="ma mb mc md me fb mi y z paragraph-image"><div class="im n er in"><div class="mj n"><div class="dm ik gg p q gf ac bz v il"><img src="The%205%20Computer%20Vision%20Techniques%20That%20Will%20Change%20How%20You%20See%20The%20World_files/1VqRKWmxwIakOSnWPURoCSA_002.png" class="gg p q gf ac ip iq" width="700" height="239"></div><img class="pc pd gg p q gf ac gd" src="The%205%20Computer%20Vision%20Techniques%20That%20Will%20Change%20How%20You%20See%20The%20World_files/1VqRKWmxwIakOSnWPURoCSA.png" width="700" height="239"><noscript><img src="https://miro.medium.com/max/1400/1*VqRKWmxwIakOSnWPURoCSA.jpeg" class="gg p q gf ac" width="700" height="239"/></noscript></div></div></figure><p id="3fb7" class="jw jx ed aq jy b jz ka kb kc kd ke kf kg kh ki kj" data-selectable-paragraph="">This
 input data is then fed through convolutional layers instead of normal 
layers. Each node only concerns itself with close neighboring cells. 
These convolutional layers also tend to shrink as they become deeper, 
mostly by easily divisible factors of the input. Besides these 
convolutional layers, they also often feature <a href="https://www.coursera.org/lecture/convolutional-neural-networks/pooling-layers-hELHk?source=post_page---------------------------" class="cz bu kk kl km kn"><em class="mg">pooling layers</em></a>. Pooling is a way to filter out details: a commonly found pooling technique is <em class="mg">max pooling</em>, where we take, say, 2 x 2 pixels and pass on the pixel with the most amount of a certain attribute.</p><p id="af50" class="jw jx ed aq jy b jz ka kb kc kd ke kf kg kh ki kj" data-selectable-paragraph="">Most image classification techniques nowadays are trained on <a href="http://www.image-net.org/?source=post_page---------------------------" class="cz bu kk kl km kn"><strong class="jy mh">ImageNet</strong></a>,
 a dataset with approximately 1.2 million high-resolution training 
images. Test images will be presented with no initial annotation (no 
segmentation or labels), and algorithms will have to produce labelings 
specifying what objects are present in the images. Some of the best 
existing computer vision methods were tried on this dataset by leading 
computer vision groups from Oxford, INRIA, and XRCE. Typically, computer
 vision systems use complicated multi-stage pipelines, and the early 
stages are typically hand-tuned by optimizing a few parameters.</p><p id="e6ef" class="jw jx ed aq jy b jz ka kb kc kd ke kf kg kh ki kj" data-selectable-paragraph="">The winner of the 1st ImageNet competition, <a href="http://www.image-net.org/challenges/LSVRC/2012/supervision.pdf?source=post_page---------------------------" class="cz bu kk kl km kn">Alex Krizhevsky (NIPS 2012)</a>,
 developed a very deep convolutional neural net of the type pioneered by
 Yann LeCun. Its architecture includes 7 hidden layers, not counting 
some max pooling layers. The early layers were convolutional, while the 
last 2 layers were globally connected. The activation functions were 
rectified linear units in every hidden layer. These train much faster 
and are more expressive than logistic units. In addition to that, it 
also uses competitive normalization to suppress hidden activities when 
nearby units have stronger activities. This helps with variations in 
intensity.</p><figure class="ma mb mc md me fb mk y z paragraph-image"><div class="im n er in"><div class="ml n"><div class="dm ik gg p q gf ac bz v il"><img src="The%205%20Computer%20Vision%20Techniques%20That%20Will%20Change%20How%20You%20See%20The%20World_files/1kQEY8G7mi88orwys7CQLjw_002.png" class="gg p q gf ac ip iq" width="700" height="395"></div><img class="pc pd gg p q gf ac gd" src="The%205%20Computer%20Vision%20Techniques%20That%20Will%20Change%20How%20You%20See%20The%20World_files/1kQEY8G7mi88orwys7CQLjw.png" width="700" height="395"><noscript><img src="https://miro.medium.com/max/1400/1*kQEY8G7mi88orwys7CQLjw.jpeg" class="gg p q gf ac" width="700" height="395"/></noscript></div></div></figure><p id="edda" class="jw jx ed aq jy b jz ka kb kc kd ke kf kg kh ki kj" data-selectable-paragraph="">In
 terms of hardware requirements, Alex uses a very efficient 
implementation of convolutional nets on 2 Nvidia GTX 580 GPUs (over 1000
 fast little cores). The GPUs are very good for matrix-matrix multiplies
 and also have very high bandwidth to memory. This allows him to train 
the network in a week and makes it quick to combine results from 10 
patches at test time. We can spread a network over many cores if we can 
communicate the states fast enough. As cores get cheaper and datasets 
get bigger, big neural nets will improve faster than old-fashioned CV 
systems. Since AlexNet, there have been multiple new models using CNN as
 their backbone architecture and achieving excellent results in 
ImageNet: <a href="https://arxiv.org/pdf/1311.2901.pdf?source=post_page---------------------------" class="cz bu kk kl km kn">ZFNet</a> (2013), <a href="https://arxiv.org/pdf/1409.4842.pdf?source=post_page---------------------------" class="cz bu kk kl km kn">GoogLeNet</a> (2014), <a href="https://arxiv.org/pdf/1409.1556.pdf?source=post_page---------------------------" class="cz bu kk kl km kn">VGGNet</a> (2014), <a href="https://arxiv.org/pdf/1512.03385.pdf?source=post_page---------------------------" class="cz bu kk kl km kn">ResNet</a> (2015), <a href="https://arxiv.org/pdf/1608.06993.pdf?source=post_page---------------------------" class="cz bu kk kl km kn">DenseNet</a> (2016) etc.</p><h1 id="66f9" class="lo kp ed aq ap eb lp lq lr ls lt lu lv lw lx ly lz" data-selectable-paragraph=""><strong class="bc">2 — Object Detection</strong></h1></div><div class="ac"><figure class="ma mb mc md me fb ac paragraph-image"><div class="im n er in"><div class="mm n"><div class="dm ik gg p q gf ac bz v il"><img src="The%205%20Computer%20Vision%20Techniques%20That%20Will%20Change%20How%20You%20See%20The%20World_files/1QSgvANEHZ99_MMYqAb1eBg_002.jpeg" class="gg p q gf ac ip iq" width="1200" height="800"></div><img class="pc pd gg p q gf ac gd" src="The%205%20Computer%20Vision%20Techniques%20That%20Will%20Change%20How%20You%20See%20The%20World_files/1QSgvANEHZ99_MMYqAb1eBg.jpeg" width="1200" height="800"><noscript><img src="https://miro.medium.com/max/2400/1*QSgvANEHZ99_MMYqAb1eBg.jpeg" class="gg p q gf ac" width="1200" height="800"/></noscript></div></div></figure></div><div class="af iu ac dl w x"><p id="dbc6" class="jw jx ed aq jy b jz ka kb kc kd ke kf kg kh ki kj" data-selectable-paragraph="">The task to <a class="cz bu kk kl km kn" href="https://heartbeat.fritz.ai/detecting-objects-in-videos-and-camera-feeds-using-keras-opencv-and-imageai-c869fe1ebcdb?source=post_page---------------------------">define objects within images</a>
 usually involves outputting bounding boxes and labels for individual 
objects. This differs from the classification / localization task by 
applying classification and localization to many objects instead of just
 a single dominant object. You only have 2 classes of object 
classification, which means object bounding boxes and non-object 
bounding boxes. For example, in car detection, you have to detect all 
cars in a given image with their bounding boxes.</p><p id="46b6" class="jw jx ed aq jy b jz ka kb kc kd ke kf kg kh ki kj" data-selectable-paragraph="">If
 we use the Sliding Window technique like the way we classify and 
localize images, we need to apply a CNN to many different crops of the 
image. Because CNN classifies each crop as object or background, we need
 to apply CNN to huge numbers of locations and scales, which is very 
computationally expensive!</p><figure class="ma mb mc md me fb gd mn bs mo mp mq mr ms bd mt mu mv mw mx my paragraph-image"><div class="y z mz"><div class="im n er in"><div class="na n"><div class="dm ik gg p q gf ac bz v il"><img src="The%205%20Computer%20Vision%20Techniques%20That%20Will%20Change%20How%20You%20See%20The%20World_files/1jIjsYKmgH5nykbqQ6Jyfhg_002.png" class="gg p q gf ac ip iq" width="500" height="318"></div><img class="pc pd gg p q gf ac gd" src="The%205%20Computer%20Vision%20Techniques%20That%20Will%20Change%20How%20You%20See%20The%20World_files/1jIjsYKmgH5nykbqQ6Jyfhg.png" width="500" height="318"><noscript><img src="https://miro.medium.com/max/1000/1*jIjsYKmgH5nykbqQ6Jyfhg.jpeg" class="gg p q gf ac" width="500" height="318"/></noscript></div></div></div></figure><p id="afa4" class="jw jx ed aq jy b jz ka kb kc kd ke kf kg kh ki kj" data-selectable-paragraph="">In order to cope with this, neural network researchers have proposed to use <strong class="jy mh">regions</strong> instead, where we find “blobby” image regions that are likely to contain objects.</p><p id="4fe2" class="jw jx ed aq jy b jz ka kb kc kd ke kf kg kh ki kj" data-selectable-paragraph="">This is relatively fast to run. The first model that kicked things off was <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf?source=post_page---------------------------" class="cz bu kk kl km kn"><strong class="jy mh">R-CNN</strong></a>(Region-based
 Convolutional Neural Network). In a R-CNN, we first scan the input 
image for possible objects using an algorithm called Selective Search, 
generating ~2,000 region proposals. Then we run a CNN on top of each of 
these region proposals. Finally, we take the output of each CNN and feed
 it into an SVM to classify the region and a linear regression to 
tighten the bounding box of the object.</p><p id="28c7" class="jw jx ed aq jy b jz ka kb kc kd ke kf kg kh ki kj" data-selectable-paragraph="">Essentially,
 we turned object detection into an image classification problem. 
However, there are some problems — the training is slow, a lot of disk 
space is required, and inference is also slow.</p><p id="86e4" class="jw jx ed aq jy b jz ka kb kc kd ke kf kg kh ki kj" data-selectable-paragraph="">An immediate descendant to R-CNN is <a href="https://arxiv.org/pdf/1504.08083.pdf?source=post_page---------------------------" class="cz bu kk kl km kn"><strong class="jy mh">Fast R-CNN</strong></a>,
 which improves the detection speed through 2 augmentations: 1) 
Performing feature extraction before proposing regions, thus only 
running one CNN over the entire image, and 2) Replacing SVM with a 
softmax layer, thus extending the neural network for predictions instead
 of creating a new model.</p><figure class="ma mb mc md me fb gd mn bs mo mp mq mr ms bd mt mu mv mw mx my paragraph-image"><div class="y z nb"><div class="im n er in"><div class="nc n"><div class="dm ik gg p q gf ac bz v il"><img src="The%205%20Computer%20Vision%20Techniques%20That%20Will%20Change%20How%20You%20See%20The%20World_files/1KD6dHKVvLX7fQIk6PM_cKw.png" class="gg p q gf ac ip iq" width="500" height="319"></div><img class="pc pd gg p q gf ac gd" src="The%205%20Computer%20Vision%20Techniques%20That%20Will%20Change%20How%20You%20See%20The%20World_files/1KD6dHKVvLX7fQIk6PM_cKw_002.png" width="500" height="319"><noscript><img src="https://miro.medium.com/max/1000/1*KD6dHKVvLX7fQIk6PM_cKw.jpeg" class="gg p q gf ac" width="500" height="319"/></noscript></div></div></div></figure><p id="c5ce" class="jw jx ed aq jy b jz ka kb kc kd ke kf kg kh ki kj" data-selectable-paragraph="">Fast
 R-CNN performed much better in terms of speed, because it trains just 
one CNN for the entire image. However, the selective search algorithm is
 still taking a lot of time to generate region proposals.</p><p id="d5cb" class="jw jx ed aq jy b jz ka kb kc kd ke kf kg kh ki kj" data-selectable-paragraph="">Thus comes the invention of <a href="https://arxiv.org/pdf/1506.01497.pdf?source=post_page---------------------------" class="cz bu kk kl km kn"><strong class="jy mh">Faster R-CNN</strong></a>,
 which now is a canonical model for deep learning-based object 
detection. It replaces the slow selective search algorithm with a fast 
neural network by inserting a <a href="https://medium.com/@tanaykarmarkar/region-proposal-network-rpn-backbone-of-faster-r-cnn-4a744a38d7f9?source=post_page---------------------------" class="cz bu kk kl km kn"><strong class="jy mh">Region Proposal Network</strong> </a>(RPN)
 to predict proposals from features. The RPN is used to decide “where” 
to look in order to reduce the computational requirements of the overall
 inference process. The RPN quickly and efficiently scans every location
 in order to assess whether further processing needs to be carried out 
in a given region. It does that by outputting <em class="mg">k</em> bounding box proposals each with 2 scores representing the probability of object or not at each location.</p><figure class="ma mb mc md me fb gd mn bs mo mp mq mr ms bd mt mu mv mw mx my paragraph-image"><div class="y z nd"><div class="im n er in"><div class="ne n"><div class="dm ik gg p q gf ac bz v il"><img src="The%205%20Computer%20Vision%20Techniques%20That%20Will%20Change%20How%20You%20See%20The%20World_files/15KOZC81rRy7GsTLzXkNjYg_002.png" class="gg p q gf ac ip iq" width="500" height="449"></div><img class="pc pd gg p q gf ac gd" src="The%205%20Computer%20Vision%20Techniques%20That%20Will%20Change%20How%20You%20See%20The%20World_files/15KOZC81rRy7GsTLzXkNjYg.png" width="500" height="449"><noscript><img src="https://miro.medium.com/max/1000/1*5KOZC81rRy7GsTLzXkNjYg.jpeg" class="gg p q gf ac" width="500" height="449"/></noscript></div></div></div></figure><p id="cadb" class="jw jx ed aq jy b jz ka kb kc kd ke kf kg kh ki kj" data-selectable-paragraph="">Once
 we have our region proposals, we feed them straight into what is 
essentially a Fast R-CNN. We add a pooling layer, some fully-connected 
layers, and finally a softmax classification layer and bounding box 
regressor.</p><p id="55a5" class="jw jx ed aq jy b jz ka kb kc kd ke kf kg kh ki kj" data-selectable-paragraph="">Altogether,
 Faster R-CNN achieved much better speeds and higher accuracy. It’s 
worth noting that although future models did a lot to increase detection
 speeds, few models managed to outperform Faster R-CNN by a significant 
margin. In other words, Faster R-CNN may not be the simplest or fastest 
method for object detection, but it’s still one of the best performing.</p><p id="13f0" class="jw jx ed aq jy b jz ka kb kc kd ke kf kg kh ki kj" data-selectable-paragraph="">Major
 Object Detection trends in recent years have shifted towards quicker, 
more efficient detection systems. This was visible in approaches like <a href="http://lanl.arxiv.org/pdf/1612.08242v1?source=post_page---------------------------" class="cz bu kk kl km kn">You Only Look Once</a> (YOLO), <a href="http://lanl.arxiv.org/pdf/1512.02325v5?source=post_page---------------------------" class="cz bu kk kl km kn">Single Shot MultiBox Detector</a> (SSD), and <a href="http://lanl.arxiv.org/pdf/1605.06409v2?source=post_page---------------------------" class="cz bu kk kl km kn">Region-Based Fully Convolutional Networks</a>
 (R-FCN) as a move towards sharing computation on a whole image. Hence, 
these approaches differentiate themselves from the costly subnetworks 
associated with the 3 R-CNN techniques. The main rationale behind these 
trends is to avoid having separate algorithms focus on their respective 
subproblems in isolation, as this typically increases training time and 
can lower network accuracy.</p></div></section><hr class="nf ct hd ng nh hy ni nj nk nl nm"><section class="ie if ig ih ii"><div class="af iu ac dl w x"><blockquote class="nn"><div id="ebe0" class="no np nq aq ix b nr ns nt nu nv nw kj" data-selectable-paragraph=""><p class="ix b nx cg au">Passionate
 about machine learning? Same! We’re curating each week’s biggest 
stories, best tutorials, and latest research so you don’t have to. <a href="https://www.deeplearningweekly.com/newsletter?utm_campaign=dlweekly-newsletter-timesaver4&amp;utm_source=heartbeat&amp;source=post_page---------------------------" class="cz bu kk kl km kn">Sign up</a> for weekly updates delivered to your inbox.</p></div></blockquote></div></section><hr class="nf ct hd ng nh hy ni nj nk nl nm"><section class="ie if ig ih ii"><div class="af iu ac dl w x"><h1 id="f87f" class="lo kp ed aq ap eb lp ny lr nz lt oa lv ob lx oc lz" data-selectable-paragraph=""><strong class="bc">3 — Object Tracking</strong></h1></div><div class="ac"><figure class="ma mb mc md me fb ac paragraph-image"><div class="im n er in"><div class="od n"><div class="dm ik gg p q gf ac bz v il"><img src="The%205%20Computer%20Vision%20Techniques%20That%20Will%20Change%20How%20You%20See%20The%20World_files/1ecz875HfaF_7S7hPsTc6pA.png" class="gg p q gf ac ip iq" width="1112" height="522"></div><img class="pc pd gg p q gf ac gd" src="The%205%20Computer%20Vision%20Techniques%20That%20Will%20Change%20How%20You%20See%20The%20World_files/1ecz875HfaF_7S7hPsTc6pA_002.png" width="1112" height="522"><noscript><img src="https://miro.medium.com/max/2224/1*ecz875HfaF_7S7hPsTc6pA.jpeg" class="gg p q gf ac" width="1112" height="522"/></noscript></div></div></figure></div><div class="af iu ac dl w x"><p id="9b83" class="jw jx ed aq jy b jz ka kb kc kd ke kf kg kh ki kj" data-selectable-paragraph=""><a href="https://www.pyimagesearch.com/2018/07/23/simple-object-tracking-with-opencv/?source=post_page---------------------------" class="cz bu kk kl km kn">Object Tracking </a>refers
 to the process of following a specific object of interest, or multiple 
objects, in a given scene. It traditionally has applications in video 
and real-world interactions where observations are made following an 
initial object detection. Now, it’s crucial to autonomous driving 
systems such as self-driving vehicles from companies like Uber and 
Tesla.</p><p id="acf7" class="jw jx ed aq jy b jz ka kb kc kd ke kf kg kh ki kj" data-selectable-paragraph="">Object
 Tracking methods can be divided into 2 categories according to the 
observation model: generative method and discriminative method. The 
generative method uses the generative model to describe the apparent 
characteristics and minimizes the reconstruction error to search the 
object, such as PCA.</p><p id="f7e8" class="jw jx ed aq jy b jz ka kb kc kd ke kf kg kh ki kj" data-selectable-paragraph="">The
 discriminative method can be used to distinguish between the object and
 the background, its performance is more robust, and it gradually 
becomes the main method in tracking. The discriminative method is also 
referred to as Tracking-by-Detection, and deep learning belongs to this 
category. To achieve tracking-by-detection, we detect candidate objects 
for all frames and use deep learning to recognize the wanted object from
 the candidates. There are 2 kinds of basic network models that can be 
used: <strong class="jy mh">stacked auto encoders (SAE)</strong> and <strong class="jy mh">convolutional neural network (CNN).</strong></p><p id="8604" class="jw jx ed aq jy b jz ka kb kc kd ke kf kg kh ki kj" data-selectable-paragraph="">The most popular deep network for tracking tasks using SAE is <a href="https://papers.nips.cc/paper/5192-learning-a-deep-compact-image-representation-for-visual-tracking.pdf?source=post_page---------------------------" class="cz bu kk kl km kn"><strong class="jy mh">Deep Learning Tracker</strong></a><strong class="jy mh">,</strong> which proposes offline pre-training and online fine-tuning the net. The process works like this:</p><ul class=""><li id="20a8" class="jw jx ed aq jy b jz ka kb kc kd ke kf kg kh ki kj lg lh li" data-selectable-paragraph="">Off-line
 unsupervised pre-train the stacked denoising auto-encoder using 
large-scale natural image datasets to obtain the general object 
representation. Stacked denoising auto-encoder can obtain more robust 
feature expression ability by adding noise in input images and 
reconstructing the original images.</li><li id="0010" class="jw jx ed aq jy b jz lj kb lk kd ll kf lm kh ln kj lg lh li" data-selectable-paragraph="">Combine
 the coding part of the pre-trained network with a classifier to get the
 classification network, then use the positive and negative samples 
obtained from the initial frame to fine-tune the network, which can 
discriminate the current object and background. DLT uses particle filter
 as the motion model to produce candidate patches of the current frame. 
The classification network outputs the probability scores for these 
patches, meaning the confidence of their classifications, then chooses 
the highest of these patches as the object.</li><li id="fc85" class="jw jx ed aq jy b jz lj kb lk kd ll kf lm kh ln kj lg lh li" data-selectable-paragraph="">In the model updating, DLT uses the way of limited threshold.</li></ul><figure class="ma mb mc md me fb oe y z paragraph-image"><div class="im n er in"><div class="of n"><div class="dm ik gg p q gf ac bz v il"><img src="The%205%20Computer%20Vision%20Techniques%20That%20Will%20Change%20How%20You%20See%20The%20World_files/1z94UOY-jMke-nZJWh3ZoKA.png" class="gg p q gf ac ip iq" width="700" height="788"></div><img class="pc pd gg p q gf ac gd" src="The%205%20Computer%20Vision%20Techniques%20That%20Will%20Change%20How%20You%20See%20The%20World_files/1z94UOY-jMke-nZJWh3ZoKA_002.png" width="700" height="788"><noscript><img src="https://miro.medium.com/max/1400/1*z94UOY-jMke-nZJWh3ZoKA.jpeg" class="gg p q gf ac" width="700" height="788"/></noscript></div></div></figure><p id="1ab2" class="jw jx ed aq jy b jz ka kb kc kd ke kf kg kh ki kj" data-selectable-paragraph="">Because
 of its superiority in image classification and object detection, CNN 
has become the mainstream deep model in computer vision and in visual 
tracking. Generally speaking, a large-scale CNN can be trained both as a
 classifier and as a tracker. 2 representative CNN-based tracking 
algorithms are <a href="https://pdfs.semanticscholar.org/bf94/906f0d7a8ca9da5f6b86e2a476fde1a34dd0.pdf?source=post_page---------------------------" class="cz bu kk kl km kn"><strong class="jy mh">fully-convolutional network tracker</strong></a> <strong class="jy mh">(FCNT)</strong> and <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Nam_Learning_Multi-Domain_Convolutional_CVPR_2016_paper.pdf?source=post_page---------------------------" class="cz bu kk kl km kn"><strong class="jy mh">multi-domain CNN</strong></a> <strong class="jy mh">(MD Net).</strong></p><p id="a910" class="jw jx ed aq jy b jz ka kb kc kd ke kf kg kh ki kj" data-selectable-paragraph="">FCNT analyzes and takes advantage of the feature maps of the <a href="http://www.robots.ox.ac.uk/~vgg/research/very_deep/?source=post_page---------------------------" class="cz bu kk kl km kn">VGG model</a> successfully, which is a pre-trained ImageNet, and results in the following observations:</p><ul class=""><li id="153f" class="jw jx ed aq jy b jz ka kb kc kd ke kf kg kh ki kj lg lh li" data-selectable-paragraph="">CNN feature maps can be used for localization and tracking.</li><li id="db9f" class="jw jx ed aq jy b jz lj kb lk kd ll kf lm kh ln kj lg lh li" data-selectable-paragraph="">Many CNN feature maps are noisy or un-related for the task of discriminating a particular object from its background.</li><li id="01ea" class="jw jx ed aq jy b jz lj kb lk kd ll kf lm kh ln kj lg lh li" data-selectable-paragraph="">Higher
 layers capture semantic concepts on object categories, whereas lower 
layers encode more discriminative features to capture intra-class 
variation.</li></ul><p id="e866" class="jw jx ed aq jy b jz ka kb kc kd ke kf kg kh ki kj" data-selectable-paragraph="">Because
 of these observations, FCNT designs the feature selection network to 
select the most relevant feature maps on the conv4–3 and conv5–3 layers 
of the VGG network. Then in order to avoid overfitting on noisy ones, it
 also designs extra two channels (called SNet and GNet) for the selected
 feature maps from two layers’ separately. The <strong class="jy mh">GNet</strong> captures the category information of the object, while the <strong class="jy mh">SNet</strong> discriminates the object from a background with a similar appearance.</p><p id="e192" class="jw jx ed aq jy b jz ka kb kc kd ke kf kg kh ki kj" data-selectable-paragraph="">Both
 of the networks are initialized with the given bounding-box in the 
first frame to get heat maps of the object, and for new frames, a region
 of interest (ROI) centered at the object location in the last frame is 
cropped and propagated. At last, through SNet and GNet, the classifier 
gets two heat maps for prediction, and the tracker decides which heat 
map will be used to generate the final tracking result according to 
whether there are distractors. The pipeline of FCNT is shown below.</p><figure class="ma mb mc md me fb mk y z paragraph-image"><div class="im n er in"><div class="og n"><div class="dm ik gg p q gf ac bz v il"><img src="The%205%20Computer%20Vision%20Techniques%20That%20Will%20Change%20How%20You%20See%20The%20World_files/1tGDaMhb--A2VODKkL3IyYQ_002.png" class="gg p q gf ac ip iq" width="700" height="198"></div><img class="pc pd gg p q gf ac gd" src="The%205%20Computer%20Vision%20Techniques%20That%20Will%20Change%20How%20You%20See%20The%20World_files/1tGDaMhb--A2VODKkL3IyYQ.png" width="700" height="198"><noscript><img src="https://miro.medium.com/max/1400/1*tGDaMhb--A2VODKkL3IyYQ.jpeg" class="gg p q gf ac" width="700" height="198"/></noscript></div></div></figure><p id="ad1d" class="jw jx ed aq jy b jz ka kb kc kd ke kf kg kh ki kj" data-selectable-paragraph="">Different
 from the idea of FCNT, MD Net uses all the sequences of a video to to 
track movements in them. The networks mentioned above use irrelevant 
image data to reduce the training demand of tracking data, and this idea
 has some deviation from tracking. The object of one class in this video
 can be the background in another video, so MD Net proposes the idea of 
multi-domain to distinguish the object and background in every domain 
independently. And a domain indicates a set of videos that contain the 
same kind of object.</p><p id="c4c9" class="jw jx ed aq jy b jz ka kb kc kd ke kf kg kh ki kj" data-selectable-paragraph="">As
 shown below, MD Net is divided into 2 parts: the shared layers and the K
 branches of domain-specific layers. Each branch contains a binary 
classification layer with softmax loss, which is used to distinguish the
 object and background in each domain, and the shared layers sharing 
with all domains to ensure the general representation.</p><figure class="ma mb mc md me fb oh y z paragraph-image"><div class="im n er in"><div class="oi n"><div class="dm ik gg p q gf ac bz v il"><img src="The%205%20Computer%20Vision%20Techniques%20That%20Will%20Change%20How%20You%20See%20The%20World_files/1HL4OhGMRtbHuy7v8depXjg_002.png" class="gg p q gf ac ip iq" width="700" height="290"></div><img class="pc pd gg p q gf ac gd" src="The%205%20Computer%20Vision%20Techniques%20That%20Will%20Change%20How%20You%20See%20The%20World_files/1HL4OhGMRtbHuy7v8depXjg.png" width="700" height="290"><noscript><img src="https://miro.medium.com/max/1400/1*HL4OhGMRtbHuy7v8depXjg.jpeg" class="gg p q gf ac" width="700" height="290"/></noscript></div></div></figure><p id="85a6" class="jw jx ed aq jy b jz ka kb kc kd ke kf kg kh ki kj" data-selectable-paragraph="">In
 recent years, deep learning researchers have tried different ways to 
adapt to features of the visual tracking task. There are many directions
 that have been explored: applying other network models such as <a class="cz bu kk kl km kn" href="https://heartbeat.fritz.ai/detecting-the-language-of-a-persons-name-using-pytorch-rnn-29a9090c20f2?source=post_page---------------------------">Recurrent Neural Net</a> and <a href="https://codeburst.io/deep-learning-deep-belief-network-fundamentals-d0dcfd80d7d4?source=post_page---------------------------" class="cz bu kk kl km kn">Deep Belief Net</a>,
 designing the network structure to adapt to video processing and 
end-to-end learning, optimizing the process, structure, and parameters, 
or even combining deep learning with traditional methods of computer 
vision or approaches in other fields such as <a class="cz bu kk kl km kn" href="https://heartbeat.fritz.ai/the-7-nlp-techniques-that-will-change-how-you-communicate-in-the-future-part-i-f0114b2f0497?source=post_page---------------------------">Language Processing</a> and Speech Recognition.</p><h1 id="27ed" class="lo kp ed aq ap eb lp lq lr ls lt lu lv lw lx ly lz" data-selectable-paragraph=""><strong class="bc">4 — Semantic Segmentation</strong></h1></div><div class="ac"><figure class="ma mb mc md me fb ac paragraph-image"><div class="im n er in"><div class="oj n"><div class="dm ik gg p q gf ac bz v il"><img src="The%205%20Computer%20Vision%20Techniques%20That%20Will%20Change%20How%20You%20See%20The%20World_files/10V2fYKOROa4nCuj3Mi3DgQ_002.jpeg" class="gg p q gf ac ip iq" width="1280" height="720"></div><img class="pc pd gg p q gf ac gd" src="The%205%20Computer%20Vision%20Techniques%20That%20Will%20Change%20How%20You%20See%20The%20World_files/10V2fYKOROa4nCuj3Mi3DgQ.jpeg" width="1280" height="720"><noscript><img src="https://miro.medium.com/max/2560/1*0V2fYKOROa4nCuj3Mi3DgQ.jpeg" class="gg p q gf ac" width="1280" height="720"/></noscript></div></div></figure></div><div class="af iu ac dl w x"><p id="5468" class="jw jx ed aq jy b jz ka kb kc kd ke kf kg kh ki kj" data-selectable-paragraph="">Central to Computer Vision is the process of <a class="cz bu kk kl km kn" href="https://heartbeat.fritz.ai/building-an-image-segmentation-app-in-ios-3377eb4a3e7c?source=post_page---------------------------">Segmentation</a>, which divides whole images into pixel groupings which can then be labelled and classified.</p><p id="4ee4" class="jw jx ed aq jy b jz ka kb kc kd ke kf kg kh ki kj" data-selectable-paragraph="">Particularly,
 Semantic Segmentation tries to semantically understand the role of each
 pixel in the image (e.g. is it a car, a motorbike, or some other type 
of class?). For example, in the picture above, apart from recognizing 
the person, the road, the cars, the trees, etc., we also have to 
delineate the boundaries of each object. Therefore, unlike 
classification, we need dense pixel-wise predictions from our models.</p><p id="9c70" class="jw jx ed aq jy b jz ka kb kc kd ke kf kg kh ki kj" data-selectable-paragraph="">As
 with other computer vision tasks, CNNs have had enormous success on 
segmentation problems. One of the popular initial approaches was patch 
classification through a sliding window, where each pixel was separately
 classified into classes using a patch of images around it. This, 
however, is very inefficient computationally because we don’t reuse the 
shared features between overlapping patches.</p><p id="9580" class="jw jx ed aq jy b jz ka kb kc kd ke kf kg kh ki kj" data-selectable-paragraph="">The solution, instead, is UC Berkeley’s <a href="https://arxiv.org/pdf/1411.4038.pdf?source=post_page---------------------------" class="cz bu kk kl km kn"><strong class="jy mh">Fully Convolutional Networks</strong></a> <strong class="jy mh">(FCN), </strong>which
 popularized end-to-end CNN architectures for dense predictions without 
any fully connected layers. This allowed segmentation maps to be 
generated for images of any size and was also much faster compared to 
the patch classification approach. Almost all subsequent approaches to 
semantic segmentation adopted this paradigm.</p><figure class="ma mb mc md me fb oe y z paragraph-image"><div class="im n er in"><div class="ok n"><div class="dm ik gg p q gf ac bz v il"><img src="The%205%20Computer%20Vision%20Techniques%20That%20Will%20Change%20How%20You%20See%20The%20World_files/1_k5SCYeFy43b_CFv_zFimQ_002.png" class="gg p q gf ac ip iq" width="700" height="363"></div><img class="pc pd gg p q gf ac gd" src="The%205%20Computer%20Vision%20Techniques%20That%20Will%20Change%20How%20You%20See%20The%20World_files/1_k5SCYeFy43b_CFv_zFimQ.png" width="700" height="363"><noscript><img src="https://miro.medium.com/max/1400/1*_k5SCYeFy43b_CFv_zFimQ.jpeg" class="gg p q gf ac" width="700" height="363"/></noscript></div></div></figure><p id="c00e" class="jw jx ed aq jy b jz ka kb kc kd ke kf kg kh ki kj" data-selectable-paragraph="">However,
 one problem remains: convolutions at original image resolution will be 
very expensive. To deal with this, FCN uses downsampling and upsampling 
inside the network. The downsampling layer is known as striped 
convolution, while the upsampling layer is known as transposed 
convolution.</p><p id="4f0d" class="jw jx ed aq jy b jz ka kb kc kd ke kf kg kh ki kj" data-selectable-paragraph="">Despite the upsampling/downsampling layers, FCN produces coarse segmentation maps because of information loss during pooling. <a href="https://arxiv.org/pdf/1511.00561.pdf?source=post_page---------------------------" class="cz bu kk kl km kn">SegNet</a>
 is a more memory efficient architecture than FCN that uses-max pooling 
and an encoder-decoder framework. In SegNet, shortcut/skip connections 
are introduced from higher resolution feature maps to improve the 
coarseness of upsampling/downsampling.</p><figure class="ma mb mc md me fb ol y z paragraph-image"><div class="im n er in"><div class="om n"><div class="dm ik gg p q gf ac bz v il"><img src="The%205%20Computer%20Vision%20Techniques%20That%20Will%20Change%20How%20You%20See%20The%20World_files/11-ulho5NzNJhq6YNR9KREg_002.png" class="gg p q gf ac ip iq" width="700" height="195"></div><img class="pc pd gg p q gf ac gd" src="The%205%20Computer%20Vision%20Techniques%20That%20Will%20Change%20How%20You%20See%20The%20World_files/11-ulho5NzNJhq6YNR9KREg.png" width="700" height="195"><noscript><img src="https://miro.medium.com/max/1400/1*1-ulho5NzNJhq6YNR9KREg.jpeg" class="gg p q gf ac" width="700" height="195"/></noscript></div></div></figure><p id="7b6b" class="jw jx ed aq jy b jz ka kb kc kd ke kf kg kh ki kj" data-selectable-paragraph="">Recent research in Semantic Segmentation all relies heavily on fully convolutional networks, such as <a href="https://arxiv.org/pdf/1511.07122.pdf?source=post_page---------------------------" class="cz bu kk kl km kn">Dilated Convolutions</a>, <a href="https://arxiv.org/pdf/1412.7062.pdf?source=post_page---------------------------" class="cz bu kk kl km kn">DeepLab</a>, and <a href="https://arxiv.org/pdf/1611.06612.pdf?source=post_page---------------------------" class="cz bu kk kl km kn">RefineNet</a>.</p><h1 id="da80" class="lo kp ed aq ap eb lp lq lr ls lt lu lv lw lx ly lz" data-selectable-paragraph=""><strong class="bc">5 — Instance Segmentation</strong></h1></div><div class="ac"><figure class="ma mb mc md me fb ac paragraph-image"><div class="im n er in"><div class="on n"><div class="dm ik gg p q gf ac bz v il"><img src="The%205%20Computer%20Vision%20Techniques%20That%20Will%20Change%20How%20You%20See%20The%20World_files/1pDJ1P9Rv-jcas51SZsVt4A_002.jpeg" class="gg p q gf ac ip iq" width="1048" height="600"></div><img class="pc pd gg p q gf ac gd" src="The%205%20Computer%20Vision%20Techniques%20That%20Will%20Change%20How%20You%20See%20The%20World_files/1pDJ1P9Rv-jcas51SZsVt4A.jpeg" width="1048" height="600"><noscript><img src="https://miro.medium.com/max/2096/1*pDJ1P9Rv-jcas51SZsVt4A.jpeg" class="gg p q gf ac" width="1048" height="600"/></noscript></div></div></figure></div><div class="af iu ac dl w x"><p id="7839" class="jw jx ed aq jy b jz ka kb kc kd ke kf kg kh ki kj" data-selectable-paragraph="">Beyond
 Semantic Segmentation, Instance Segmentation segments different 
instances of classes, such as labelling 5 cars with 5 different colors. 
In classification, there’s generally an image with a single object as 
the focus and the task is to say what that image is. But in order to 
segment instances, we need to carry out far more complex tasks. We see 
complicated sights with multiple overlapping objects and different 
backgrounds, and we not only classify these different objects but also 
identify their boundaries, differences, and relations to one another!</p><figure class="ma mb mc md me fb gd mn bs mo mp mq mr ms bd mt mu mv mw mx my paragraph-image"><div class="y z oo"><div class="im n er in"><div class="op n"><div class="dm ik gg p q gf ac bz v il"><img src="The%205%20Computer%20Vision%20Techniques%20That%20Will%20Change%20How%20You%20See%20The%20World_files/1ClYLqVgNwZP_nUON061x_w_002.png" class="gg p q gf ac ip iq" width="500" height="378"></div><img class="pc pd gg p q gf ac gd" src="The%205%20Computer%20Vision%20Techniques%20That%20Will%20Change%20How%20You%20See%20The%20World_files/1ClYLqVgNwZP_nUON061x_w.png" width="500" height="378"><noscript><img src="https://miro.medium.com/max/1000/1*ClYLqVgNwZP_nUON061x_w.jpeg" class="gg p q gf ac" width="500" height="378"/></noscript></div></div></div></figure><p id="fb61" class="jw jx ed aq jy b jz ka kb kc kd ke kf kg kh ki kj" data-selectable-paragraph="">So
 far, we’ve seen how to use CNN features in many interesting ways to 
effectively locate different objects in an image with bounding boxes. 
Can we extend such techniques to locate exact pixels of each object 
instead of just bounding boxes? This instance segmentation problem is 
explored at Facebook AI using an architecture known as <a href="https://arxiv.org/pdf/1703.06870.pdf?source=post_page---------------------------" class="cz bu kk kl km kn"><strong class="jy mh">Mask R-CNN</strong></a>.</p><p id="9678" class="jw jx ed aq jy b jz ka kb kc kd ke kf kg kh ki kj" data-selectable-paragraph="">Much
 like Fast R-CNN, and Faster R-CNN, Mask R-CNN’s underlying intuition is
 straightforward Given that Faster R-CNN works so well for object 
detection, could we extend it to also carry out pixel-level 
segmentation?</p><p id="7c24" class="jw jx ed aq jy b jz ka kb kc kd ke kf kg kh ki kj" data-selectable-paragraph="">Mask
 R-CNN does this by adding a branch to Faster R-CNN that outputs a 
binary mask that says whether or not a given pixel is part of an object.
 The branch is a Fully Convolutional Network on top of a CNN-based 
feature map. Given the CNN Feature Map as the input, the network outputs
 a matrix with 1s on all locations where the pixel belongs to the object
 and 0s elsewhere (this is known as a <a href="https://en.wikipedia.org/wiki/Mask_%28computing%29?source=post_page---------------------------" class="cz bu kk kl km kn">binary mask</a>).</p><figure class="ma mb mc md me fb oq y z paragraph-image"><div class="im n er in"><div class="or n"><div class="dm ik gg p q gf ac bz v il"><img src="The%205%20Computer%20Vision%20Techniques%20That%20Will%20Change%20How%20You%20See%20The%20World_files/1QgOk_xUmBM-_MlWSXBK-Dg.png" class="gg p q gf ac ip iq" width="692" height="300"></div><img class="pc pd gg p q gf ac gd" src="The%205%20Computer%20Vision%20Techniques%20That%20Will%20Change%20How%20You%20See%20The%20World_files/1QgOk_xUmBM-_MlWSXBK-Dg_002.png" width="692" height="300"><noscript><img src="https://miro.medium.com/max/1384/1*QgOk_xUmBM-_MlWSXBK-Dg.jpeg" class="gg p q gf ac" width="692" height="300"/></noscript></div></div></figure><p id="dbeb" class="jw jx ed aq jy b jz ka kb kc kd ke kf kg kh ki kj" data-selectable-paragraph="">Additionally,
 when run without modifications on the original Faster R-CNN 
architecture, the regions of the feature map selected by RoIPool (Region
 of Interests Pool) were slightly misaligned from the regions of the 
original image. Since image segmentation requires pixel-level 
specificity, unlike bounding boxes, this naturally led to inaccuracies. 
Mask R-CNN solves this problem by adjusting RoIPool to be more precisely
 aligned using a method known as <strong class="jy mh">RoIAlign</strong>
 (Region of Interests Align). Essentially, RoIAlign uses bilinear 
interpolation to avoid error in rounding, which causes inaccuracies in 
detection and segmentation.</p><p id="4cd4" class="jw jx ed aq jy b jz ka kb kc kd ke kf kg kh ki kj" data-selectable-paragraph="">Once
 these masks are generated, Mask R-CNN combines them with the 
classifications and bounding boxes from Faster R-CNN to generate such 
wonderfully precise segmentations:</p></div><div class="af iu ac ae"><figure class="ma mb mc md me fb os ot ou paragraph-image"><div class="im n er in"><div class="ov n"><div class="pc pd gg p q gf ac bz v il"><img src="The%205%20Computer%20Vision%20Techniques%20That%20Will%20Change%20How%20You%20See%20The%20World_files/1fbDDJ5z8q5xaZ4BhiQGDIw.png" class="gg p q gf ac ip iq" width="1000" height="539"></div><img class="dm ik gg p q gf ac gd" width="1000" height="539"><noscript><img src="https://miro.medium.com/max/2000/1*fbDDJ5z8q5xaZ4BhiQGDIw.jpeg" class="gg p q gf ac" width="1000" height="539"/></noscript></div></div></figure></div><div class="af iu ac dl w x"><h1 id="6373" class="lo kp ed aq ap eb lp ny lr nz lt oa lv ob lx oc lz" data-selectable-paragraph=""><strong class="bc">Conclusion</strong></h1><p id="c592" class="jw jx ed aq jy b jz lb kb lc kd ld kf le kh lf kj" data-selectable-paragraph="">These
 5 major computer vision techniques can help a computer extract, 
analyze, and understand useful information from a single or a sequence 
of images. There are many other advanced techniques that I haven’t 
touched, including <a class="cz bu kk kl km kn" href="https://heartbeat.fritz.ai/20-minute-masterpiece-4b6043fdfff5?source=post_page---------------------------">style transfer</a>, colorization, action recognition, <a class="cz bu kk kl km kn" href="https://heartbeat.fritz.ai/3d-face-reconstruction-with-position-map-regression-networks-36f0ac2d3ef1?source=post_page---------------------------">3D objects</a>, human pose estimation, and more.</p><p id="d53b" class="jw jx ed aq jy b jz ka kb kc kd ke kf kg kh ki kj" data-selectable-paragraph="">Indeed,
 the field of Computer Vision is too expensive to cover in depth, and I 
would encourage you to explore it further, whether through online 
courses, blog tutorials, or formal documents. I’d highly recommend 
CS231n for starters, as you’ll learn to implement, train, and debug your
 own neural networks. As a bonus, you can get all the lecture slides and
 assignment guidelines from <a href="https://github.com/khanhnamle1994/computer-vision?source=post_page---------------------------" class="cz bu kk kl km kn"><strong class="jy mh">my GitHub repository</strong></a>. I hope it’ll guide you in the quest of changing how to see the world!</p><p id="6a6b" class="jw jx ed aq jy b jz ka kb kc kd ke kf kg kh ki kj" data-selectable-paragraph=""><em class="mg">If you enjoyed this piece, I’d love it if you hit the clap button</em> 👏 <em class="mg">so others might stumble upon it. You can find my own code on</em> <a href="https://github.com/khanhnamle1994?source=post_page---------------------------" class="cz bu kk kl km kn"><em class="mg">GitHub</em></a><em class="mg">, and more of my writing and projects at</em> <a href="https://jameskle.com/?source=post_page---------------------------" class="cz bu kk kl km kn"><em class="mg">https://jameskle.com/</em></a><em class="mg">. You can also follow me on </em><a href="https://twitter.com/@james_aka_yale?source=post_page---------------------------" class="cz bu kk kl km kn"><em class="mg">Twitter</em></a><em class="mg">, </em><a href="mailto:khanhle.1013@gmail.com?source=post_page---------------------------" class="cz bu kk kl km kn"><em class="mg">email me directly</em></a><em class="mg"> or </em><a href="http://www.linkedin.com/in/khanhnamle94?source=post_page---------------------------" class="cz bu kk kl km kn"><em class="mg">find me on LinkedIn</em></a><em class="mg">. </em><a href="http://eepurl.com/deWjzb?source=post_page---------------------------" class="cz bu kk kl km kn"><em class="mg">Sign up for my newsletter</em></a><em class="mg"> to receive my latest thoughts on data science, machine learning, and artificial intelligence right at your inbox!</em></p><p id="b037" class="jw jx ed aq jy b jz ka kb kc kd ke kf kg kh ki kj" data-selectable-paragraph=""><strong class="jy mh">Discuss this post on </strong><a href="https://news.ycombinator.com/item?id=16820833&amp;source=post_page---------------------------" class="cz bu kk kl km kn"><strong class="jy mh">Hacker News</strong></a></p></div></section><hr class="nf ct hd ng nh hy ni nj nk nl nm"><section class="ie if ig ih ii"><div class="af iu ac dl w x"><blockquote class="nn"><div id="2b84" class="no np nq aq ix b nr ns nt nu nv nw kj" data-selectable-paragraph=""><p class="ix b nx cg au">Did you know: Machine learning isn’t just happening on servers and in the cloud. It’s also being deployed to the edge. <a href="https://fritz.ai/?utm_campaign=educate2&amp;utm_source=heartbeat&amp;source=post_page---------------------------" class="cz bu kk kl km kn">Learn more</a> about how Fritz is making this transition possible.</p></div></blockquote><p id="a4be" class="jw jx ed aq jy b jz ow kb ox kd oy kf oz kh pa kj" data-selectable-paragraph=""><em class="mg">Editor’s Note:</em><a href="http://bit.ly/heartbeatslack?source=post_page---------------------------" class="cz bu kk kl km kn"><em class="mg"> Join Heartbeat on Slack</em></a><em class="mg"> and follow us on</em><a href="https://twitter.com/fritzlabs?source=post_page---------------------------" class="cz bu kk kl km kn"><em class="mg"> Twitter</em></a><em class="mg"> and</em><a href="https://www.linkedin.com/company/fritz-labs-inc/?source=post_page---------------------------" class="cz bu kk kl km kn"><em class="mg"> LinkedIn</em></a><em class="mg"> for the all the latest content, news, and more in machine learning, mobile development, and where the two intersect.</em></p><figure class="ma mb mc md me"><div class="im n er"><div class="qj n"><iframe src="The%205%20Computer%20Vision%20Techniques%20That%20Will%20Change%20How%20You%20See%20The%20World_files/media.html" title="Like what you're reading?" class="gg p q gf ac" width="680" height="455" frameborder="0"></iframe></div></div></figure></div></section></div></article><div class="pc qk do o dp dq dr ds dt e" data-test-id="post-sidebar"><div class="ag du"><div class="dv dw n"><a href="https://heartbeat.fritz.ai/?source=post_sidebar--------------------------post_sidebar-" class="cz da ay az ba bb bc bd be bf dx dy bi bj dz ea"><h2 class="ap eb ec ar ed">Heartbeat</h2></a><div class="ee ef n"><h4 class="ap ct eg ar bz eh ei ej ek el au">The latest on mobile machine learning.</h4></div><div class="bt"><button class="em bn aw ax bo bg bh bp bf bq ap b aq ar as at br bs af bt bu bi">Follow</button></div></div><div class="en eo ep ag"><div class="ag ah"><div class="eq n er"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fheartbeat.fritz.ai%2Fthe-5-computer-vision-techniques-that-will-change-how-you-see-the-world-1ee19334354b&amp;source=post_sidebar-----1ee19334354b---------------------clap_sidebar-" class="cz da ay az ba bb bc bd be bf dx dy bi bj dz ea"><div class="bd es et eu ev ew ex ax ey"><svg width="29" height="29"><g fill-rule="evenodd"><path d="M13.74 1l.76 2.97.76-2.97zM16.82 4.78l1.84-2.56-1.43-.47zM10.38 2.22l1.84 2.56-.41-3.03zM22.38 22.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M9.1 22.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L6.1 15.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L6.4 11.26l-1.18-1.18a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L11.96 14a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L8.43 9.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L20.63 15c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM13 6.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 23 23.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></div></a></div><div class="ez n"><div class="fa"><h4 class="ap ct eg ar au"><button class="cz da ay az ba bb bc bd be bf dx dy bi bj dz ea">3.6K </button></h4></div></div></div></div><div><div class="bt"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fheartbeat.fritz.ai%2Fthe-5-computer-vision-techniques-that-will-change-how-you-see-the-world-1ee19334354b&amp;source=post_sidebar--------------------------bookmark_sidebar-" class="cz da ay az ba bb bc bd be bf dx dy bi bj dz ea"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></a></div></div></div></div><div><div class="dk fb ag du fc"><section class="w x y z ac dl af n"><ul class="bd be"><li class="bt cf fd fe"><a href="https://heartbeat.fritz.ai/tag/machine-learning" class="ff fg bu au n fh fi a b cu">Machine Learning</a></li><li class="bt cf fd fe"><a href="https://heartbeat.fritz.ai/tag/deep-learning" class="ff fg bu au n fh fi a b cu">Deep Learning</a></li><li class="bt cf fd fe"><a href="https://heartbeat.fritz.ai/tag/computer-vision" class="ff fg bu au n fh fi a b cu">Computer Vision</a></li><li class="bt cf fd fe"><a href="https://heartbeat.fritz.ai/tag/heartbeat" class="ff fg bu au n fh fi a b cu">Heartbeat</a></li><li class="bt cf fd fe"><a href="https://heartbeat.fritz.ai/tag/machine-learning-tools" class="ff fg bu au n fh fi a b cu">Machine Learning Tools</a></li></ul><div class="ag fj fk"><div class="ag ah"><div class="fl n er"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fheartbeat.fritz.ai%2Fthe-5-computer-vision-techniques-that-will-change-how-you-see-the-world-1ee19334354b&amp;source=post_actions_footer-----1ee19334354b---------------------clap_footer-" class="cz da ay az ba bb bc bd be bf dx dy bi bj dz ea"><div class="c fm fn ag ah fo er fp fq bp fr fs ft fu fv fw fx fy fz ga gb"><div class="bd es et eu ev ew gc ah gd fn ag ax fc ge q gf gg p ac ey"><svg width="33" height="33" viewBox="0 0 33 33"><path d="M28.86 17.34l-3.64-6.4c-.3-.43-.71-.73-1.16-.8a1.12 1.12 0 0 0-.9.21c-.62.5-.73 1.18-.32 2.06l1.22 2.6 1.4 2.45c2.23 4.09 1.51 8-2.15 11.66a9.6 9.6 0 0 1-.8.71 6.53 6.53 0 0 0 4.3-2.1c3.82-3.82 3.57-7.87 2.05-10.39zm-6.25 11.08c3.35-3.35 4-6.78 1.98-10.47L21.2 12c-.3-.43-.71-.72-1.16-.8a1.12 1.12 0 0 0-.9.22c-.62.49-.74 1.18-.32 2.06l1.72 3.63a.5.5 0 0 1-.81.57l-8.91-8.9a1.33 1.33 0 0 0-1.89 1.88l5.3 5.3a.5.5 0 0 1-.71.7l-5.3-5.3-1.49-1.49c-.5-.5-1.38-.5-1.88 0a1.34 1.34 0 0 0 0 1.89l1.49 1.5 5.3 5.28a.5.5 0 0 1-.36.86.5.5 0 0 1-.36-.15l-5.29-5.29a1.34 1.34 0 0 0-1.88 0 1.34 1.34 0 0 0 0 1.89l2.23 2.23L9.3 21.4a.5.5 0 0 1-.36.85.5.5 0 0 1-.35-.14l-3.32-3.33a1.33 1.33 0 0 0-1.89 0 1.32 1.32 0 0 0-.39.95c0 .35.14.69.4.94l6.39 6.4c3.53 3.53 8.86 5.3 12.82 1.35zM12.73 9.26l5.68 5.68-.49-1.04c-.52-1.1-.43-2.13.22-2.89l-3.3-3.3a1.34 1.34 0 0 0-1.88 0 1.33 1.33 0 0 0-.4.94c0 .22.07.42.17.61zm14.79 19.18a7.46 7.46 0 0 1-6.41 2.31 7.92 7.92 0 0 1-3.67.9c-3.05 0-6.12-1.63-8.36-3.88l-6.4-6.4A2.31 2.31 0 0 1 2 19.72a2.33 2.33 0 0 1 1.92-2.3l-.87-.87a2.34 2.34 0 0 1 0-3.3 2.33 2.33 0 0 1 1.24-.64l-.14-.14a2.34 2.34 0 0 1 0-3.3 2.39 2.39 0 0 1 3.3 0l.14.14a2.33 2.33 0 0 1 3.95-1.24l.09.09c.09-.42.29-.83.62-1.16a2.34 2.34 0 0 1 3.3 0l3.38 3.39a2.17 2.17 0 0 1 1.27-.17c.54.08 1.03.35 1.45.76.1-.55.41-1.03.9-1.42a2.12 2.12 0 0 1 1.67-.4 2.8 2.8 0 0 1 1.85 1.25l3.65 6.43c1.7 2.83 2.03 7.37-2.2 11.6zM13.22.48l-1.92.89 2.37 2.83-.45-3.72zm8.48.88L19.78.5l-.44 3.7 2.36-2.84zM16.5 3.3L15.48 0h2.04L16.5 3.3z" fill-rule="evenodd"></path></svg></div></div></a></div><div class="ez n"><div class="fa"><h4 class="ap ct eg ar ed"><button class="cz da ay az ba bb bc bd be bf dx dy bi bj dz ea">3.6K claps</button></h4></div></div></div><div class="ag ah"><div class="gh n an g"><a href="https://medium.com/p/1ee19334354b/share/twitter?source=follow_footer--------------------------follow_footer-" class="cz da ay az ba bb bc bd be bf dx dy bi bj dz ea"><svg width="29" height="29" class="am"><path d="M22.05 7.54a4.47 4.47 0 0 0-3.3-1.46 4.53 4.53 0 0 0-4.53 4.53c0 .35.04.7.08 1.05A12.9 12.9 0 0 1 5 6.89a5.1 5.1 0 0 0-.65 2.26c.03 1.6.83 2.99 2.02 3.79a4.3 4.3 0 0 1-2.02-.57v.08a4.55 4.55 0 0 0 3.63 4.44c-.4.08-.8.13-1.21.16l-.81-.08a4.54 4.54 0 0 0 4.2 3.15 9.56 9.56 0 0 1-5.66 1.94l-1.05-.08c2 1.27 4.38 2.02 6.94 2.02 8.3 0 12.86-6.9 12.84-12.85.02-.24 0-.43 0-.65a8.68 8.68 0 0 0 2.26-2.34c-.82.38-1.7.62-2.6.72a4.37 4.37 0 0 0 1.95-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></a></div><div class="gh n an g"><a href="https://medium.com/p/1ee19334354b/share/facebook?source=follow_footer--------------------------follow_footer-" class="cz da ay az ba bb bc bd be bf dx dy bi bj dz ea"><svg width="29" height="29" class="am"><path d="M23.2 5H5.8a.8.8 0 0 0-.8.8V23.2c0 .44.35.8.8.8h9.3v-7.13h-2.38V13.9h2.38v-2.38c0-2.45 1.55-3.66 3.74-3.66 1.05 0 1.95.08 2.2.11v2.57h-1.5c-1.2 0-1.48.57-1.48 1.4v1.96h2.97l-.6 2.97h-2.37l.05 7.12h5.1a.8.8 0 0 0 .79-.8V5.8a.8.8 0 0 0-.8-.79"></path></svg></a></div><div class="gh gi bx"><div class="bt"><button class="cz da ay az ba bb bc bd be bf dx dy bi bj dz ea"><svg width="25" height="25" class="am"><g fill-rule="evenodd"><path d="M15.6 5a.42.42 0 0 0 .17-.3.42.42 0 0 0-.12-.33l-2.8-2.79a.5.5 0 0 0-.7 0l-2.8 2.8a.4.4 0 0 0-.1.32c0 .12.07.23.16.3h.02a.45.45 0 0 0 .57-.04l2-2V10c0 .28.23.5.5.5s.5-.22.5-.5V2.93l2.02 2.02c.08.07.18.12.3.13.11.01.21-.02.3-.08v.01"></path><path d="M18 7h-1.5a.5.5 0 0 0 0 1h1.6c.5 0 .9.4.9.9v10.2c0 .5-.4.9-.9.9H6.9a.9.9 0 0 1-.9-.9V8.9c0-.5.4-.9.9-.9h1.6a.5.5 0 0 0 .35-.15A.5.5 0 0 0 9 7.5a.5.5 0 0 0-.15-.35A.5.5 0 0 0 8.5 7H7a2 2 0 0 0-2 2v10c0 1.1.9 2 2 2h11a2 2 0 0 0 2-2V9a2 2 0 0 0-2-2"></path></g></svg></button></div></div><div class="gh n an"><div><div class="bt"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fheartbeat.fritz.ai%2Fthe-5-computer-vision-techniques-that-will-change-how-you-see-the-world-1ee19334354b&amp;source=post_actions_footer--------------------------bookmark_sidebar-" class="cz da ay az ba bb bc bd be bf dx dy bi bj dz ea"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></a></div></div></div><div class="bt"><div class="bt"><div class="n an"><button class="cz da ay az ba bb bc bd be bf dx dy bi bj dz ea"><svg width="25" height="25" viewBox="-480.5 272.5 21 21" class="am"><path d="M-463 284.6c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5zm-7-.9c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5zm-7-.9c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5z"></path></svg></button></div></div></div></div></div><div class="gj gk gl gm"><div class="gn go n er"><span class="n gp aj gq"><div class="n gg gr gs"><a href="https://heartbeat.fritz.ai/@james_aka_yale?source=follow_footer--------------------------follow_footer-"><div class="er gt cm"><svg width="82" height="92" viewBox="0 0 82 92" class="gu gg gv gw gx gy dn"><path d="M1.58 26.29C8.86 11.67 23.78 1.65 41 1.65V.35C23.26.35 7.9 10.67.42 25.71l1.16.58zM41 1.65c17.22 0 32.14 10.02 39.42 24.64l1.16-.58C74.1 10.67 58.74.35 41 .35v1.3zm39.42 64.06C73.14 80.33 58.22 90.35 41 90.35v1.3c17.74 0 33.1-10.32 40.58-25.36l-1.16-.58zM41 90.35c-17.22 0-32.14-10.02-39.42-24.64l-1.16.58C7.9 81.33 23.26 91.65 41 91.65v-1.3z"></path></svg><img alt="James Le" src="The%205%20Computer%20Vision%20Techniques%20That%20Will%20Change%20How%20You%20See%20The%20World_files/1kbXSc2-EEtk9ekKq36woIQ_002.jpeg" class="n fn cm gt" width="80" height="80"></div></a></div><span class="n"><div class="gz n ha"><p class="ap ct cu ar au cx hb">Written by</p></div><div class="gz hc ag ha"><div class="ac ag ah fj"><h2 class="ap eb hd he ed"><a href="https://heartbeat.fritz.ai/@james_aka_yale?source=follow_footer--------------------------follow_footer-" class="cz da ay az ba bb bc bd be bf dx dy bi bj dz ea">James Le</a></h2><div class="n g"><button class="em ed am bn hf hg hh hi bf dz hj hk hl hm hn ho bq ap b aq ar as at br bs af bt bu bi">Follow</button></div></div></div></span></span><div class="gz hp n ha bx"><div class="hq n"><h4 class="ap ct ec hr au">Blue Ocean Thinker (https://jameskle.com/)</h4></div><div class="gi hs bx"><button class="em ed am bn hf hg hh hi bf dz hj hk hl hm hn ho bq ap b aq ar as at br bs af bt bu bi">Follow</button></div></div></div><div class="gl n"></div><div class="gn go n er"><span class="n gp aj gq"><div class="n gg gr gs"><a href="https://heartbeat.fritz.ai/?source=follow_footer--------------------------follow_footer-"><img alt="Heartbeat" src="The%205%20Computer%20Vision%20Techniques%20That%20Will%20Change%20How%20You%20See%20The%20World_files/1Ctcvx33DP415DmqrjubU1Q.png" class="bq gt cm" width="80" height="80"></a></div><span class="n"><div class="gz hc ag ha"><div class="ac ag ah fj"><h2 class="ap eb hd he ed"><a href="https://heartbeat.fritz.ai/?source=follow_footer--------------------------follow_footer-" class="cz da ay az ba bb bc bd be bf dx dy bi bj dz ea">Heartbeat</a></h2><div class="n g"><div class="bt"><button class="em bn aw ax bo bg bh bp bf bq ap b aq ar as at br bs af bt bu bi">Follow</button></div></div></div></div></span></span><div class="gz ht n ha bx"><div class="hq n"><h4 class="ap ct ec hr au">The latest on mobile machine learning.</h4></div><div class="gi hs bx"><div class="bt"><button class="em bn aw ax bo bg bh bp bf bq ap b aq ar as at br bs af bt bu bi">Follow</button></div></div></div></div></div><div class="hu hv n"><a href="https://medium.com/p/1ee19334354b/responses/show?source=follow_footer--------------------------follow_footer-" class="cz da ay az ba bb bc bd be bf dx dy bi bj dz ea"><div class="hw hx ff n hy bx"><span class="aw">See responses (2)</span></div></a></div></section><div class="hz n ia"><section class="w x y z ac ae af n"><section class="y jg z ac ph af ag fj"><section class="gn y z ac ph af n"><div class="pi dw gn n"><h2 class="ap eb pj pk ed">More From Medium</h2></div><div class="ag pl"><div class="ou pm n ak pn"><div class="ac gf"><div class="gn ag du"><div class="po n"><h4 class="ap ct eg ar au">More from Heartbeat</h4></div><div class="my n"><a class="cz da ay az ba bb bc bd be bf dx dy bi bj dz ea n" href="https://heartbeat.fritz.ai/how-to-build-an-email-authentication-app-with-firebase-firestore-and-react-native-a18a8ba78574?source=post_recirc---------0------------------"><div class="pp er"><div class="gf gg ac"><div class="pq n pr ps gf ac pt pu"></div></div></div></a></div><div class="my n"><a href="https://heartbeat.fritz.ai/how-to-build-an-email-authentication-app-with-firebase-firestore-and-react-native-a18a8ba78574?source=post_recirc---------0------------------"><h3 class="ed am ix pv aq nr pw px">How to build an Email Authentication app with Firebase, Firestore, and React Native</h3></a></div><div class="ag ah fj"><div class="py n ce"><div class="ah ag"><div><a href="https://heartbeat.fritz.ai/@amanhimself?source=post_recirc---------0------------------"><img alt="Aman Mittal" src="The%205%20Computer%20Vision%20Techniques%20That%20Will%20Change%20How%20You%20See%20The%20World_files/1L2HqlPuCG1ULIVPiucnQ6g.jpeg" class="n fn pz qa" width="40" height="40"></a></div><div class="jn ac n"><div class="ag"><div style="flex: 1 1 0%;"><span class="ap b aq ar as at n ed am"><div class="df ag ah jp" data-test-id="postByline"><span class="ap ct eg ar bz jq ei ej jr el ed"><a href="https://heartbeat.fritz.ai/@amanhimself?source=post_recirc---------0------------------" class="cz da ay az ba bb bc bd be bf js bi bj dz ea">Aman Mittal</a><span> in <a href="https://heartbeat.fritz.ai/?source=post_recirc---------0------------------" class="cz da ay az ba bb bc bd be bf js bi bj dz ea">Heartbeat</a></span></span></div></span></div></div><span class="ap b aq ar as at n au av"><span class="ap ct eg ar bz jq ei ej jr el au"><div><a class="cz da ay az ba bb bc bd be bf js bi bj dz ea" href="https://heartbeat.fritz.ai/how-to-build-an-email-authentication-app-with-firebase-firestore-and-react-native-a18a8ba78574?source=post_recirc---------0------------------">Jun 28</a> · 14 min read</div></span></span></div></div></div><div class="ag ah"><div class="ag ah"><div class="eq n er"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fheartbeat.fritz.ai%2Fthe-5-computer-vision-techniques-that-will-change-how-you-see-the-world-1ee19334354b&amp;source=post_recirc-----a18a8ba78574----0-----------------clap_preview-" class="cz da ay az ba bb bc bd be bf dx dy bi bj dz ea"><div class="bd es et eu ev ew ex qb qc"><svg width="25" height="25" viewBox="0 0 25 25"><g fill-rule="evenodd"><path d="M11.74 0l.76 2.97.76-2.97zM14.81 3.78l1.84-2.56-1.42-.47zM8.38 1.22l1.84 2.56L9.8.75zM20.38 21.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M7.1 21.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L4.1 14.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L4.4 10.26 3.22 9.08a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L9.96 13a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L6.43 8.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L18.63 14c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM11 5.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 21 22.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></div></a></div><div class="ez n"><div class="fa"><h4 class="ap ct eg ar au">201 </h4></div></div></div><div class="qd jn py cn qe n"></div><div><div class="bt"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fheartbeat.fritz.ai%2Fthe-5-computer-vision-techniques-that-will-change-how-you-see-the-world-1ee19334354b&amp;source=post_recirc---------0-----------------bookmark_sidebar-" class="cz da ay az ba bb bc bd be bf dx dy bi bj dz ea"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></a></div></div></div></div></div></div></div><div class="ou pm n ak pn"><div class="ac gf"><div class="gn ag du"><div class="po n"><h4 class="ap ct eg ar au">More from Heartbeat</h4></div><div class="my n"><a class="cz da ay az ba bb bc bd be bf dx dy bi bj dz ea n" href="https://heartbeat.fritz.ai/swift-5-value-vs-reference-types-e65afe127090?source=post_recirc---------1------------------"><div class="pp er"><div class="gf gg ac"><div class="qf n pr ps gf ac pt pu"></div></div></div></a></div><div class="my n"><a href="https://heartbeat.fritz.ai/swift-5-value-vs-reference-types-e65afe127090?source=post_recirc---------1------------------"><h3 class="ed am ix pv aq nr pw px">Swift 5: Value Vs Reference types</h3></a></div><div class="ag ah fj"><div class="py n ce"><div class="ah ag"><div><a href="https://heartbeat.fritz.ai/@navdeepsingh_2336?source=post_recirc---------1------------------"><div class="er qa pz"><svg width="46" height="50" viewBox="0 0 46 50" class="gu gg jj jk jl jm dn"><path d="M1.45 15.22C5.43 7.07 13.59 1.5 23 1.5v-1C13.18.5 4.69 6.32.55 14.78l.9.44zM23 1.5c9.4 0 17.57 5.57 21.55 13.72l.9-.44C41.3 6.32 32.82.5 23 .5v1zm21.55 33.28C40.57 42.93 32.41 48.5 23 48.5v1c9.82 0 18.31-5.82 22.45-14.28l-.9-.44zM23 48.5c-9.4 0-17.57-5.57-21.55-13.72l-.9.44C4.7 43.68 13.18 49.5 23 49.5v-1z"></path></svg><img alt="Navdeep Singh" src="The%205%20Computer%20Vision%20Techniques%20That%20Will%20Change%20How%20You%20See%20The%20World_files/0Kll4LpolU8tT5F19.jpeg" class="n fn pz qa" width="40" height="40"></div></a></div><div class="jn ac n"><div class="ag"><div style="flex: 1 1 0%;"><span class="ap b aq ar as at n ed am"><div class="df ag ah jp" data-test-id="postByline"><span class="ap ct eg ar bz jq ei ej jr el ed"><a href="https://heartbeat.fritz.ai/@navdeepsingh_2336?source=post_recirc---------1------------------" class="cz da ay az ba bb bc bd be bf js bi bj dz ea">Navdeep Singh</a><span> in <a href="https://heartbeat.fritz.ai/?source=post_recirc---------1------------------" class="cz da ay az ba bb bc bd be bf js bi bj dz ea">Heartbeat</a></span></span></div></span></div></div><span class="ap b aq ar as at n au av"><span class="ap ct eg ar bz jq ei ej jr el au"><div><a class="cz da ay az ba bb bc bd be bf js bi bj dz ea" href="https://heartbeat.fritz.ai/swift-5-value-vs-reference-types-e65afe127090?source=post_recirc---------1------------------">Jun 19</a> · 4 min read</div></span></span></div></div></div><div class="ag ah"><div class="ag ah"><div class="eq n er"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fheartbeat.fritz.ai%2Fthe-5-computer-vision-techniques-that-will-change-how-you-see-the-world-1ee19334354b&amp;source=post_recirc-----e65afe127090----1-----------------clap_preview-" class="cz da ay az ba bb bc bd be bf dx dy bi bj dz ea"><div class="bd es et eu ev ew ex qb qc"><svg width="25" height="25" viewBox="0 0 25 25"><g fill-rule="evenodd"><path d="M11.74 0l.76 2.97.76-2.97zM14.81 3.78l1.84-2.56-1.42-.47zM8.38 1.22l1.84 2.56L9.8.75zM20.38 21.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M7.1 21.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L4.1 14.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L4.4 10.26 3.22 9.08a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L9.96 13a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L6.43 8.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L18.63 14c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM11 5.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 21 22.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></div></a></div><div class="ez n"><div class="fa"><h4 class="ap ct eg ar au">315 </h4></div></div></div><div class="qd jn py cn qe n"></div><div><div class="bt"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fheartbeat.fritz.ai%2Fthe-5-computer-vision-techniques-that-will-change-how-you-see-the-world-1ee19334354b&amp;source=post_recirc---------1-----------------bookmark_sidebar-" class="cz da ay az ba bb bc bd be bf dx dy bi bj dz ea"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></a></div></div></div></div></div></div></div><div class="qg pm n ak pn"><div class="ac gf"><div class="gn ag du"><div class="po n"><h4 class="ap ct eg ar au">More from Heartbeat</h4></div><div class="my n"><a class="cz da ay az ba bb bc bd be bf dx dy bi bj dz ea n" href="https://heartbeat.fritz.ai/top-7-libraries-and-packages-of-the-year-for-data-science-and-ai-python-r-6b7cca2bf000?source=post_recirc---------2------------------"><div class="pp er"><div class="gf gg ac"><div class="qh n pr ps gf ac pt pu"></div></div></div></a></div><div class="my n"><a href="https://heartbeat.fritz.ai/top-7-libraries-and-packages-of-the-year-for-data-science-and-ai-python-r-6b7cca2bf000?source=post_recirc---------2------------------"><h3 class="ed am ix pv aq nr pw px">Top 7 libraries and packages of the year for Data Science and AI: Python &amp; R</h3></a></div><div class="ag ah fj"><div class="py n ce"><div class="ah ag"><div><a href="https://heartbeat.fritz.ai/@faviovazquez?source=post_recirc---------2------------------"><div class="er qa pz"><svg width="46" height="50" viewBox="0 0 46 50" class="gu gg jj jk jl jm dn"><path d="M1.45 15.22C5.43 7.07 13.59 1.5 23 1.5v-1C13.18.5 4.69 6.32.55 14.78l.9.44zM23 1.5c9.4 0 17.57 5.57 21.55 13.72l.9-.44C41.3 6.32 32.82.5 23 .5v1zm21.55 33.28C40.57 42.93 32.41 48.5 23 48.5v1c9.82 0 18.31-5.82 22.45-14.28l-.9-.44zM23 48.5c-9.4 0-17.57-5.57-21.55-13.72l-.9.44C4.7 43.68 13.18 49.5 23 49.5v-1z"></path></svg><img alt="Favio Vázquez" src="The%205%20Computer%20Vision%20Techniques%20That%20Will%20Change%20How%20You%20See%20The%20World_files/2zcPDtaZcELwMoXEFOeBKiA.png" class="n fn pz qa" width="40" height="40"></div></a></div><div class="jn ac n"><div class="ag"><div style="flex: 1 1 0%;"><span class="ap b aq ar as at n ed am"><div class="df ag ah jp" data-test-id="postByline"><span class="ap ct eg ar bz jq ei ej jr el ed"><a href="https://heartbeat.fritz.ai/@faviovazquez?source=post_recirc---------2------------------" class="cz da ay az ba bb bc bd be bf js bi bj dz ea">Favio Vázquez</a><span> in <a href="https://heartbeat.fritz.ai/?source=post_recirc---------2------------------" class="cz da ay az ba bb bc bd be bf js bi bj dz ea">Heartbeat</a></span></span></div></span></div></div><span class="ap b aq ar as at n au av"><span class="ap ct eg ar bz jq ei ej jr el au"><div><a class="cz da ay az ba bb bc bd be bf js bi bj dz ea" href="https://heartbeat.fritz.ai/top-7-libraries-and-packages-of-the-year-for-data-science-and-ai-python-r-6b7cca2bf000?source=post_recirc---------2------------------">Dec 31, 2018</a> · 26 min read</div></span></span></div></div></div><div class="ag ah"><div class="ag ah"><div class="eq n er"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fheartbeat.fritz.ai%2Fthe-5-computer-vision-techniques-that-will-change-how-you-see-the-world-1ee19334354b&amp;source=post_recirc-----6b7cca2bf000----2-----------------clap_preview-" class="cz da ay az ba bb bc bd be bf dx dy bi bj dz ea"><div class="bd es et eu ev ew ex qb qc"><svg width="25" height="25" viewBox="0 0 25 25"><g fill-rule="evenodd"><path d="M11.74 0l.76 2.97.76-2.97zM14.81 3.78l1.84-2.56-1.42-.47zM8.38 1.22l1.84 2.56L9.8.75zM20.38 21.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M7.1 21.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L4.1 14.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L4.4 10.26 3.22 9.08a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L9.96 13a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L6.43 8.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L18.63 14c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM11 5.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 21 22.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></div></a></div><div class="ez n"><div class="fa"><h4 class="ap ct eg ar au">2.2K </h4></div></div></div><div class="qd jn py cn qe n"></div><div><div class="bt"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fheartbeat.fritz.ai%2Fthe-5-computer-vision-techniques-that-will-change-how-you-see-the-world-1ee19334354b&amp;source=post_recirc---------2-----------------bookmark_sidebar-" class="cz da ay az ba bb bc bd be bf dx dy bi bj dz ea"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></a></div></div></div></div></div></div></div></div></section></section></section></div></div></div><script>window.PARSELY = window.PARSELY || {autotrack: false}</script></div></div><script>window.__BUILD_ID__ = "development"</script><script>window.__GRAPHQL_URI__ = "https://heartbeat.fritz.ai/_/graphql"</script><script>window.__PRELOADED_STATE__ = {"config":{"nodeEnv":"production","version":"master-20190725-172748-a8311c1bcc","productName":"Medium","publicUrl":"https:\u002F\u002Fcdn-client.medium.com\u002Flite","authDomain":"medium.com","authGoogleClientId":"216296035834-k1k6qe060s2tp2a2jam4ljdcms00sttg.apps.googleusercontent.com","favicon":"production","glyphUrl":"https:\u002F\u002Fglyph.medium.com","iTunesAppId":"828256236","branchKey":"key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm","lightStep":{"name":"lite-web","host":"collector-medium.lightstep.com","token":"ce5be895bef60919541332990ac9fef2","appVersion":"master-20190725-172748-a8311c1bcc"},"algolia":{"appId":"MQ57UUUQZ2","apiKeySearch":"394474ced050e3911ae2249ecc774921","indexPrefix":"medium_","host":"-dsn.algolia.net"},"recaptchaKey":"6LdAokEUAAAAAC7seICd4vtC8chDb3jIXDQulyUJ","sentry":{"dsn":"https:\u002F\u002F589e367c28ca47b195ce200d1507d18b@sentry.io\u002F1423575","environment":"production"},"isAmp":false,"googleAnalyticsCode":"UA-24232453-2","signInWallCustomDomainCollectionIds":["3a8144eabfe3","336d898217ee","61061eb0c96b","138adf9c44c","819cc2aaeee0"]},"debug":{"requestId":"e15ee865-9596-4dd0-8447-312c194ed48d","originalSpanCarrier":{"ot-tracer-spanid":"47c9b1fb16976099","ot-tracer-traceid":"1a9e60ae4cff3a95","ot-tracer-sampled":"true"}},"session":{"user":{"id":"lo_u1EUALNPlpAr"},"xsrf":""},"stats":{"itemCount":0,"sending":false,"timeout":null,"backup":{}},"navigation":{"showBranchBanner":null,"hideGoogleOneTap":false,"currentLocation":"https:\u002F\u002Fheartbeat.fritz.ai\u002Fthe-5-computer-vision-techniques-that-will-change-how-you-see-the-world-1ee19334354b","host":"heartbeat.fritz.ai","hostname":"heartbeat.fritz.ai","currentHash":""},"client":{"isBot":false,"isDnt":false,"isEu":false,"isNativeMedium":false,"isCustomDomain":true},"multiVote":{"clapsPerPost":{}},"metadata":{"faviconImageId":null}}</script><script>window.__APOLLO_STATE__ = {"ROOT_QUERY":{"viewer":null,"variantFlags":[{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.0","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.1","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.2","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.3","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.4","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.5","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.6","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.7","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.8","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.9","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.10","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.11","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.12","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.13","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.14","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.15","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.16","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.17","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.18","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.19","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.20","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.21","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.22","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.23","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.24","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.25","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.26","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.27","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.28","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.29","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.30","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.31","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.32","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.33","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.34","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.35","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.36","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.37","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.38","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.39","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.40","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.41","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.42","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.43","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.44","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.45","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.46","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.47","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.48","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.49","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.50","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.51","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.52","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.53","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.54","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.55","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.56","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.57","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.58","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.59","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.60","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.61","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.62","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.63","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.64","typename":"VariantFlag"}],"meterPost({\"postId\":\"1ee19334354b\",\"postMeteringOptions\":{}})":{"type":"id","generated":false,"id":"MeteringInfo:singleton","typename":"MeteringInfo"},"postResult({\"id\":\"1ee19334354b\"})":{"type":"id","generated":false,"id":"Post:1ee19334354b","typename":"Post"}},"ROOT_QUERY.variantFlags.0":{"name":"allow_access","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.0.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.0.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.1":{"name":"allow_signup","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.1.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.1.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.2":{"name":"allow_test_auth","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.2.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.2.valueType":{"__typename":"VariantFlagString","value":"disallow"},"ROOT_QUERY.variantFlags.3":{"name":"signin_services","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.3.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.3.valueType":{"__typename":"VariantFlagString","value":"twitter,facebook,google,email,google-fastidv,google-one-tap"},"ROOT_QUERY.variantFlags.4":{"name":"signup_services","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.4.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.4.valueType":{"__typename":"VariantFlagString","value":"twitter,facebook,google,email,google-fastidv,google-one-tap"},"ROOT_QUERY.variantFlags.5":{"name":"google_sign_in_android","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.5.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.5.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.6":{"name":"browsable_stream_config_bucket","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.6.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.6.valueType":{"__typename":"VariantFlagString","value":"curated-topics"},"ROOT_QUERY.variantFlags.7":{"name":"enable_dedicated_series_tab_api_ios","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.7.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.7.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.8":{"name":"enable_post_import","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.8.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.8.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.9":{"name":"available_monthly_plan","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.9.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.9.valueType":{"__typename":"VariantFlagString","value":"60e220181034"},"ROOT_QUERY.variantFlags.10":{"name":"available_annual_plan","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.10.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.10.valueType":{"__typename":"VariantFlagString","value":"2c754bcc2995"},"ROOT_QUERY.variantFlags.11":{"name":"disable_ios_resume_reading_toast","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.11.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.11.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.12":{"name":"is_not_medium_subscriber","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.12.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.12.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.13":{"name":"glyph_font_set","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.13.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.13.valueType":{"__typename":"VariantFlagString","value":"m2"},"ROOT_QUERY.variantFlags.14":{"name":"enable_branding","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.14.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.14.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.15":{"name":"enable_branding_fonts","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.15.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.15.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.16":{"name":"enable_automated_mission_control_triggers","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.16.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.16.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.17":{"name":"enable_lite_profile","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.17.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.17.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.18":{"name":"enable_gosocial_aurora_shadow_reads","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.18.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.18.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.19":{"name":"enable_marketing_emails","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.19.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.19.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.20":{"name":"enable_parsely","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.20.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.20.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.21":{"name":"enable_branch_io","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.21.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.21.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.22":{"name":"enable_ios_post_stats","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.22.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.22.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.23":{"name":"enable_lite_topics","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.23.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.23.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.24":{"name":"enable_lite_stories","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.24.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.24.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.25":{"name":"redis_read_write_splitting","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.25.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.25.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.26":{"name":"enable_tipalti_onboarding","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.26.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.26.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.27":{"name":"enable_annual_renewal_reminder_email","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.27.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.27.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.28":{"name":"enable_janky_spam_rules","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.28.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.28.valueType":{"__typename":"VariantFlagString","value":"users,posts"},"ROOT_QUERY.variantFlags.29":{"name":"enable_new_collaborative_filtering_data","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.29.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.29.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.30":{"name":"enable_google_one_tap","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.30.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.30.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.31":{"name":"enable_email_sign_in_captcha","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.31.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.31.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.32":{"name":"enable_primary_topic_for_mobile","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.32.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.32.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.33":{"name":"enable_lite_post","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.33.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.33.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.34":{"name":"enable_logged_out_homepage_signup","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.34.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.34.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.35":{"name":"use_new_admin_topic_backend","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.35.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.35.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.36":{"name":"enable_quarantine_rules","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.36.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.36.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.37":{"name":"enable_patronus_on_kubernetes","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.37.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.37.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.38":{"name":"pub_sidebar","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.38.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.38.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.39":{"name":"disable_mobile_featured_chunk","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.39.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.39.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.40":{"name":"enable_embedding_based_diversification","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.40.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.40.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.41":{"name":"enable_lite_pub_header_menu","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.41.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.41.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.42":{"name":"enable_lite_claps","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.42.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.42.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.43":{"name":"enable_live_user_post_scoring","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.43.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.43.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.44":{"name":"enable_lite_post_highlights","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.44.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.44.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.45":{"name":"enable_lite_post_highlights_view_only","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.45.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.45.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.46":{"name":"enable_tick_landing_page","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.46.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.46.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.47":{"name":"enable_lite_post_cd","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.47.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.47.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.48":{"name":"enable_lite_private_notes","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.48.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.48.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.49":{"name":"enable_lite_private_notes_li_100","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.49.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.49.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.50":{"name":"enable_trumpland_landing_page","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.50.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.50.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.51":{"name":"enable_lite_email_sign_in_flow","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.51.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.51.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.52":{"name":"enable_lite_paywall_alert","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.52.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.52.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.53":{"name":"enable_edit_alt_text","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.53.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.53.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.54":{"name":"enable_serve_recs_from_ml_rank_homepage","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.54.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.54.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.55":{"name":"enable_serve_recs_from_ml_rank_digest","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.55.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.55.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.56":{"name":"enable_serve_recs_from_ml_rank_app_highlights","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.56.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.56.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.57":{"name":"enable_lite_thanks_to","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.57.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.57.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.58":{"name":"enable_lite_google_captcha","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.58.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.58.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.59":{"name":"enable_lite_branch_io","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.59.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.59.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.60":{"name":"enable_lite_notifications","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.60.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.60.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.61":{"name":"enable_lite_audio_upsells","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.61.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.61.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.62":{"name":"enable_lite_verify_email_butter_bar","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.62.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.62.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.63":{"name":"remove_social_proof_on_digest","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.63.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.63.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.64":{"name":"enable_lite_unread_notification_count","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.64.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.64.valueType":{"__typename":"VariantFlagBoolean","value":true},"MeteringInfo:singleton":{"__typename":"MeteringInfo","postIds":{"type":"json","json":[]},"maxUnlockCount":3,"unlocksRemaining":3},"Post:1ee19334354b":{"__typename":"Post","creator":{"type":"id","generated":false,"id":"User:52aa38cb8e25","typename":"User"},"isLocked":false,"lockedSource":"LOCKED_POST_SOURCE_NONE","id":"1ee19334354b","collection":{"type":"id","generated":false,"id":"Collection:680eee12c50d","typename":"Collection"},"sequence":null,"firstPublishedAt":1523540761956,"isPublished":true,"title":"The 5 Computer Vision Techniques That Will Change How You See The World","canonicalUrl":"","layerCake":3,"primaryTopic":{"type":"id","generated":false,"id":"data-science","typename":"Topic"},"content({\"postMeteringOptions\":{}})":{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}})","typename":"PostContent"},"highlights":[{"type":"id","generated":false,"id":"Quote:anon_767dbfbf170d","typename":"Quote"}],"latestPublishedVersion":"4b121a4d065b","mediumUrl":"https:\u002F\u002Fheartbeat.fritz.ai\u002Fthe-5-computer-vision-techniques-that-will-change-how-you-see-the-world-1ee19334354b","readingTime":15.983018867924528,"statusForCollection":"APPROVED","visibility":"PUBLIC","allowResponses":true,"tags":[{"type":"id","generated":false,"id":"Tag:machine-learning","typename":"Tag"},{"type":"id","generated":false,"id":"Tag:deep-learning","typename":"Tag"},{"type":"id","generated":false,"id":"Tag:computer-vision","typename":"Tag"},{"type":"id","generated":false,"id":"Tag:heartbeat","typename":"Tag"},{"type":"id","generated":false,"id":"Tag:machine-learning-tools","typename":"Tag"}],"viewerClapCount":null,"readingList":"READING_LIST_NONE","clapCount":3623,"voterCount":619,"recommenders":[],"pendingCollection":null,"responsesCount":2,"collaborators":[],"inResponseToPostResult":null,"inResponseToMediaResource":null,"curationEligibleAt":0,"audioVersionUrl":null,"socialTitle":"","socialDek":"","metaDescription":"In-depth overviews of common Computer Vision techniques: Image classification, object detection, object tracking, semantic segmentation, and instance segmentation","latestPublishedAt":1558703671694,"previewContent":{"type":"id","generated":true,"id":"$Post:1ee19334354b.previewContent","typename":"PreviewContent"},"previewImage":{"type":"id","generated":false,"id":"ImageMetadata:1*4zHA9YtHIzKTm5Ix98rKZg.png","typename":"ImageMetadata"},"updatedAt":1558703671930,"topics":[{"type":"id","generated":false,"id":"data-science","typename":"Topic"}],"isSuspended":false},"User:52aa38cb8e25":{"id":"52aa38cb8e25","__typename":"User","isSuspended":false,"allowNotes":true,"name":"James Le","isFollowing":false,"username":"james_aka_yale","bio":"Blue Ocean Thinker (https:\u002F\u002Fjameskle.com\u002F)","imageId":"1*kbXSc2-EEtk9ekKq36woIQ.jpeg","mediumMemberAt":1554188400000,"isBlocking":false,"isPartnerProgramEnrolled":false,"twitterScreenName":"james_aka_yale"},"Collection:680eee12c50d":{"id":"680eee12c50d","__typename":"Collection","slug":"fritzheartbeat","domain":"heartbeat.fritz.ai","googleAnalyticsId":"UA-109026462-6","colorBehavior":"ACCENT_COLOR_AND_FILL_BACKGROUND","name":"Heartbeat","logo":{"type":"id","generated":false,"id":"ImageMetadata:1*XdqEfc3Jep6vKtS-KQXicw.png","typename":"ImageMetadata"},"creator":{"type":"id","generated":false,"id":"User:218f5259741","typename":"User"},"viewerCanManage":false,"avatar":{"type":"id","generated":false,"id":"ImageMetadata:1*Ctcvx33DP415DmqrjubU1Q.png","typename":"ImageMetadata"},"isEnrolledInHightower":false,"navItems":[{"type":"id","generated":true,"id":"Collection:680eee12c50d.navItems.0","typename":"NavItem"},{"type":"id","generated":true,"id":"Collection:680eee12c50d.navItems.1","typename":"NavItem"},{"type":"id","generated":true,"id":"Collection:680eee12c50d.navItems.2","typename":"NavItem"},{"type":"id","generated":true,"id":"Collection:680eee12c50d.navItems.3","typename":"NavItem"},{"type":"id","generated":true,"id":"Collection:680eee12c50d.navItems.4","typename":"NavItem"},{"type":"id","generated":true,"id":"Collection:680eee12c50d.navItems.5","typename":"NavItem"},{"type":"id","generated":true,"id":"Collection:680eee12c50d.navItems.6","typename":"NavItem"}],"colorPalette":{"type":"id","generated":true,"id":"$Collection:680eee12c50d.colorPalette","typename":"ColorPalette"},"viewerCanEditOwnPosts":false,"viewerCanEditPosts":false,"description":"The latest on mobile machine learning.","viewerIsFollowing":false,"viewerIsSubscribedToLetters":false,"mediumNewsletterId":"","isUserSubscribedToMediumNewsletter":false,"ampEnabled":false,"twitterUsername":"fritzlabs","facebookPageId":null,"favicon":{"type":"id","generated":false,"id":"ImageMetadata:1*KpWOVdzvNX_R3ewGUwLmYA.png","typename":"ImageMetadata"}},"data-science":{"name":"Data Science","slug":"data-science","__typename":"Topic"},"ImageMetadata:1*XdqEfc3Jep6vKtS-KQXicw.png":{"id":"1*XdqEfc3Jep6vKtS-KQXicw.png","originalWidth":469,"originalHeight":87,"__typename":"ImageMetadata"},"User:218f5259741":{"id":"218f5259741","__typename":"User"},"ImageMetadata:1*Ctcvx33DP415DmqrjubU1Q.png":{"id":"1*Ctcvx33DP415DmqrjubU1Q.png","__typename":"ImageMetadata"},"Collection:680eee12c50d.navItems.0":{"title":"MOBILE","url":"https:\u002F\u002Fheartbeat.fritz.ai\u002Fmobile-ml\u002Fhome","type":"TOPIC_PAGE","__typename":"NavItem"},"Collection:680eee12c50d.navItems.1":{"title":"MACHINE LEARNING","url":"https:\u002F\u002Fheartbeat.fritz.ai\u002Fmachine-learning\u002Fhome","type":"TOPIC_PAGE","__typename":"NavItem"},"Collection:680eee12c50d.navItems.2":{"title":"NEWSLETTER","url":"https:\u002F\u002Fheartbeat.fritz.ai\u002Fheartbeat-fritz-ai-newsletter\u002Fhome","type":"TOPIC_PAGE","__typename":"NavItem"},"Collection:680eee12c50d.navItems.3":{"title":"COMMUNITY","url":"https:\u002F\u002Fheartbeat.fritz.ai\u002Fheartbeat-fritz-ai-community\u002Fhome","type":"TOPIC_PAGE","__typename":"NavItem"},"Collection:680eee12c50d.navItems.4":{"title":"CONTRIBUTE","url":"https:\u002F\u002Fheartbeat.fritz.ai\u002Fcall-for-contributors-october-2018-update-fee7f5b80f3e","type":"POST_NAV_ITEM","__typename":"NavItem"},"Collection:680eee12c50d.navItems.5":{"title":"ALL","url":"https:\u002F\u002Fheartbeat.fritz.ai\u002Farchive","type":"ARCHIVE_NAV_ITEM","__typename":"NavItem"},"Collection:680eee12c50d.navItems.6":{"title":"ABOUT FRITZ","url":"http:\u002F\u002Ffritz.ai","type":"EXTERNAL_LINK_NAV_ITEM","__typename":"NavItem"},"$Collection:680eee12c50d.colorPalette.tintBackgroundSpectrum":{"backgroundColor":"#FF000000","colorPoints":[{"type":"id","generated":true,"id":"$Collection:680eee12c50d.colorPalette.tintBackgroundSpectrum.colorPoints.0","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:680eee12c50d.colorPalette.tintBackgroundSpectrum.colorPoints.1","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:680eee12c50d.colorPalette.tintBackgroundSpectrum.colorPoints.2","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:680eee12c50d.colorPalette.tintBackgroundSpectrum.colorPoints.3","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:680eee12c50d.colorPalette.tintBackgroundSpectrum.colorPoints.4","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:680eee12c50d.colorPalette.tintBackgroundSpectrum.colorPoints.5","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:680eee12c50d.colorPalette.tintBackgroundSpectrum.colorPoints.6","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:680eee12c50d.colorPalette.tintBackgroundSpectrum.colorPoints.7","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:680eee12c50d.colorPalette.tintBackgroundSpectrum.colorPoints.8","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:680eee12c50d.colorPalette.tintBackgroundSpectrum.colorPoints.9","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:680eee12c50d.colorPalette.tintBackgroundSpectrum.colorPoints.10","typename":"ColorPoint"}],"__typename":"ColorSpectrum"},"$Collection:680eee12c50d.colorPalette.tintBackgroundSpectrum.colorPoints.0":{"color":"#FF000000","point":0,"__typename":"ColorPoint"},"$Collection:680eee12c50d.colorPalette.tintBackgroundSpectrum.colorPoints.1":{"color":"#FF1E1D1D","point":0.1,"__typename":"ColorPoint"},"$Collection:680eee12c50d.colorPalette.tintBackgroundSpectrum.colorPoints.2":{"color":"#FF3C3B3B","point":0.2,"__typename":"ColorPoint"},"$Collection:680eee12c50d.colorPalette.tintBackgroundSpectrum.colorPoints.3":{"color":"#FF565555","point":0.3,"__typename":"ColorPoint"},"$Collection:680eee12c50d.colorPalette.tintBackgroundSpectrum.colorPoints.4":{"color":"#FF6F6D6D","point":0.4,"__typename":"ColorPoint"},"$Collection:680eee12c50d.colorPalette.tintBackgroundSpectrum.colorPoints.5":{"color":"#FF868484","point":0.5,"__typename":"ColorPoint"},"$Collection:680eee12c50d.colorPalette.tintBackgroundSpectrum.colorPoints.6":{"color":"#FF9C9A99","point":0.6,"__typename":"ColorPoint"},"$Collection:680eee12c50d.colorPalette.tintBackgroundSpectrum.colorPoints.7":{"color":"#FFB1AEAE","point":0.7,"__typename":"ColorPoint"},"$Collection:680eee12c50d.colorPalette.tintBackgroundSpectrum.colorPoints.8":{"color":"#FFC5C3C2","point":0.8,"__typename":"ColorPoint"},"$Collection:680eee12c50d.colorPalette.tintBackgroundSpectrum.colorPoints.9":{"color":"#FFD9D6D6","point":0.9,"__typename":"ColorPoint"},"$Collection:680eee12c50d.colorPalette.tintBackgroundSpectrum.colorPoints.10":{"color":"#FFECE9E9","point":1,"__typename":"ColorPoint"},"$Collection:680eee12c50d.colorPalette":{"tintBackgroundSpectrum":{"type":"id","generated":true,"id":"$Collection:680eee12c50d.colorPalette.tintBackgroundSpectrum","typename":"ColorSpectrum"},"__typename":"ColorPalette","defaultBackgroundSpectrum":{"type":"id","generated":true,"id":"$Collection:680eee12c50d.colorPalette.defaultBackgroundSpectrum","typename":"ColorSpectrum"},"highlightSpectrum":{"type":"id","generated":true,"id":"$Collection:680eee12c50d.colorPalette.highlightSpectrum","typename":"ColorSpectrum"}},"$Collection:680eee12c50d.colorPalette.defaultBackgroundSpectrum":{"backgroundColor":"#FFFFFFFF","colorPoints":[{"type":"id","generated":true,"id":"$Collection:680eee12c50d.colorPalette.defaultBackgroundSpectrum.colorPoints.0","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:680eee12c50d.colorPalette.defaultBackgroundSpectrum.colorPoints.1","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:680eee12c50d.colorPalette.defaultBackgroundSpectrum.colorPoints.2","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:680eee12c50d.colorPalette.defaultBackgroundSpectrum.colorPoints.3","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:680eee12c50d.colorPalette.defaultBackgroundSpectrum.colorPoints.4","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:680eee12c50d.colorPalette.defaultBackgroundSpectrum.colorPoints.5","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:680eee12c50d.colorPalette.defaultBackgroundSpectrum.colorPoints.6","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:680eee12c50d.colorPalette.defaultBackgroundSpectrum.colorPoints.7","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:680eee12c50d.colorPalette.defaultBackgroundSpectrum.colorPoints.8","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:680eee12c50d.colorPalette.defaultBackgroundSpectrum.colorPoints.9","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:680eee12c50d.colorPalette.defaultBackgroundSpectrum.colorPoints.10","typename":"ColorPoint"}],"__typename":"ColorSpectrum"},"$Collection:680eee12c50d.colorPalette.defaultBackgroundSpectrum.colorPoints.0":{"color":"#FF868484","point":0,"__typename":"ColorPoint"},"$Collection:680eee12c50d.colorPalette.defaultBackgroundSpectrum.colorPoints.1":{"color":"#FF7C7B7A","point":0.1,"__typename":"ColorPoint"},"$Collection:680eee12c50d.colorPalette.defaultBackgroundSpectrum.colorPoints.2":{"color":"#FF737171","point":0.2,"__typename":"ColorPoint"},"$Collection:680eee12c50d.colorPalette.defaultBackgroundSpectrum.colorPoints.3":{"color":"#FF696867","point":0.3,"__typename":"ColorPoint"},"$Collection:680eee12c50d.colorPalette.defaultBackgroundSpectrum.colorPoints.4":{"color":"#FF5F5E5E","point":0.4,"__typename":"ColorPoint"},"$Collection:680eee12c50d.colorPalette.defaultBackgroundSpectrum.colorPoints.5":{"color":"#FF555454","point":0.5,"__typename":"ColorPoint"},"$Collection:680eee12c50d.colorPalette.defaultBackgroundSpectrum.colorPoints.6":{"color":"#FF4A4949","point":0.6,"__typename":"ColorPoint"},"$Collection:680eee12c50d.colorPalette.defaultBackgroundSpectrum.colorPoints.7":{"color":"#FF3F3E3E","point":0.7,"__typename":"ColorPoint"},"$Collection:680eee12c50d.colorPalette.defaultBackgroundSpectrum.colorPoints.8":{"color":"#FF343333","point":0.8,"__typename":"ColorPoint"},"$Collection:680eee12c50d.colorPalette.defaultBackgroundSpectrum.colorPoints.9":{"color":"#FF272727","point":0.9,"__typename":"ColorPoint"},"$Collection:680eee12c50d.colorPalette.defaultBackgroundSpectrum.colorPoints.10":{"color":"#FF1A1A1A","point":1,"__typename":"ColorPoint"},"$Collection:680eee12c50d.colorPalette.highlightSpectrum":{"backgroundColor":"#FFFFFFFF","colorPoints":[{"type":"id","generated":true,"id":"$Collection:680eee12c50d.colorPalette.highlightSpectrum.colorPoints.0","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:680eee12c50d.colorPalette.highlightSpectrum.colorPoints.1","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:680eee12c50d.colorPalette.highlightSpectrum.colorPoints.2","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:680eee12c50d.colorPalette.highlightSpectrum.colorPoints.3","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:680eee12c50d.colorPalette.highlightSpectrum.colorPoints.4","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:680eee12c50d.colorPalette.highlightSpectrum.colorPoints.5","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:680eee12c50d.colorPalette.highlightSpectrum.colorPoints.6","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:680eee12c50d.colorPalette.highlightSpectrum.colorPoints.7","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:680eee12c50d.colorPalette.highlightSpectrum.colorPoints.8","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:680eee12c50d.colorPalette.highlightSpectrum.colorPoints.9","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:680eee12c50d.colorPalette.highlightSpectrum.colorPoints.10","typename":"ColorPoint"}],"__typename":"ColorSpectrum"},"$Collection:680eee12c50d.colorPalette.highlightSpectrum.colorPoints.0":{"color":"#FFF5F2F1","point":0,"__typename":"ColorPoint"},"$Collection:680eee12c50d.colorPalette.highlightSpectrum.colorPoints.1":{"color":"#FFF3F0EF","point":0.1,"__typename":"ColorPoint"},"$Collection:680eee12c50d.colorPalette.highlightSpectrum.colorPoints.2":{"color":"#FFF1EEED","point":0.2,"__typename":"ColorPoint"},"$Collection:680eee12c50d.colorPalette.highlightSpectrum.colorPoints.3":{"color":"#FFEFECEC","point":0.3,"__typename":"ColorPoint"},"$Collection:680eee12c50d.colorPalette.highlightSpectrum.colorPoints.4":{"color":"#FFEDEAEA","point":0.4,"__typename":"ColorPoint"},"$Collection:680eee12c50d.colorPalette.highlightSpectrum.colorPoints.5":{"color":"#FFEBE8E8","point":0.5,"__typename":"ColorPoint"},"$Collection:680eee12c50d.colorPalette.highlightSpectrum.colorPoints.6":{"color":"#FFE9E6E6","point":0.6,"__typename":"ColorPoint"},"$Collection:680eee12c50d.colorPalette.highlightSpectrum.colorPoints.7":{"color":"#FFE7E5E4","point":0.7,"__typename":"ColorPoint"},"$Collection:680eee12c50d.colorPalette.highlightSpectrum.colorPoints.8":{"color":"#FFE5E3E2","point":0.8,"__typename":"ColorPoint"},"$Collection:680eee12c50d.colorPalette.highlightSpectrum.colorPoints.9":{"color":"#FFE4E1E0","point":0.9,"__typename":"ColorPoint"},"$Collection:680eee12c50d.colorPalette.highlightSpectrum.colorPoints.10":{"color":"#FFE2DFDE","point":1,"__typename":"ColorPoint"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}})":{"isLockedPreviewOnly":false,"__typename":"PostContent","bodyModel":{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel","typename":"RichText"}},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.sections.0":{"name":"7a16","startIndex":0,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null,"__typename":"Section"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.sections.1":{"name":"d18c","startIndex":51,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null,"__typename":"Section"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.sections.2":{"name":"db48","startIndex":52,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null,"__typename":"Section"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.sections.3":{"name":"7378","startIndex":101,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null,"__typename":"Section"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel":{"sections":[{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.sections.0","typename":"Section"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.sections.1","typename":"Section"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.sections.2","typename":"Section"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.sections.3","typename":"Section"}],"paragraphs":[{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.0","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.1","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.2","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.3","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.4","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.5","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.6","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.7","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.8","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.9","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.10","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.11","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.12","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.13","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.14","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.15","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.16","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.17","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.18","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.19","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.20","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.21","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.22","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.23","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.24","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.25","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.26","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.27","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.28","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.29","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.30","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.31","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.32","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.33","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.34","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.35","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.36","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.37","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.38","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.39","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.40","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.41","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.42","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.43","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.44","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.45","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.46","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.47","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.48","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.49","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.50","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.51","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.52","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.53","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.54","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.55","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.56","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.57","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.58","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.59","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.60","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.61","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.62","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.63","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.64","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.65","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.66","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.67","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.68","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.69","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.70","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.71","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.72","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.73","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.74","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.75","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.76","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.77","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.78","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.79","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.80","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.81","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.82","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.83","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.84","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.85","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.86","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.87","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.88","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.89","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.90","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.91","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.92","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.93","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.94","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.95","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.96","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.97","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.98","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.99","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.100","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.101","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.102","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.103","typename":"Paragraph"}],"__typename":"RichText"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.0":{"name":"c076","__typename":"Paragraph","type":"IMG","href":null,"layout":"FULL_WIDTH","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*4zHA9YtHIzKTm5Ix98rKZg.png","typename":"ImageMetadata"},"text":"Digital Eye","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*4zHA9YtHIzKTm5Ix98rKZg.png":{"id":"1*4zHA9YtHIzKTm5Ix98rKZg.png","originalHeight":616,"originalWidth":1046,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.1":{"name":"bde7","__typename":"Paragraph","type":"H3","href":null,"layout":null,"metadata":null,"text":"The 5 Computer Vision Techniques That Will Change How You See The World","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.2":{"name":"7594","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Computer Vision is one of the hottest research fields within Deep Learning at the moment. It sits at the intersection of many academic subjects, such as Computer Science (Graphics, Algorithms, Theory, Systems, Architecture), Mathematics (Information Retrieval, Machine Learning), Engineering (Robotics, Speech, NLP, Image Processing), Physics (Optics), Biology (Neuroscience), and Psychology (Cognitive Science).","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.2.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.2.markups.0":{"type":"A","start":0,"end":15,"href":"https:\u002F\u002Fheartbeat.fritz.ai\u002Fthe-5-trends-that-dominated-computer-vision-in-2018-de38fbb9bd86","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.3":{"name":"aec3","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"As Computer Vision represents a relative understanding of visual environments and their contexts, many scientists believe the field paves the way towards Artificial General Intelligence due to its cross-domain mastery.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.4":{"name":"a7b3","__typename":"Paragraph","type":"H4","href":null,"layout":null,"metadata":null,"text":"So what is Computer Vision?","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.4.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.4.markups.0":{"type":"STRONG","start":3,"end":27,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.5":{"name":"854e","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Here are a couple of formal textbook definitions:","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.6":{"name":"d661","__typename":"Paragraph","type":"ULI","href":null,"layout":null,"metadata":null,"text":"“the construction of explicit, meaningful descriptions of physical objects from images” (Ballard & Brown, 1982)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.6.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.6.markups.0":{"type":"A","start":89,"end":104,"href":"https:\u002F\u002Fwww.amazon.com\u002FComputer-Vision-Dana-H-Ballard\u002Fdp\u002F0131653164","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.7":{"name":"cf11","__typename":"Paragraph","type":"ULI","href":null,"layout":null,"metadata":null,"text":"“computing properties of the 3D world from one or more digital images” (Trucco & Verri, 1998)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.7.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.7.markups.0":{"type":"A","start":72,"end":86,"href":"https:\u002F\u002Fwww.amazon.com\u002FIntroductory-Techniques-3-D-Computer-Vision\u002Fdp\u002F0132611082","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.8":{"name":"acca","__typename":"Paragraph","type":"ULI","href":null,"layout":null,"metadata":null,"text":"“to make useful decisions about real physical objects and scenes based on sensed images” (Sockman & Shapiro, 2001)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.8.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.8.markups.0":{"type":"A","start":90,"end":107,"href":"https:\u002F\u002Fwww.amazon.com\u002FComputer-Vision-Linda-G-Shapiro\u002Fdp\u002F0130307963","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.9":{"name":"825d","__typename":"Paragraph","type":"H4","href":null,"layout":null,"metadata":null,"text":"Why study Computer Vision?","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.9.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.9.markups.0":{"type":"STRONG","start":0,"end":26,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.10":{"name":"4b5a","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"The most obvious answer is that there’s a fast-growing collection of useful applications derived from this field of study. Here are just a handful of them:","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.11":{"name":"dd43","__typename":"Paragraph","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Face recognition: Snapchat and Facebook use face-detection algorithms to apply filters and recognize you in pictures.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.11.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.11.markups.0":{"type":"A","start":44,"end":58,"href":"https:\u002F\u002Fheartbeat.fritz.ai\u002Fbuilding-a-real-time-face-detector-in-android-with-ml-kit-f930eb7b36d9","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.12":{"name":"bc7a","__typename":"Paragraph","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Image retrieval: Google Images uses content-based queries to search relevant images. The algorithms analyze the content in the query image and return results based on best-matched content.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.13":{"name":"ffb7","__typename":"Paragraph","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Gaming and controls: A great commercial product in gaming that uses stereo vision is Microsoft Kinect.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.14":{"name":"a75f","__typename":"Paragraph","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Surveillance: Surveillance cameras are ubiquitous at public locations and are used to detect suspicious behaviors.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.15":{"name":"73d1","__typename":"Paragraph","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Biometrics: Fingerprint, iris and face matching remains some common methods in biometric identification.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.15.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.15.markups.0":{"type":"A","start":34,"end":47,"href":"https:\u002F\u002Fsupport.apple.com\u002Fen-us\u002FHT208108","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.16":{"name":"325c","__typename":"Paragraph","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Smart cars: Vision remains the main source of information to detect traffic signs and lights and other visual features.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.17":{"name":"06c6","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"I recently finished Stanford’s wonderful CS231n course on using Convolutional Neural Networks for visual recognition. Visual recognition tasks such as image classification, localization, and detection are key components of Computer vision.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.17.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.17.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.17.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.17.markups.3","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.17.markups.0":{"type":"A","start":41,"end":54,"href":"http:\u002F\u002Fcs231n.stanford.edu\u002F","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.17.markups.1":{"type":"A","start":64,"end":93,"href":"https:\u002F\u002Fheartbeat.fritz.ai\u002Fa-beginners-guide-to-convolutional-neural-networks-cnn-cf26c5ee17ed","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.17.markups.2":{"type":"A","start":173,"end":185,"href":"https:\u002F\u002Fheartbeat.fritz.ai\u002Fgentle-guide-on-how-yolo-object-localization-works-with-keras-part-2-65fe59ac12d","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.17.markups.3":{"type":"A","start":191,"end":200,"href":"https:\u002F\u002Fheartbeat.fritz.ai\u002Fdetecting-objects-in-videos-and-camera-feeds-using-keras-opencv-and-imageai-c869fe1ebcdb","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.18":{"name":"f7b7","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Recent developments in neural networks and deep learning approaches have greatly advanced the performance of these state-of-the-art visual recognition systems. The course is a phenomenal resource that taught me the details of deep learning architectures being used in cutting-edge computer vision research. In this article, I want to share the 5 major computer vision techniques I’ve learned as well as major deep learning models and applications using each of them.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.19":{"name":"f4aa","__typename":"Paragraph","type":"H3","href":null,"layout":null,"metadata":null,"text":"1 — Image Classification","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.19.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.19.markups.0":{"type":"STRONG","start":0,"end":24,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.20":{"name":"2794","__typename":"Paragraph","type":"IMG","href":null,"layout":"FULL_WIDTH","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*TaXXuvQ6kBn1nCcLVlhpAA.jpeg","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*TaXXuvQ6kBn1nCcLVlhpAA.jpeg":{"id":"1*TaXXuvQ6kBn1nCcLVlhpAA.jpeg","originalHeight":653,"originalWidth":2000,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.21":{"name":"c398","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"The problem of image classification goes like this: Given a set of images that are all labeled with a single category, we’re asked to predict these categories for a novel set of test images and measure the accuracy of the predictions. There are a variety of challenges associated with this task, including viewpoint variation, scale variation, intra-class variation, image deformation, image occlusion, illumination conditions, and background clutter.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.21.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.21.markups.0":{"type":"A","start":15,"end":35,"href":"https:\u002F\u002Fheartbeat.fritz.ai\u002Fbasics-of-image-classification-with-pytorch-2f8973c51864","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.22":{"name":"62b7","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"How might we go about writing an algorithm that can classify images into distinct categories? Computer Vision researchers have come up with a data-driven approach to solve this. Instead of trying to specify what every one of the image categories of interest look like directly in code, they provide the computer with many examples of each image class and then develop learning algorithms that look at these examples and learn about the visual appearance of each class.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.23":{"name":"44d4","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"In other words, they first accumulate a training dataset of labeled images, then feed it to the computer to process the data. Given that fact, the complete image classification pipeline can be formalized as follows:","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.24":{"name":"c2ad","__typename":"Paragraph","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Our input is a training dataset that consists of N images, each labeled with one of K different classes.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.24.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.24.markups.1","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.24.markups.0":{"type":"EM","start":49,"end":50,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.24.markups.1":{"type":"EM","start":84,"end":85,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.25":{"name":"a619","__typename":"Paragraph","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Then, we use this training set to train a classifier to learn what every one of the classes looks like.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.25.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.25.markups.0":{"type":"A","start":34,"end":52,"href":"https:\u002F\u002Fheartbeat.fritz.ai\u002Ftraining-a-core-ml-model-with-turi-create-to-classify-dog-breeds-d10009bd30b6","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.26":{"name":"3c97","__typename":"Paragraph","type":"ULI","href":null,"layout":null,"metadata":null,"text":"In the end, we evaluate the quality of the classifier by asking it to predict labels for a new set of images that it’s never seen before. We’ll then compare the true labels of these images to the ones predicted by the classifier.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.27":{"name":"91cd","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"The most popular architecture used for image classification is Convolutional Neural Networks (CNNs). A typical use case for CNNs is where you feed the network images and the network classifies the data. CNNs tend to start with an input “scanner” which isn’t intended to parse all the training data at once. For example, to input an image of 100 x 100 pixels, you wouldn’t want a layer with 10,000 nodes.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.27.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.27.markups.0":{"type":"STRONG","start":63,"end":101,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.28":{"name":"7544","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Rather, you create a scanning input layer of say 10 x 10 which you feed the first 10 x 10 pixels of the image. Once you passed that input, you feed it the next 10 x 10 pixels by moving the scanner one pixel to the right. This technique is known as sliding windows.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.28.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.28.markups.0":{"type":"STRONG","start":248,"end":263,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.29":{"name":"c654","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*VqRKWmxwIakOSnWPURoCSA.jpeg","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*VqRKWmxwIakOSnWPURoCSA.jpeg":{"id":"1*VqRKWmxwIakOSnWPURoCSA.jpeg","originalHeight":341,"originalWidth":1000,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.30":{"name":"3fb7","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"This input data is then fed through convolutional layers instead of normal layers. Each node only concerns itself with close neighboring cells. These convolutional layers also tend to shrink as they become deeper, mostly by easily divisible factors of the input. Besides these convolutional layers, they also often feature pooling layers. Pooling is a way to filter out details: a commonly found pooling technique is max pooling, where we take, say, 2 x 2 pixels and pass on the pixel with the most amount of a certain attribute.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.30.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.30.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.30.markups.2","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.30.markups.0":{"type":"A","start":323,"end":337,"href":"https:\u002F\u002Fwww.coursera.org\u002Flecture\u002Fconvolutional-neural-networks\u002Fpooling-layers-hELHk","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.30.markups.1":{"type":"EM","start":323,"end":337,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.30.markups.2":{"type":"EM","start":417,"end":428,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.31":{"name":"af50","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Most image classification techniques nowadays are trained on ImageNet, a dataset with approximately 1.2 million high-resolution training images. Test images will be presented with no initial annotation (no segmentation or labels), and algorithms will have to produce labelings specifying what objects are present in the images. Some of the best existing computer vision methods were tried on this dataset by leading computer vision groups from Oxford, INRIA, and XRCE. Typically, computer vision systems use complicated multi-stage pipelines, and the early stages are typically hand-tuned by optimizing a few parameters.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.31.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.31.markups.1","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.31.markups.0":{"type":"A","start":61,"end":69,"href":"http:\u002F\u002Fwww.image-net.org\u002F","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.31.markups.1":{"type":"STRONG","start":61,"end":69,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.32":{"name":"e6ef","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"The winner of the 1st ImageNet competition, Alex Krizhevsky (NIPS 2012), developed a very deep convolutional neural net of the type pioneered by Yann LeCun. Its architecture includes 7 hidden layers, not counting some max pooling layers. The early layers were convolutional, while the last 2 layers were globally connected. The activation functions were rectified linear units in every hidden layer. These train much faster and are more expressive than logistic units. In addition to that, it also uses competitive normalization to suppress hidden activities when nearby units have stronger activities. This helps with variations in intensity.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.32.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.32.markups.0":{"type":"A","start":44,"end":71,"href":"http:\u002F\u002Fwww.image-net.org\u002Fchallenges\u002FLSVRC\u002F2012\u002Fsupervision.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.33":{"name":"6a10","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*kQEY8G7mi88orwys7CQLjw.jpeg","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*kQEY8G7mi88orwys7CQLjw.jpeg":{"id":"1*kQEY8G7mi88orwys7CQLjw.jpeg","originalHeight":3390,"originalWidth":6000,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.34":{"name":"edda","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"In terms of hardware requirements, Alex uses a very efficient implementation of convolutional nets on 2 Nvidia GTX 580 GPUs (over 1000 fast little cores). The GPUs are very good for matrix-matrix multiplies and also have very high bandwidth to memory. This allows him to train the network in a week and makes it quick to combine results from 10 patches at test time. We can spread a network over many cores if we can communicate the states fast enough. As cores get cheaper and datasets get bigger, big neural nets will improve faster than old-fashioned CV systems. Since AlexNet, there have been multiple new models using CNN as their backbone architecture and achieving excellent results in ImageNet: ZFNet (2013), GoogLeNet (2014), VGGNet (2014), ResNet (2015), DenseNet (2016) etc.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.34.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.34.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.34.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.34.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.34.markups.4","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.34.markups.0":{"type":"A","start":703,"end":708,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1311.2901.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.34.markups.1":{"type":"A","start":717,"end":726,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1409.4842.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.34.markups.2":{"type":"A","start":735,"end":741,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1409.1556.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.34.markups.3":{"type":"A","start":750,"end":756,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1512.03385.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.34.markups.4":{"type":"A","start":765,"end":773,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1608.06993.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.35":{"name":"66f9","__typename":"Paragraph","type":"H3","href":null,"layout":null,"metadata":null,"text":"2 — Object Detection","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.35.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.35.markups.0":{"type":"STRONG","start":0,"end":20,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.36":{"name":"f833","__typename":"Paragraph","type":"IMG","href":null,"layout":"FULL_WIDTH","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*QSgvANEHZ99_MMYqAb1eBg.jpeg","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*QSgvANEHZ99_MMYqAb1eBg.jpeg":{"id":"1*QSgvANEHZ99_MMYqAb1eBg.jpeg","originalHeight":800,"originalWidth":1200,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.37":{"name":"dbc6","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"The task to define objects within images usually involves outputting bounding boxes and labels for individual objects. This differs from the classification \u002F localization task by applying classification and localization to many objects instead of just a single dominant object. You only have 2 classes of object classification, which means object bounding boxes and non-object bounding boxes. For example, in car detection, you have to detect all cars in a given image with their bounding boxes.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.37.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.37.markups.0":{"type":"A","start":12,"end":40,"href":"https:\u002F\u002Fheartbeat.fritz.ai\u002Fdetecting-objects-in-videos-and-camera-feeds-using-keras-opencv-and-imageai-c869fe1ebcdb","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.38":{"name":"46b6","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"If we use the Sliding Window technique like the way we classify and localize images, we need to apply a CNN to many different crops of the image. Because CNN classifies each crop as object or background, we need to apply CNN to huge numbers of locations and scales, which is very computationally expensive!","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.39":{"name":"90a5","__typename":"Paragraph","type":"IMG","href":null,"layout":"OUTSET_LEFT","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*jIjsYKmgH5nykbqQ6Jyfhg.jpeg","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*jIjsYKmgH5nykbqQ6Jyfhg.jpeg":{"id":"1*jIjsYKmgH5nykbqQ6Jyfhg.jpeg","originalHeight":956,"originalWidth":1503,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.40":{"name":"afa4","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"In order to cope with this, neural network researchers have proposed to use regions instead, where we find “blobby” image regions that are likely to contain objects.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.40.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.40.markups.0":{"type":"STRONG","start":76,"end":83,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.41":{"name":"4fe2","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"This is relatively fast to run. The first model that kicked things off was R-CNN(Region-based Convolutional Neural Network). In a R-CNN, we first scan the input image for possible objects using an algorithm called Selective Search, generating ~2,000 region proposals. Then we run a CNN on top of each of these region proposals. Finally, we take the output of each CNN and feed it into an SVM to classify the region and a linear regression to tighten the bounding box of the object.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.41.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.41.markups.1","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.41.markups.0":{"type":"A","start":75,"end":80,"href":"https:\u002F\u002Fwww.cv-foundation.org\u002Fopenaccess\u002Fcontent_cvpr_2014\u002Fpapers\u002FGirshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.41.markups.1":{"type":"STRONG","start":75,"end":80,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.42":{"name":"28c7","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Essentially, we turned object detection into an image classification problem. However, there are some problems — the training is slow, a lot of disk space is required, and inference is also slow.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.43":{"name":"86e4","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"An immediate descendant to R-CNN is Fast R-CNN, which improves the detection speed through 2 augmentations: 1) Performing feature extraction before proposing regions, thus only running one CNN over the entire image, and 2) Replacing SVM with a softmax layer, thus extending the neural network for predictions instead of creating a new model.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.43.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.43.markups.1","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.43.markups.0":{"type":"A","start":36,"end":46,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1504.08083.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.43.markups.1":{"type":"STRONG","start":36,"end":46,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.44":{"name":"0466","__typename":"Paragraph","type":"IMG","href":null,"layout":"OUTSET_LEFT","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*KD6dHKVvLX7fQIk6PM_cKw.jpeg","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*KD6dHKVvLX7fQIk6PM_cKw.jpeg":{"id":"1*KD6dHKVvLX7fQIk6PM_cKw.jpeg","originalHeight":852,"originalWidth":1334,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.45":{"name":"c5ce","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Fast R-CNN performed much better in terms of speed, because it trains just one CNN for the entire image. However, the selective search algorithm is still taking a lot of time to generate region proposals.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.46":{"name":"d5cb","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Thus comes the invention of Faster R-CNN, which now is a canonical model for deep learning-based object detection. It replaces the slow selective search algorithm with a fast neural network by inserting a Region Proposal Network (RPN) to predict proposals from features. The RPN is used to decide “where” to look in order to reduce the computational requirements of the overall inference process. The RPN quickly and efficiently scans every location in order to assess whether further processing needs to be carried out in a given region. It does that by outputting k bounding box proposals each with 2 scores representing the probability of object or not at each location.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.46.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.46.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.46.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.46.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.46.markups.4","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.46.markups.0":{"type":"A","start":28,"end":40,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1506.01497.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.46.markups.1":{"type":"A","start":205,"end":229,"href":"https:\u002F\u002Fmedium.com\u002F@tanaykarmarkar\u002Fregion-proposal-network-rpn-backbone-of-faster-r-cnn-4a744a38d7f9","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.46.markups.2":{"type":"STRONG","start":28,"end":40,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.46.markups.3":{"type":"STRONG","start":205,"end":228,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.46.markups.4":{"type":"EM","start":566,"end":567,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.47":{"name":"c8e4","__typename":"Paragraph","type":"IMG","href":null,"layout":"OUTSET_LEFT","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*5KOZC81rRy7GsTLzXkNjYg.jpeg","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*5KOZC81rRy7GsTLzXkNjYg.jpeg":{"id":"1*5KOZC81rRy7GsTLzXkNjYg.jpeg","originalHeight":680,"originalWidth":758,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.48":{"name":"cadb","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Once we have our region proposals, we feed them straight into what is essentially a Fast R-CNN. We add a pooling layer, some fully-connected layers, and finally a softmax classification layer and bounding box regressor.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.49":{"name":"55a5","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Altogether, Faster R-CNN achieved much better speeds and higher accuracy. It’s worth noting that although future models did a lot to increase detection speeds, few models managed to outperform Faster R-CNN by a significant margin. In other words, Faster R-CNN may not be the simplest or fastest method for object detection, but it’s still one of the best performing.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.50":{"name":"13f0","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Major Object Detection trends in recent years have shifted towards quicker, more efficient detection systems. This was visible in approaches like You Only Look Once (YOLO), Single Shot MultiBox Detector (SSD), and Region-Based Fully Convolutional Networks (R-FCN) as a move towards sharing computation on a whole image. Hence, these approaches differentiate themselves from the costly subnetworks associated with the 3 R-CNN techniques. The main rationale behind these trends is to avoid having separate algorithms focus on their respective subproblems in isolation, as this typically increases training time and can lower network accuracy.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.50.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.50.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.50.markups.2","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.50.markups.0":{"type":"A","start":146,"end":164,"href":"http:\u002F\u002Flanl.arxiv.org\u002Fpdf\u002F1612.08242v1","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.50.markups.1":{"type":"A","start":173,"end":202,"href":"http:\u002F\u002Flanl.arxiv.org\u002Fpdf\u002F1512.02325v5","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.50.markups.2":{"type":"A","start":214,"end":255,"href":"http:\u002F\u002Flanl.arxiv.org\u002Fpdf\u002F1605.06409v2","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.51":{"name":"ebe0","__typename":"Paragraph","type":"PQ","href":null,"layout":null,"metadata":null,"text":"Passionate about machine learning? Same! We’re curating each week’s biggest stories, best tutorials, and latest research so you don’t have to. Sign up for weekly updates delivered to your inbox.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.51.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.51.markups.0":{"type":"A","start":143,"end":150,"href":"https:\u002F\u002Fwww.deeplearningweekly.com\u002Fnewsletter?utm_campaign=dlweekly-newsletter-timesaver4&utm_source=heartbeat","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.52":{"name":"f87f","__typename":"Paragraph","type":"H3","href":null,"layout":null,"metadata":null,"text":"3 — Object Tracking","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.52.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.52.markups.0":{"type":"STRONG","start":0,"end":19,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.53":{"name":"e9c6","__typename":"Paragraph","type":"IMG","href":null,"layout":"FULL_WIDTH","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*ecz875HfaF_7S7hPsTc6pA.jpeg","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*ecz875HfaF_7S7hPsTc6pA.jpeg":{"id":"1*ecz875HfaF_7S7hPsTc6pA.jpeg","originalHeight":522,"originalWidth":1112,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.54":{"name":"9b83","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Object Tracking refers to the process of following a specific object of interest, or multiple objects, in a given scene. It traditionally has applications in video and real-world interactions where observations are made following an initial object detection. Now, it’s crucial to autonomous driving systems such as self-driving vehicles from companies like Uber and Tesla.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.54.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.54.markups.0":{"type":"A","start":0,"end":16,"href":"https:\u002F\u002Fwww.pyimagesearch.com\u002F2018\u002F07\u002F23\u002Fsimple-object-tracking-with-opencv\u002F","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.55":{"name":"acf7","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Object Tracking methods can be divided into 2 categories according to the observation model: generative method and discriminative method. The generative method uses the generative model to describe the apparent characteristics and minimizes the reconstruction error to search the object, such as PCA.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.56":{"name":"f7e8","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"The discriminative method can be used to distinguish between the object and the background, its performance is more robust, and it gradually becomes the main method in tracking. The discriminative method is also referred to as Tracking-by-Detection, and deep learning belongs to this category. To achieve tracking-by-detection, we detect candidate objects for all frames and use deep learning to recognize the wanted object from the candidates. There are 2 kinds of basic network models that can be used: stacked auto encoders (SAE) and convolutional neural network (CNN).","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.56.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.56.markups.1","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.56.markups.0":{"type":"STRONG","start":505,"end":532,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.56.markups.1":{"type":"STRONG","start":537,"end":572,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.57":{"name":"8604","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"The most popular deep network for tracking tasks using SAE is Deep Learning Tracker, which proposes offline pre-training and online fine-tuning the net. The process works like this:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.57.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.57.markups.1","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.57.markups.0":{"type":"A","start":62,"end":83,"href":"https:\u002F\u002Fpapers.nips.cc\u002Fpaper\u002F5192-learning-a-deep-compact-image-representation-for-visual-tracking.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.57.markups.1":{"type":"STRONG","start":62,"end":84,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.58":{"name":"20a8","__typename":"Paragraph","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Off-line unsupervised pre-train the stacked denoising auto-encoder using large-scale natural image datasets to obtain the general object representation. Stacked denoising auto-encoder can obtain more robust feature expression ability by adding noise in input images and reconstructing the original images.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.59":{"name":"0010","__typename":"Paragraph","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Combine the coding part of the pre-trained network with a classifier to get the classification network, then use the positive and negative samples obtained from the initial frame to fine-tune the network, which can discriminate the current object and background. DLT uses particle filter as the motion model to produce candidate patches of the current frame. The classification network outputs the probability scores for these patches, meaning the confidence of their classifications, then chooses the highest of these patches as the object.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.60":{"name":"fc85","__typename":"Paragraph","type":"ULI","href":null,"layout":null,"metadata":null,"text":"In the model updating, DLT uses the way of limited threshold.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.61":{"name":"649a","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*z94UOY-jMke-nZJWh3ZoKA.jpeg","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*z94UOY-jMke-nZJWh3ZoKA.jpeg":{"id":"1*z94UOY-jMke-nZJWh3ZoKA.jpeg","originalHeight":1148,"originalWidth":1020,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.62":{"name":"1ab2","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Because of its superiority in image classification and object detection, CNN has become the mainstream deep model in computer vision and in visual tracking. Generally speaking, a large-scale CNN can be trained both as a classifier and as a tracker. 2 representative CNN-based tracking algorithms are fully-convolutional network tracker (FCNT) and multi-domain CNN (MD Net).","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.62.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.62.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.62.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.62.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.62.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.62.markups.5","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.62.markups.0":{"type":"A","start":300,"end":335,"href":"https:\u002F\u002Fpdfs.semanticscholar.org\u002Fbf94\u002F906f0d7a8ca9da5f6b86e2a476fde1a34dd0.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.62.markups.1":{"type":"A","start":347,"end":363,"href":"https:\u002F\u002Fwww.cv-foundation.org\u002Fopenaccess\u002Fcontent_cvpr_2016\u002Fpapers\u002FNam_Learning_Multi-Domain_Convolutional_CVPR_2016_paper.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.62.markups.2":{"type":"STRONG","start":300,"end":335,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.62.markups.3":{"type":"STRONG","start":336,"end":342,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.62.markups.4":{"type":"STRONG","start":347,"end":363,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.62.markups.5":{"type":"STRONG","start":364,"end":373,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.63":{"name":"a910","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"FCNT analyzes and takes advantage of the feature maps of the VGG model successfully, which is a pre-trained ImageNet, and results in the following observations:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.63.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.63.markups.0":{"type":"A","start":61,"end":70,"href":"http:\u002F\u002Fwww.robots.ox.ac.uk\u002F~vgg\u002Fresearch\u002Fvery_deep\u002F","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.64":{"name":"153f","__typename":"Paragraph","type":"ULI","href":null,"layout":null,"metadata":null,"text":"CNN feature maps can be used for localization and tracking.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.65":{"name":"db9f","__typename":"Paragraph","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Many CNN feature maps are noisy or un-related for the task of discriminating a particular object from its background.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.66":{"name":"01ea","__typename":"Paragraph","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Higher layers capture semantic concepts on object categories, whereas lower layers encode more discriminative features to capture intra-class variation.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.67":{"name":"e866","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Because of these observations, FCNT designs the feature selection network to select the most relevant feature maps on the conv4–3 and conv5–3 layers of the VGG network. Then in order to avoid overfitting on noisy ones, it also designs extra two channels (called SNet and GNet) for the selected feature maps from two layers’ separately. The GNet captures the category information of the object, while the SNet discriminates the object from a background with a similar appearance.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.67.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.67.markups.1","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.67.markups.0":{"type":"STRONG","start":340,"end":344,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.67.markups.1":{"type":"STRONG","start":404,"end":408,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.68":{"name":"e192","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Both of the networks are initialized with the given bounding-box in the first frame to get heat maps of the object, and for new frames, a region of interest (ROI) centered at the object location in the last frame is cropped and propagated. At last, through SNet and GNet, the classifier gets two heat maps for prediction, and the tracker decides which heat map will be used to generate the final tracking result according to whether there are distractors. The pipeline of FCNT is shown below.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.69":{"name":"95cc","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*tGDaMhb--A2VODKkL3IyYQ.jpeg","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*tGDaMhb--A2VODKkL3IyYQ.jpeg":{"id":"1*tGDaMhb--A2VODKkL3IyYQ.jpeg","originalHeight":1700,"originalWidth":6000,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.70":{"name":"ad1d","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Different from the idea of FCNT, MD Net uses all the sequences of a video to to track movements in them. The networks mentioned above use irrelevant image data to reduce the training demand of tracking data, and this idea has some deviation from tracking. The object of one class in this video can be the background in another video, so MD Net proposes the idea of multi-domain to distinguish the object and background in every domain independently. And a domain indicates a set of videos that contain the same kind of object.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.71":{"name":"c4c9","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"As shown below, MD Net is divided into 2 parts: the shared layers and the K branches of domain-specific layers. Each branch contains a binary classification layer with softmax loss, which is used to distinguish the object and background in each domain, and the shared layers sharing with all domains to ensure the general representation.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.72":{"name":"0b88","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*HL4OhGMRtbHuy7v8depXjg.jpeg","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*HL4OhGMRtbHuy7v8depXjg.jpeg":{"id":"1*HL4OhGMRtbHuy7v8depXjg.jpeg","originalHeight":998,"originalWidth":2408,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.73":{"name":"85a6","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"In recent years, deep learning researchers have tried different ways to adapt to features of the visual tracking task. There are many directions that have been explored: applying other network models such as Recurrent Neural Net and Deep Belief Net, designing the network structure to adapt to video processing and end-to-end learning, optimizing the process, structure, and parameters, or even combining deep learning with traditional methods of computer vision or approaches in other fields such as Language Processing and Speech Recognition.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.73.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.73.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.73.markups.2","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.73.markups.0":{"type":"A","start":208,"end":228,"href":"https:\u002F\u002Fheartbeat.fritz.ai\u002Fdetecting-the-language-of-a-persons-name-using-pytorch-rnn-29a9090c20f2","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.73.markups.1":{"type":"A","start":233,"end":248,"href":"https:\u002F\u002Fcodeburst.io\u002Fdeep-learning-deep-belief-network-fundamentals-d0dcfd80d7d4","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.73.markups.2":{"type":"A","start":501,"end":520,"href":"https:\u002F\u002Fheartbeat.fritz.ai\u002Fthe-7-nlp-techniques-that-will-change-how-you-communicate-in-the-future-part-i-f0114b2f0497","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.74":{"name":"27ed","__typename":"Paragraph","type":"H3","href":null,"layout":null,"metadata":null,"text":"4 — Semantic Segmentation","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.74.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.74.markups.0":{"type":"STRONG","start":0,"end":25,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.75":{"name":"2c6e","__typename":"Paragraph","type":"IMG","href":null,"layout":"FULL_WIDTH","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*0V2fYKOROa4nCuj3Mi3DgQ.jpeg","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*0V2fYKOROa4nCuj3Mi3DgQ.jpeg":{"id":"1*0V2fYKOROa4nCuj3Mi3DgQ.jpeg","originalHeight":720,"originalWidth":1280,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.76":{"name":"5468","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Central to Computer Vision is the process of Segmentation, which divides whole images into pixel groupings which can then be labelled and classified.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.76.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.76.markups.0":{"type":"A","start":45,"end":57,"href":"https:\u002F\u002Fheartbeat.fritz.ai\u002Fbuilding-an-image-segmentation-app-in-ios-3377eb4a3e7c","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.77":{"name":"4ee4","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Particularly, Semantic Segmentation tries to semantically understand the role of each pixel in the image (e.g. is it a car, a motorbike, or some other type of class?). For example, in the picture above, apart from recognizing the person, the road, the cars, the trees, etc., we also have to delineate the boundaries of each object. Therefore, unlike classification, we need dense pixel-wise predictions from our models.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.78":{"name":"9c70","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"As with other computer vision tasks, CNNs have had enormous success on segmentation problems. One of the popular initial approaches was patch classification through a sliding window, where each pixel was separately classified into classes using a patch of images around it. This, however, is very inefficient computationally because we don’t reuse the shared features between overlapping patches.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.79":{"name":"9580","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"The solution, instead, is UC Berkeley’s Fully Convolutional Networks (FCN), which popularized end-to-end CNN architectures for dense predictions without any fully connected layers. This allowed segmentation maps to be generated for images of any size and was also much faster compared to the patch classification approach. Almost all subsequent approaches to semantic segmentation adopted this paradigm.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.79.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.79.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.79.markups.2","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.79.markups.0":{"type":"A","start":40,"end":68,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1411.4038.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.79.markups.1":{"type":"STRONG","start":40,"end":68,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.79.markups.2":{"type":"STRONG","start":69,"end":76,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.80":{"name":"7c04","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*_k5SCYeFy43b_CFv_zFimQ.jpeg","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*_k5SCYeFy43b_CFv_zFimQ.jpeg":{"id":"1*_k5SCYeFy43b_CFv_zFimQ.jpeg","originalHeight":529,"originalWidth":1020,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.81":{"name":"c00e","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"However, one problem remains: convolutions at original image resolution will be very expensive. To deal with this, FCN uses downsampling and upsampling inside the network. The downsampling layer is known as striped convolution, while the upsampling layer is known as transposed convolution.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.82":{"name":"4f0d","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Despite the upsampling\u002Fdownsampling layers, FCN produces coarse segmentation maps because of information loss during pooling. SegNet is a more memory efficient architecture than FCN that uses-max pooling and an encoder-decoder framework. In SegNet, shortcut\u002Fskip connections are introduced from higher resolution feature maps to improve the coarseness of upsampling\u002Fdownsampling.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.82.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.82.markups.0":{"type":"A","start":126,"end":132,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1511.00561.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.83":{"name":"df1e","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*1-ulho5NzNJhq6YNR9KREg.jpeg","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*1-ulho5NzNJhq6YNR9KREg.jpeg":{"id":"1*1-ulho5NzNJhq6YNR9KREg.jpeg","originalHeight":446,"originalWidth":1600,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.84":{"name":"7b6b","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Recent research in Semantic Segmentation all relies heavily on fully convolutional networks, such as Dilated Convolutions, DeepLab, and RefineNet.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.84.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.84.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.84.markups.2","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.84.markups.0":{"type":"A","start":101,"end":121,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1511.07122.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.84.markups.1":{"type":"A","start":123,"end":130,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1412.7062.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.84.markups.2":{"type":"A","start":136,"end":145,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1611.06612.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.85":{"name":"da80","__typename":"Paragraph","type":"H3","href":null,"layout":null,"metadata":null,"text":"5 — Instance Segmentation","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.85.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.85.markups.0":{"type":"STRONG","start":0,"end":25,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.86":{"name":"871b","__typename":"Paragraph","type":"IMG","href":null,"layout":"FULL_WIDTH","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*pDJ1P9Rv-jcas51SZsVt4A.jpeg","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*pDJ1P9Rv-jcas51SZsVt4A.jpeg":{"id":"1*pDJ1P9Rv-jcas51SZsVt4A.jpeg","originalHeight":600,"originalWidth":1048,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.87":{"name":"7839","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Beyond Semantic Segmentation, Instance Segmentation segments different instances of classes, such as labelling 5 cars with 5 different colors. In classification, there’s generally an image with a single object as the focus and the task is to say what that image is. But in order to segment instances, we need to carry out far more complex tasks. We see complicated sights with multiple overlapping objects and different backgrounds, and we not only classify these different objects but also identify their boundaries, differences, and relations to one another!","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.88":{"name":"3a47","__typename":"Paragraph","type":"IMG","href":null,"layout":"OUTSET_LEFT","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*ClYLqVgNwZP_nUON061x_w.jpeg","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*ClYLqVgNwZP_nUON061x_w.jpeg":{"id":"1*ClYLqVgNwZP_nUON061x_w.jpeg","originalHeight":422,"originalWidth":558,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.89":{"name":"fb61","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"So far, we’ve seen how to use CNN features in many interesting ways to effectively locate different objects in an image with bounding boxes. Can we extend such techniques to locate exact pixels of each object instead of just bounding boxes? This instance segmentation problem is explored at Facebook AI using an architecture known as Mask R-CNN.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.89.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.89.markups.1","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.89.markups.0":{"type":"A","start":334,"end":344,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1703.06870.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.89.markups.1":{"type":"STRONG","start":334,"end":344,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.90":{"name":"9678","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Much like Fast R-CNN, and Faster R-CNN, Mask R-CNN’s underlying intuition is straightforward Given that Faster R-CNN works so well for object detection, could we extend it to also carry out pixel-level segmentation?","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.91":{"name":"7c24","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Mask R-CNN does this by adding a branch to Faster R-CNN that outputs a binary mask that says whether or not a given pixel is part of an object. The branch is a Fully Convolutional Network on top of a CNN-based feature map. Given the CNN Feature Map as the input, the network outputs a matrix with 1s on all locations where the pixel belongs to the object and 0s elsewhere (this is known as a binary mask).","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.91.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.91.markups.0":{"type":"A","start":392,"end":403,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FMask_%28computing%29","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.92":{"name":"f187","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*QgOk_xUmBM-_MlWSXBK-Dg.jpeg","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*QgOk_xUmBM-_MlWSXBK-Dg.jpeg":{"id":"1*QgOk_xUmBM-_MlWSXBK-Dg.jpeg","originalHeight":300,"originalWidth":692,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.93":{"name":"dbeb","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Additionally, when run without modifications on the original Faster R-CNN architecture, the regions of the feature map selected by RoIPool (Region of Interests Pool) were slightly misaligned from the regions of the original image. Since image segmentation requires pixel-level specificity, unlike bounding boxes, this naturally led to inaccuracies. Mask R-CNN solves this problem by adjusting RoIPool to be more precisely aligned using a method known as RoIAlign (Region of Interests Align). Essentially, RoIAlign uses bilinear interpolation to avoid error in rounding, which causes inaccuracies in detection and segmentation.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.93.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.93.markups.0":{"type":"STRONG","start":454,"end":462,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.94":{"name":"4cd4","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Once these masks are generated, Mask R-CNN combines them with the classifications and bounding boxes from Faster R-CNN to generate such wonderfully precise segmentations:","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.95":{"name":"0623","__typename":"Paragraph","type":"IMG","href":null,"layout":"OUTSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*fbDDJ5z8q5xaZ4BhiQGDIw.jpeg","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*fbDDJ5z8q5xaZ4BhiQGDIw.jpeg":{"id":"1*fbDDJ5z8q5xaZ4BhiQGDIw.jpeg","originalHeight":1077,"originalWidth":2000,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.96":{"name":"6373","__typename":"Paragraph","type":"H3","href":null,"layout":null,"metadata":null,"text":"Conclusion","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.96.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.96.markups.0":{"type":"STRONG","start":0,"end":10,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.97":{"name":"c592","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"These 5 major computer vision techniques can help a computer extract, analyze, and understand useful information from a single or a sequence of images. There are many other advanced techniques that I haven’t touched, including style transfer, colorization, action recognition, 3D objects, human pose estimation, and more.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.97.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.97.markups.1","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.97.markups.0":{"type":"A","start":227,"end":241,"href":"https:\u002F\u002Fheartbeat.fritz.ai\u002F20-minute-masterpiece-4b6043fdfff5","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.97.markups.1":{"type":"A","start":277,"end":287,"href":"https:\u002F\u002Fheartbeat.fritz.ai\u002F3d-face-reconstruction-with-position-map-regression-networks-36f0ac2d3ef1","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.98":{"name":"d53b","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Indeed, the field of Computer Vision is too expensive to cover in depth, and I would encourage you to explore it further, whether through online courses, blog tutorials, or formal documents. I’d highly recommend CS231n for starters, as you’ll learn to implement, train, and debug your own neural networks. As a bonus, you can get all the lecture slides and assignment guidelines from my GitHub repository. I hope it’ll guide you in the quest of changing how to see the world!","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.98.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.98.markups.1","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.98.markups.0":{"type":"A","start":384,"end":404,"href":"https:\u002F\u002Fgithub.com\u002Fkhanhnamle1994\u002Fcomputer-vision","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.98.markups.1":{"type":"STRONG","start":384,"end":404,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.99":{"name":"6a6b","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"If you enjoyed this piece, I’d love it if you hit the clap button 👏 so others might stumble upon it. You can find my own code on GitHub, and more of my writing and projects at https:\u002F\u002Fjameskle.com\u002F. You can also follow me on Twitter, email me directly or find me on LinkedIn. Sign up for my newsletter to receive my latest thoughts on data science, machine learning, and artificial intelligence right at your inbox!","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.99.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.99.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.99.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.99.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.99.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.99.markups.5","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.99.markups.6","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.99.markups.7","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.99.markups.8","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.99.markups.9","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.99.markups.0":{"type":"A","start":130,"end":136,"href":"https:\u002F\u002Fgithub.com\u002Fkhanhnamle1994","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.99.markups.1":{"type":"A","start":177,"end":198,"href":"https:\u002F\u002Fjameskle.com","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.99.markups.2":{"type":"A","start":226,"end":233,"href":"https:\u002F\u002Ftwitter.com\u002F@james_aka_yale","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.99.markups.3":{"type":"A","start":235,"end":252,"href":"mailto:khanhle.1013@gmail.com","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.99.markups.4":{"type":"A","start":256,"end":275,"href":"http:\u002F\u002Fwww.linkedin.com\u002Fin\u002Fkhanhnamle94","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.99.markups.5":{"type":"A","start":277,"end":302,"href":"http:\u002F\u002Feepurl.com\u002FdeWjzb","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.99.markups.6":{"type":"EM","start":0,"end":65,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.99.markups.7":{"type":"EM","start":69,"end":129,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.99.markups.8":{"type":"EM","start":130,"end":176,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.99.markups.9":{"type":"EM","start":177,"end":416,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.100":{"name":"b037","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Discuss this post on Hacker News","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.100.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.100.markups.1","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.100.markups.0":{"type":"A","start":21,"end":32,"href":"https:\u002F\u002Fnews.ycombinator.com\u002Fitem?id=16820833","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.100.markups.1":{"type":"STRONG","start":0,"end":32,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.101":{"name":"2b84","__typename":"Paragraph","type":"PQ","href":null,"layout":null,"metadata":null,"text":"Did you know: Machine learning isn’t just happening on servers and in the cloud. It’s also being deployed to the edge. Learn more about how Fritz is making this transition possible.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.101.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.101.markups.0":{"type":"A","start":119,"end":129,"href":"https:\u002F\u002Ffritz.ai?utm_campaign=educate2&utm_source=heartbeat","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.102":{"name":"a4be","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Editor’s Note: Join Heartbeat on Slack and follow us on Twitter and LinkedIn for the all the latest content, news, and more in machine learning, mobile development, and where the two intersect.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.102.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.102.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.102.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.102.markups.3","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.102.markups.0":{"type":"A","start":14,"end":38,"href":"http:\u002F\u002Fbit.ly\u002Fheartbeatslack","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.102.markups.1":{"type":"A","start":55,"end":63,"href":"https:\u002F\u002Ftwitter.com\u002Ffritzlabs","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.102.markups.2":{"type":"A","start":67,"end":76,"href":"https:\u002F\u002Fwww.linkedin.com\u002Fcompany\u002Ffritz-labs-inc\u002F","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.102.markups.3":{"type":"EM","start":0,"end":193,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.103":{"name":"a02e","__typename":"Paragraph","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":{"type":"id","generated":true,"id":"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.103.iframe","typename":"Iframe"},"mixtapeMetadata":null},"MediaResource:05616eaceabf5537ffbda5b6811c367c":{"id":"05616eaceabf5537ffbda5b6811c367c","iframeSrc":"https:\u002F\u002Fcdn.embedly.com\u002Fwidgets\u002Fmedia.html?src=https%3A%2F%2Fupscri.be%2Fdfdb75%3Fas_embed%3Dtrue&dntp=1&url=https%3A%2F%2Fupscri.be%2Fdfdb75%2F&image=http%3A%2F%2Fapi.screenshotlayer.com%2Fapi%2Fcapture%3Faccess_key%3Dfe59908dad3baab69ffab249a2224b03%26viewport%3D1024x612%26width%3D1000%26url%3Dhttps%253A%252F%252Fupscri.be%252Fdfdb75%253Fscreenshot&key=d04bfffea46d4aeda930ec88cc64b87c&type=text%2Fhtml&schema=upscri","iframeHeight":400,"iframeWidth":800,"title":"Like what you're reading?","__typename":"MediaResource"},"$Post:1ee19334354b.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.103.iframe":{"mediaResource":{"type":"id","generated":false,"id":"MediaResource:05616eaceabf5537ffbda5b6811c367c","typename":"MediaResource"},"__typename":"Iframe"},"Quote:anon_767dbfbf170d":{"id":"anon_767dbfbf170d","paragraphs":[{"type":"id","generated":true,"id":"Quote:anon_767dbfbf170d.paragraphs.0","typename":"Paragraph"}],"__typename":"Quote","userId":"anon","startOffset":118,"endOffset":239,"user":null},"Quote:anon_767dbfbf170d.paragraphs.0":{"name":"06c6","__typename":"Paragraph"},"Tag:machine-learning":{"id":"machine-learning","displayTitle":"Machine Learning","__typename":"Tag"},"Tag:deep-learning":{"id":"deep-learning","displayTitle":"Deep Learning","__typename":"Tag"},"Tag:computer-vision":{"id":"computer-vision","displayTitle":"Computer Vision","__typename":"Tag"},"Tag:heartbeat":{"id":"heartbeat","displayTitle":"Heartbeat","__typename":"Tag"},"Tag:machine-learning-tools":{"id":"machine-learning-tools","displayTitle":"Machine Learning Tools","__typename":"Tag"},"ImageMetadata:1*KpWOVdzvNX_R3ewGUwLmYA.png":{"id":"1*KpWOVdzvNX_R3ewGUwLmYA.png","__typename":"ImageMetadata"},"$Post:1ee19334354b.previewContent":{"subtitle":"","__typename":"PreviewContent"}}</script><script src="The%205%20Computer%20Vision%20Techniques%20That%20Will%20Change%20How%20You%20See%20The%20World_files/manifest.js"></script><script src="The%205%20Computer%20Vision%20Techniques%20That%20Will%20Change%20How%20You%20See%20The%20World_files/vendorsmain.js"></script><script src="The%205%20Computer%20Vision%20Techniques%20That%20Will%20Change%20How%20You%20See%20The%20World_files/main.js"></script><script src="The%205%20Computer%20Vision%20Techniques%20That%20Will%20Change%20How%20You%20See%20The%20World_files/vendorsscreen.js"></script>
<script src="The%205%20Computer%20Vision%20Techniques%20That%20Will%20Change%20How%20You%20See%20The%20World_files/screen.js"></script>
<script src="The%205%20Computer%20Vision%20Techniques%20That%20Will%20Change%20How%20You%20See%20The%20World_files/screen_004.js"></script>
<script src="The%205%20Computer%20Vision%20Techniques%20That%20Will%20Change%20How%20You%20See%20The%20World_files/screen_002.js"></script>
<script src="The%205%20Computer%20Vision%20Techniques%20That%20Will%20Change%20How%20You%20See%20The%20World_files/screen_003.js"></script><script>window.main();</script><script src="The%205%20Computer%20Vision%20Techniques%20That%20Will%20Change%20How%20You%20See%20The%20World_files/p.js" async="" id="parsely-cf"></script></body></html>