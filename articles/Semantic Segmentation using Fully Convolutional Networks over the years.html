<!DOCTYPE html>
<html><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
	  <meta charset="utf-8">
	  <meta http-equiv="X-UA-Compatible" content="IE=edge">
	  <meta name="viewport" content="width=device-width, initial-scale=1">

	  <title>Semantic Segmentation using Fully Convolutional Networks over the years</title>
	  <meta name="description" content="">

	  <link rel="stylesheet" href="Semantic%20Segmentation%20using%20Fully%20Convolutional%20Networks%20over%20the%20years_files/main.css">
	  <link rel="canonical" href="http://meetshah1995.github.io/semantic-segmentation/deep-learning/pytorch/visdom/2017/06/01/semantic-segmentation-over-the-years.html">
	  <link rel="alternate" type="application/rss+xml" title="int const change;" href="http://meetshah1995.github.io/feed.xml">
	</head>


  <body>

      <header class="site-header">

    <div class="wrapper">

      <a class="site-title" href="https://meetshah1995.github.io/">int const change;</a>

      <nav class="site-nav">
        <a href="#" class="menu-icon">
          <svg viewBox="0 0 18 15">
            <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"></path>
            <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"></path>
            <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"></path>
          </svg>
        </a>

        <div class="trigger">
            <a class="page-link" href="https://meetshah1995.github.io/">Home</a>
        </div>
      </nav>

    </div>

  </header>


    <div class="page-content">
      <div class="wrapper">
        <article class="post" itemscope="" itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">Semantic Segmentation using Fully Convolutional Networks over the years</h1>
    <p class="post-meta"><time datetime="2017-06-01T01:48:00+00:00" itemprop="datePublished">Jun 1, 2017</time></p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <style>

table {
  border-collapse: collapse;  
  padding: 0; }
  table tr {
    border-top: 0px solid #cccccc;
    background-color: white;
    margin: 0;
    padding: 0; }
    table tr:nth-child(2n) {
      background-color: #f8f8f8; }
    table tr th {
      font-weight: bold;
      border: 1px solid #cccccc;
      text-align: left;
      margin: 0;
      padding: 6px 13px; }
    table tr td {
      border: 1px solid #cccccc;
      text-align: left;
      margin: 0;
      padding: 6px 13px; }
    table tr th :first-child, table tr td :first-child {
      margin-top: 0; }
    table tr th :last-child, table tr td :last-child {
      margin-bottom: 0; }
</style>

<h3 id="semantic-segmentation">Semantic Segmentation</h3>

<h4 id="introduction">Introduction</h4>

<p>Semantic Segmentation of an image is to assign each pixel in the 
input image a semantic class in order to get a pixel-wise dense 
classification. While semantic segmentation / scene parsing has been a 
part of the computer vision community since <a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2007/index.html" target="_blank">2007</a>,
 but much like other areas in computer vision, major breakthrough came 
when fully convolutional neural networks were first used by <a href="https://arxiv.org/abs/1411.4038" target="_blank">2014 Long et. al.</a> to perform end-to-end segmentation of natural images.</p>

<p><br></p>
<p align="center">
<iframe class="juxtapose" src="Semantic%20Segmentation%20using%20Fully%20Convolutional%20Networks%20over%20the%20years_files/index.html" width="60%" height="319" frameborder="0"></iframe>
<br>
<small><b>Figure : </b>Example of semantic segmentation (Left) generated by FCN-8s ( trained using <a href="https://github.com/meetshah1995/pytorch-semseg" target="_blank">pytorch-semseg</a> repository) overlayed on the input image (Right)</small>
</p>

<p>The <a href="#sec_fcn">FCN-8s</a> architecture put forth achieved a <strong>20% relative improvement to 62.2% mean IU on Pascal VOC 2012 dataset</strong>.
 This architecture was in my opinion a baseline for semantic 
segmentation on top of which several newer and better architectures were
 developed.</p>

<p>Fully Convolutional Networks (FCNs) are being used for semantic 
segmentation of natural images, for multi-modal medical image analysis 
and multispectral satellite image segmentation. Very similar to deep 
classification networks like AlexNet, VGG, ResNet etc. there is also a 
large variety of deep architectures that perform semantic segmentation.</p>

<p>I summarize networks like <strong><a href="#sec_fcn">FCN</a>, <a href="#sec_segnet">SegNet</a>, <a href="#sec_unet">U-Net</a>, <a href="#sec_fcdensenet">FC-Densenet</a> <a href="#sec_elinknet">E-Net &amp; Link-Net</a>, <a href="#sec_refinenet">RefineNet</a>, <a href="#sec_pspnet">PSPNet</a>, <a href="#sec_maskrcnn">Mask-RCNN</a>, and some semi-supervised approaches like <a href="#sec_decouplednet">DecoupledNet</a> and <a href="#sec_gan">GAN-SS</a></strong> here and provide reference <a href="https://github.com/meetshah1995/pytorch-semseg" target="_blank"><strong>PyTorch</strong></a> and <a href="https://github.com/meetshah1995/pytorch-semseg" target="_blank"><strong>Keras</strong></a> (<strong>in progress</strong>) implementations for a number of them. In the last part of the post I summarize some popular <a href="#sec_datasets">datasets</a> and visualize a few <a href="#sec_results">results</a> with the trained networks.</p>

<h4 id="network-architectures">Network Architectures</h4>

<p>A general semantic segmentation architecture can be broadly thought of as an <strong>encoder</strong> network followed by a <strong>decoder</strong> network. The <strong>encoder</strong> is usually is a pre-trained classification network like VGG/ResNet followed by a decoder network. The <strong>decoder</strong> network/mechanism is mostly where these architectures differ. The task of the <strong>decoder</strong>
 is to semantically project the discriminative features (lower 
resolution) learnt by the encoder onto the pixel space (higher 
resolution) to get a dense classification.</p>

<p>Unlike classification where the end result of the very deep network (
 i.e. the class presence probability) is the only important thing, 
semantic segmentation not only requires discrimination at pixel level 
but also a mechanism to project the discriminative features learnt at 
different stages of the encoder onto the pixel space. Different 
architectures employ different mechanisms (skip connections, pyramid 
pooling etc) as a part of the decoding mechanism.</p>

<p>A number of above architectures and loaders for datasets is available in PyTorch at:</p>

<ul>
  <li><strong>PyTorch</strong>: <a href="https://github.com/meetshah1995/pytorch-semseg" target="_blank">meetshah1995/pytorch-semseg</a> 
<!--* **Keras**: [meetshah1995/keras-semseg](https://github.com/meetshah1995/keras-semseg) (**in progress**)--></li>
</ul>

<p>A more formal summarization of semantic segmentation ( including recurrent style networks ) can also be found <a href="https://arxiv.org/pdf/1704.06857.pdf" target="_blank">here</a></p>

<!-- Transfer learning from encoders like VGG16 and ResNet.

Difference in decoders - unpooling, deconv, billinear, residual, skip connections
Difference in encoders - 
 -->
<p><a name="sec_fcn"></a></p>

<p><strong><a href="https://arxiv.org/abs/1411.4038" target="_blank">Fully Convolution Networks (FCNs)</a></strong></p>

<table>
  <tbody>
    <tr>
      <td>CVPR 2015</td>
      <td>Fully Convolutional Networks for Semantic Segmentation</td>
      <td><a href="https://arxiv.org/abs/1411.4038" target="_blank">Arxiv</a></td>
    </tr>
  </tbody>
</table>

<p><br></p>

<blockquote>
  <p>We adapt contemporary classification networks (AlexNet, the VGG 
net, and GoogLeNet) into fully convolutional networks and transfer their
 learned representations by fine-tuning to the segmentation task. We 
then define a novel architecture that combines semantic information from
 a deep, coarse layer with appearance information from a shallow, fine 
layer to produce accurate and detailed segmentations. Our fully 
convolutional network achieves state-of-the-art segmentation of PASCAL 
VOC (20% relative improvement to 62.2% mean IU on 2012), NYUDv2, and 
SIFT Flow, while inference takes one third of a second for a typical 
image.</p>
</blockquote>

<p><br></p>
<p align="center">
<span>
<img src="http://meetshah1995.github.io/images/blog/ss/fcn.png" alt="arch_fcn" width="580px">
</span> &nbsp;&nbsp;&nbsp;
<br>
<small><b>Figure : </b> The FCN end-to-end dense prediction pipeline.</small>
</p>

<p>A few key features of networks of this type are:</p>

<ul>
  <li>The features are merged from different stages in the encoder which vary in <strong>coarseness of semantic information</strong>.</li>
  <li>The upsampling of learned low resolution semantic feature maps is done using <strong>deconvolutions which are initialized with billinear interpolation filters</strong>.</li>
  <li>Excellent example for <strong>knowledge transfer from modern classifier networks</strong> like VGG16, Alexnet to perform semantic segmentation</li>
</ul>

<p><br></p>
<p align="center">
<span>
<img src="http://meetshah1995.github.io/images/blog/ss/fcn_1.png" alt="arch_fcn_" width="450px">
</span> &nbsp;&nbsp;&nbsp;
<br>
<small><b>Figure : </b> Transforming fully connected layers into convolutions enables a classification network to output a class heatmap.</small>
</p>

<p>The fully connected layers (<code class="highlighter-rouge">fc6</code>, <code class="highlighter-rouge">fc7</code>) of classification networks like <code class="highlighter-rouge">VGG16</code>
 were converted to fully convolutional layers and as shown in the figure
 above, it produces a class presence heatmap in low resolution, which 
then is upsampled using billinearly initialized deconvolutions and at 
each stage of upsampling further refined by fusing (simple addition) 
features from coarser but higher resolution feature maps from lower 
layers in the VGG 16 (<code class="highlighter-rouge">conv4</code> and <code class="highlighter-rouge">conv3</code>) . A more detailed netscope-style visualization of the network can be found in at <a href="http://ethereon.github.io/netscope/#/preset/fcn-8s-pascal" target="_blank">here</a></p>

<p>In conventional classification CNNs, pooling is used to increase the 
field of view and at the same time reduce the feature map resolution. 
While this works best for classification as the end goal is to just find
 the presence of a particular class, while the spatial location of the 
object is not of relevance. Thus pooling is introduced after each 
convolution block, to enable the succeeding block to extract more 
abstract, class-sailent features from the pooled features.</p>

<p><br></p>
<p align="center">
<span>
<img src="http://meetshah1995.github.io/images/blog/ss/fcn_2.png" alt="arch_fcn__" width="580px">
</span> &nbsp;&nbsp;&nbsp;
<br>
<small><b>Figure :</b>  The FCN-32s Architecture</small>
</p>

<p>On the other hand any sort of operation - pooling or strided 
convolutions is deterimental to for semantic segmentation as spatial 
information is lost. Most of the architectures listed below mainly 
differ in the mechanism employed by them in the <strong>decoder</strong> to <em>recover</em> the information lost while reducing the resolution in the <strong>encoder</strong>. As seen above, FCN-8s fused features from different coarseness (<code class="highlighter-rouge">conv3</code>, <code class="highlighter-rouge">conv4</code> and <code class="highlighter-rouge">fc7</code>) to refine the segmentation using spatial information from different resolutions at different stages from the encoder.</p>

<p align="center">
<span>
<img src="Semantic%20Segmentation%20using%20Fully%20Convolutional%20Networks%20over%20the%20years_files/conv_1_1_gradient.png" alt="conv_1_1_gradient" width="250px">
</span> &nbsp;&nbsp;&nbsp;
<span>
<img src="Semantic%20Segmentation%20using%20Fully%20Convolutional%20Networks%20over%20the%20years_files/conv_4_1_gradient.png" alt="conv_4_1_gradient" width="250px">
</span> &nbsp;&nbsp;&nbsp;
<br>
<span>
<img src="Semantic%20Segmentation%20using%20Fully%20Convolutional%20Networks%20over%20the%20years_files/conv_4_2_gradient.png" alt="conv_4_2_gradient" width="250px">
</span> &nbsp;&nbsp;&nbsp;
<span>
<img src="Semantic%20Segmentation%20using%20Fully%20Convolutional%20Networks%20over%20the%20years_files/conv_4_3_gradient.png" alt="conv_4_3_gradient" width="250px">
</span> &nbsp;&nbsp;&nbsp;
<br>
<small><b>Figure :</b> Gradients at conv layers when training FCNs <a href="https://github.com/shekkizh/FCN.tensorflow" target="_blank">Source</a></small>
</p>

<p>The first conv layers captures low level geometric information and 
since this entrirely dataset dependent you notice the gradients 
adjusting the first layer weights to accustom the model to the dataset. 
Deeper conv layers from VGG have very small gradients flowing as the 
higher level semantic concepts captured here are good enough for 
segmentation. This is what amazes me about how well transfer learning 
works.</p>

<p align="center">
<span>
<img src="http://meetshah1995.github.io/images/blog/ss/deconv.gif" alt="deconv" width="250px">
</span> &nbsp;&nbsp;&nbsp;
<span>
<img src="http://meetshah1995.github.io/images/blog/ss/dilation.gif" alt="dilated" width="250px">
</span> &nbsp;&nbsp;&nbsp;
<br>
<small><b>Left : </b> Deconvolution (Transposed Convolution) and <b>Right : </b> Dilated (Atrous) Convolution <a href="https://github.com/vdumoulin/conv_arithmetic" target="_blank">Source</a></small>
</p>

<p>Other important aspect for a semantic segmentation architecture is the mechanism used for <strong>feature upsampling</strong>
 the low-resolution segmentation maps to input image resolution using 
learned deconvolutions or partially avoid the reduction of resolution 
altogether in the encoder using dilated convolutions at the cost of 
computation. Dilated convolutions are very expensive, even on modern 
GPUs. This post on <a href="http://distill.pub/2016/deconv-checkerboard/" target="_blank">distill.pub</a> explains in a much more detail about deconvolutions.</p>

<hr>
<p><br></p>

<p><a name="sec_segnet"></a></p>

<p><strong><a href="https://arxiv.org/abs/1511.00561" target="_blank">SegNet</a></strong></p>

<table>
  <tbody>
    <tr>
      <td>2015</td>
      <td>SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation</td>
      <td><a href="https://arxiv.org/abs/1511.00561" target="_blank">Arxiv</a></td>
    </tr>
  </tbody>
</table>

<p><br></p>

<blockquote>
  <p>The novelty of SegNet lies is in the manner in which the decoder 
upsamples its lower resolution input feature map(s). Specifically, the 
decoder uses pooling indices computed in the max-pooling step of the 
corresponding encoder to perform non-linear upsampling. This eliminates 
the need for learning to upsample. The upsampled maps are sparse and are
 then convolved with trainable filters to produce dense feature maps. We
 compare our proposed architecture with the widely adopted FCN and also 
with the well known DeepLab-LargeFOV, DeconvNet architectures. This 
comparison reveals the memory versus accuracy trade-off involved in 
achieving good segmentation performance.</p>
</blockquote>

<p><br></p>
<p align="center">
<span>
<img src="http://meetshah1995.github.io/images/blog/ss/segnet.png" alt="arch_segnet" width="580px">
</span> &nbsp;&nbsp;&nbsp;
<br>
<small><b>Figure : </b> The SegNet Architecture</small>
</p>

<p>A few key features of networks of this type are:</p>

<ul>
  <li>SegNet uses <strong>unpooling</strong> to upsample feature maps in decoder to use and keep high frequency details intact in the segmentation.</li>
  <li>This encoder doesn’t use the fully connected layers (by 
convolutionizing them as FCN) and hence is lightweight network lesser 
parameters.</li>
</ul>

<p align="center">
<span>
<img src="http://meetshah1995.github.io/images/blog/ss/unpooling.png" alt="unpool" width="450px">
</span> &nbsp;&nbsp;&nbsp;
<br>
<small><b>Figure : </b> Max Unpooling</small>
</p>

<p>As shown in the above image, the indices at each max-pooling layer in
 encoder are stored and later used to upsample the correspoing feature 
map in the decoder by unpooling it using those stored indices. While 
this helps keep the high-frequency information intact, it also misses 
neighbouring information when unpooling from low-resolution feature 
maps.</p>

<hr>
<p><br></p>

<p><a name="sec_unet"></a></p>

<p><strong><a href="https://arxiv.org/abs/1505.04597" target="_blank">U-Net</a></strong></p>

<table>
  <tbody>
    <tr>
      <td>MICCAI 2015</td>
      <td>U-Net: Convolutional Networks for Biomedical Image Segmentation</td>
      <td><a href="https://arxiv.org/abs/1505.04597" target="_blank">Arxiv</a></td>
    </tr>
  </tbody>
</table>

<p><br></p>

<blockquote>
  <p>The architecture consists of a contracting path to capture context 
and a symmetric expanding path that enables precise localization. We 
show that such a network can be trained end-to-end from very few images 
and outperforms the prior best method (a sliding-window convolutional 
network) on the ISBI challenge for segmentation of neuronal structures 
in electron microscopic stacks. Using the same network trained on 
transmitted light microscopy images (phase contrast and DIC) we won the 
ISBI cell tracking challenge 2015 in these categories by a large margin.
 Moreover, the network is fast. Segmentation of a 512x512 image takes 
less than a second on a recent GPU</p>
</blockquote>

<p><br></p>
<p align="center">
<span>
<img src="http://meetshah1995.github.io/images/blog/ss/unet.png" alt="arch_unet" width="580px">
</span> &nbsp;&nbsp;&nbsp;
<br>
<small><b>Figure : </b> The U-Net Architecture</small>
</p>

<ul>
  <li>U-Net simply <em>concatenates</em> the <strong>encoder</strong> feature maps to upsampled feature maps from the <strong>decoder</strong> at every stage to form a ladder like structure. The network quite resembles <a href="https://arxiv.org/abs/1507.02672" target="_blank">Ladder Networks</a> type architecture.</li>
  <li>The architecture by its skip <code class="highlighter-rouge">concatenation</code> connections allows the decoder at each stage to learn back relevant features that are lost when pooled in the encoder.</li>
</ul>

<p>U-Net achieved state-of-art results on EM Stacks dataset which 
contained only 30 densely annoted medical images and other medical image
 datasets and was later extended to a 3D version <a href="https://arxiv.org/abs/1606.06650" target="_blank">3D-U-Net</a>.
 While U-Net was initally published for bio-medical segmentation, the 
utility of the network and its capacity to learn from very little data, 
it has found use in several other fields <a href="https://www.kaggle.com/c/dstl-satellite-imagery-feature-detection" target="_blank">satellite image segmentation</a> and also has been part of winning solutions of many <a href="https://www.kaggle.com/c/ultrasound-nerve-segmentation" target="_blank">kaggle contests</a> on medical image segmentation.</p>

<hr>
<p><br></p>

<p><a name="sec_fcdensenet"></a>
<strong><a href="https://arxiv.org/abs/1611.09326" target="_blank">Fully Convolutional DenseNet</a></strong></p>

<table>
  <tbody>
    <tr>
      <td>2016</td>
      <td>The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation</td>
      <td><a href="https://arxiv.org/abs/1611.09326" target="_blank">Arxiv</a></td>
    </tr>
  </tbody>
</table>

<p><br></p>

<blockquote>
  <p>In this paper, we extend DenseNets to deal with the problem of 
semantic segmentation. We achieve state-of-the-art results on urban 
scene benchmark datasets such as CamVid and Gatech, without any further 
post-processing module nor pretraining. Moreover, due to smart 
construction of the model, our approach has much less parameters than 
currently published best entries for these datasets.</p>
</blockquote>

<p><br></p>
<p align="center">
<span>
<img src="http://meetshah1995.github.io/images/blog/ss/densenet.png" alt="arch_fcnet" width="350px">
</span> &nbsp;&nbsp;&nbsp;
<br>
<small><b>Figure : </b> The Fully Convolutional DenseNet Architecture</small>
</p>

<p>Fully Convolutional DenseNet uses a <a href="https://arxiv.org/abs/1608.06993" target="_blank">DenseNet</a> as it’s base encoder and also in a fashion similar to U-Net concatenates features from encoder and decoder at each rung.</p>

<hr>
<p><br></p>

<p><a name="sec_elinknet"></a></p>

<p><strong><a href="https://arxiv.org/abs/1606.02147" target="_blank">E-Net and Link-Net</a></strong></p>

<table>
  <tbody>
    <tr>
      <td>2016</td>
      <td>ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation</td>
      <td><a href="https://arxiv.org/abs/1606.02147" target="_blank">Arxiv</a></td>
    </tr>
    <tr>
      <td>2017</td>
      <td>LinkNet: Feature Forwarding: Exploiting Encoder Representations for Efficient Semantic Segmentation</td>
      <td><a href="https://codeac29.github.io/projects/linknet/" target="_blank">Blog</a></td>
    </tr>
  </tbody>
</table>

<p><br></p>

<blockquote>
  <p>In this paper, we propose
a novel deep neural network architecture named ENet (efficient neural network),
created specifically for tasks requiring low latency operation. ENet is up to 18×
faster, requires 75× less FLOPs, has 79× less parameters, and provides similar or
better accuracy to existing models. We have tested it on CamVid, Cityscapes and
SUN datasets and report on comparisons with existing state-of-the-art methods,
and the trade-offs between accuracy and processing time of a network</p>
</blockquote>

<blockquote>
  <p>LinkNet can process an input image of resolution 1280x720 on TX1 and Titan X at a rate of 2 fps and 19 fps respectively</p>
</blockquote>

<p><br></p>
<p align="center">
<span>
<img src="http://meetshah1995.github.io/images/blog/ss/linknet.png" alt="arch_linknet" width="350px">
</span> &nbsp;&nbsp;&nbsp;
<span>
<img src="http://meetshah1995.github.io/images/blog/ss/linknet_1.png" alt="arch_linknet_" width="350px">
</span> &nbsp;&nbsp;&nbsp;
<br>
<small><b>Left : </b> The LinkNet Architecture <b>Right : </b> The encoder and decoder blocks used in LinkNet</small>
</p>

<p>The LinkNet Architecture resembles a <a href="https://arxiv.org/abs/1507.02672" target="_blank">ladder network</a> architecture where feature maps from the encoder (<em>laterals</em>) are <code class="highlighter-rouge">summed</code> with the upsampled feature maps from the decoder (<em>verticals</em>).
 Also note that the decoder block consists of considerable less 
parameters due to it’s channel reduction scheme. A feature map with 
shape <code class="highlighter-rouge">[H, W, n_channels]</code> is first convolved with a <code class="highlighter-rouge">1*1</code> kernel to get a feature map with shape  <code class="highlighter-rouge">[H, W, n_channels / 4 ]</code> and then a deconvolution takes it to <code class="highlighter-rouge">[2*H, 2*W, n_channels / 4 ]</code>  a final <code class="highlighter-rouge">1*1</code> kernel convolution to take it to <code class="highlighter-rouge">[2*H, 2*W, n_channels / 2 ]</code>.
 Thus the decoder block fewer parameters due to this channel reduction 
scheme. These networks, while being considerably close to state-the-art 
accuracy, can perform segmentation in real-time on embedded GPUs.</p>

<hr>
<p><br></p>

<p><a name="sec_maskrcnn"></a></p>

<p><strong><a href="https://arxiv.org/abs/1703.06870" target="_blank">Mask R-CNN</a></strong></p>

<table>
  <tbody>
    <tr>
      <td>2017</td>
      <td>Mask R-CNN 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>
      <td><a href="https://arxiv.org/abs/1703.06870" target="_blank">Arxiv</a></td>
    </tr>
  </tbody>
</table>

<p><br></p>

<blockquote>
  <p>The method, called Mask R-CNN, extends Faster
R-CNN by adding a branch for predicting an object mask in
parallel with the existing branch for bounding box recognition.
Mask R-CNN is simple to train and adds only a small
overhead to Faster R-CNN, running at 5 fps. Moreover,
Mask R-CNN is easy to generalize to other tasks, e.g., allowing
us to estimate human poses in the same framework.
We show top results in all three tracks of the COCO suite of
challenges, including instance segmentation, bounding-box
object detection, and person keypoint detection. Without
tricks, Mask R-CNN outperforms all existing, single-model
entries on every task, including the COCO 2016 challenge
winners.</p>
</blockquote>

<p align="center">
<span>
<img src="http://meetshah1995.github.io/images/blog/ss/maskrcnn.png" alt="arch_maskrcnn" width="550px">
</span> &nbsp;&nbsp;&nbsp;
<br>
<span>
<img src="http://meetshah1995.github.io/images/blog/ss/maskrcnn_1.png" alt="arch_maskrcnn_" width="550px">
</span> &nbsp;&nbsp;&nbsp;
<br>
<small><b>Top : </b> The Mask R-CNN Segmentation Pipeline <br><b>Bottom : </b> The auxillary segmentation branch in addition to original Faster-RCNN architecture</small>
</p>

<p>The Mask R-CNN architecture is fairly simple, it an extension of 
popular Faster R-CNN architecture with requisite changes made to perform
 semantic segmentation.</p>

<p>Some Key features of this architecture are:</p>

<ul>
  <li>Faster R-CNN with an auxillary branch to perform semantic segmentation.</li>
  <li>The <strong>RoIPool</strong> operation used for attending to each instance, has been modified to <strong>RoIAlign</strong>
 which avoids spatial quantization for feature extraction since keeping 
spatial-features intact in the highest resolution possible is important 
for semantic segmentation.</li>
  <li>Mask R-CNN was combined with <a href="https://arxiv.org/pdf/1612.03144.pdf" target="_blank">Feature Pyramid Networks</a> (which performs pyramid pooling of features in a style similar to <a href="#sec_pspnet]">PSPNet</a> ) achieves state-of-the-art results on <a href="http://mscoco.org/" target="_blank">MS COCO</a> dataset.</li>
</ul>

<p>There is no working implementation of Mask R-CNN available online as 
of 01-06-2017 and it has not been benchmarked on Pascal VOC, but the 
segmentation masks as shown the paper look very close to ground truth.</p>

<hr>
<p><br></p>

<p><a name="sec_pspnet"></a></p>

<p><strong><a href="https://arxiv.org/abs/1612.01105" target="_blank">PSPNet</a></strong></p>

<table>
  <tbody>
    <tr>
      <td>CVPR 2017</td>
      <td>PSPNet: Pyramid Scene Parsing Network</td>
      <td><a href="https://arxiv.org/abs/1612.01105" target="_blank">Arxiv</a></td>
    </tr>
  </tbody>
</table>

<p><br></p>

<blockquote>
  <p>In this paper, we exploit the
capability of global context information by different-regionbased
context aggregation through our pyramid pooling
module together with the proposed pyramid scene parsing
network (PSPNet). Our global prior representation is effective
to produce good quality results on the scene parsing
task, while PSPNet provides a superior framework for pixellevel
prediction. The proposed approach achieves state-ofthe-art
performance on various datasets. It came first in ImageNet
scene parsing challenge 2016, PASCAL VOC 2012
benchmark and Cityscapes benchmark.</p>
</blockquote>

<p><br></p>
<p align="center">
<span>
<img src="http://meetshah1995.github.io/images/blog/ss/pspnet.png" alt="arch_pspnet" width="650px">
</span> &nbsp;&nbsp;&nbsp;
<br>
<span>
<img src="http://meetshah1995.github.io/images/blog/ss/pspnet_1.png" alt="arch_pspnet_" width="550px">
</span> &nbsp;&nbsp;&nbsp;
<br>
<small><b>Top : </b> The PSPNet Architecture<br><b>Bottom : </b> The Spatial Pyramid Pooling in visualized in detail using netscope</small>
</p>

<p>Some Key features of this architecture are:</p>

<ul>
  <li>PSPNet <strong>modifies the base ResNet architecture by incorporating dilated convolutions</strong> and the features, after the inital pooling, is processed at the same resolution ( <code class="highlighter-rouge">1/4th</code> of the original image input) throughout the encoder network until it reaches the spatial pooling module.</li>
  <li>Introcution of <strong>auxillary loss</strong> at intermediate layers of the ResNet to optimize learning overall learning.</li>
  <li><strong>Spatial Pyramid Pooling</strong> at the top of the modified ResNet encoder to aggregate global context.</li>
</ul>

<p><br></p>
<p align="center">
<span>
<img src="http://meetshah1995.github.io/images/blog/ss/info.png" alt="context" width="650px">
</span> &nbsp;&nbsp;&nbsp;
<br>
<small><b>Figure : </b> An illustration to showcase the importance of 
global spatial context for semantic segmentation. It shows the 
relationship between receptive field and size across layers. In this 
case, the larger and more discriminative receptive (<b>blue</b>) maybe of importance in refining the representation carried by an earlier layer (<b>orange</b>) to resolve ambiguity.</small>
</p>

<p>The PSPNet architecture <strong>is currently the state-of-the-art in <a href="https://www.cityscapes-dataset.com/" target="_blank">CityScapes</a>, <a href="http://groups.csail.mit.edu/vision/datasets/ADE20K/" target="_blank">ADE20K</a> and <a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/" target="_blank">Pascal VOC 2012</a> (without MS COCO training data unlike most other methods)</strong>. A full visualisation of the network in netscope can be found <a href="https://meetshah1995.github.io/netscope/#/gist/841186c850a7ae49fd782111ebdb736f">here</a>.</p>

<hr>
<p><br></p>

<p><a name="sec_refinenet"></a></p>

<p><strong><a href="https://arxiv.org/abs/1611.06612" target="_blank">RefineNet</a></strong></p>

<table>
  <tbody>
    <tr>
      <td>CVPR 2017</td>
      <td>RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation</td>
      <td><a href="https://arxiv.org/abs/1611.06612" target="_blank">Arxiv</a></td>
    </tr>
  </tbody>
</table>

<p><br></p>

<blockquote>
  <p>Here, we present
RefineNet, a generic multi-path refinement network that
explicitly exploits all the information available along the
down-sampling process to enable high-resolution prediction
using long-range residual connections. In this way,
the deeper layers that capture high-level semantic features
can be directly refined using fine-grained features from earlier
convolutions. The individual components of RefineNet
employ residual connections following the identity mapping
mindset, which allows for effective end-to-end training.</p>
</blockquote>

<p><br></p>
<p align="center">
<span>
<img src="http://meetshah1995.github.io/images/blog/ss/refinenet_1.png" alt="arch_refinenet" width="550px">
</span> &nbsp;&nbsp;&nbsp;
<br>
<span>
<img src="http://meetshah1995.github.io/images/blog/ss/refinenet.png" alt="arch_refinenet1" width="550px">
</span> &nbsp;&nbsp;&nbsp;
<br>
<small><b>Top : </b> The RefineNet Architecture<br><b>Bottom : </b> Building Blocks of RefineNet - Residual Conv Units, Multiresolution Fusion and Chained Residual Pooling.</small>
</p>

<p>RefineNet approaches the problem of spatial resolution reduction in 
conventional convnets in a manner very different to PSPNet (which uses 
computationally expensive dilated convolutions). The proposed 
achitecture iteratively pools features increasing resolutions using 
special  RefineNet blocks for several ranges of resolutions and finally 
produces a high resolution segmentation map.</p>

<p>Some features of this architecture are:</p>

<ul>
  <li>Uses inputs at <strong>multiple resolutions</strong>, fuses the extracted features and passes them to the next stage.</li>
  <li>Introduces <strong>Chained Residual Pooling</strong> which is able
 to capture background context from a large image region. It does
so by efficiently pooling features with multiple window sizes and fusing
 them together with residual connections and learnable weights</li>
  <li>All feature fusion is done using <code class="highlighter-rouge">sum</code> (ResNet style) to allow end-to-end training.</li>
  <li>Uses vanilla ResNet style residual layers <strong>without expensive dilated convolutions</strong></li>
</ul>

<hr>
<p><br></p>

<p><a name="sec_grfnet"></a></p>

<p><strong><a href="http://www.cs.umanitoba.ca/~ywang/papers/cvpr17.pdf" target="_blank">G-FRNet</a></strong></p>

<table>
  <tbody>
    <tr>
      <td>CVPR 2017</td>
      <td>G-FRNet: Gated Feedback Refinement Network for Dense Image Labeling</td>
      <td><a href="http://www.cs.umanitoba.ca/~ywang/papers/cvpr17.pdf" target="_blank">Arxiv</a></td>
    </tr>
  </tbody>
</table>

<p><br></p>

<blockquote>
  <p>In this paper we propose
Gated Feedback Refinement Network (G-FRNet), an end-to-end
deep learning framework for dense labeling tasks that
addresses this limitation of existing methods. Initially, GFRNet
makes a coarse prediction and then it progressively
refines the details by efficiently integrating local and global
contextual information during the refinement stages. We
introduce gate units that control the information passed forward
in order to filter out ambiguity.</p>
</blockquote>

<p><br></p>
<p align="center">
<span>
<img src="http://meetshah1995.github.io/images/blog/ss/grfnet.png" alt="arch_gfrnet" width="850px">
</span> &nbsp;&nbsp;&nbsp;
<br>
<span>
<img src="http://meetshah1995.github.io/images/blog/ss/grfnet_1.png" alt="arch_gfrnet1" width="550px">
</span> &nbsp;&nbsp;&nbsp;
<br>
<small><b>Top : </b> The G-FRNet Architecture<br><b>Bottom : </b> The Gated Refinement Unit</small>
</p>

<p>Most architectures above rely on simple feature passing from encoder to decoder using <code class="highlighter-rouge">concatenation</code>, <code class="highlighter-rouge">unpooling</code> or simple <code class="highlighter-rouge">sum</code>.
 However, The information that flows from higher resolution ( less 
discrimnative ) layers in the encoder to the corresponding upsampled 
feature maps in the decoder may or may not be of utility for 
segmentation. Gating the information flow from the encoder to the 
decoder at each stage using <strong>Gated Refinement Feedback Units</strong> can assist the decoder in resolving ambiguities and forming more relevant <code class="highlighter-rouge">gated</code> spatial context.</p>

<p>On a side note - The experiments in this paper reveal that ResNet is a
 far superior encoder base than VGG16 for semantic segmentation tasks. 
Something which I wasn’t able to find in any of the previous papers.</p>

<hr>
<p><br></p>

<h4 id="semi-supervised-semantic-segmentation">Semi-Supervised Semantic Segmentation</h4>

<p><a name="sec_decouplednet"></a></p>

<p><strong><a href="https://arxiv.org/abs/1506.04924" target="_blank">DecoupledNet</a></strong></p>

<table>
  <tbody>
    <tr>
      <td>NIPS 2015</td>
      <td>Decoupled Deep Neural Network for Semi-supervised Semantic Segmentation</td>
      <td><a href="https://arxiv.org/abs/1506.04924" target="_blank">Arxiv</a></td>
    </tr>
  </tbody>
</table>

<p><br></p>

<blockquote>
  <p>Contrary to existing approaches posing semantic segmentation as a 
single task of region-based classifi-
cation, our algorithm decouples classification and segmentation, and 
learns a separate
network for each task. In this architecture, labels associated with an 
image
are identified by classification network, and binary segmentation is 
subsequently
performed for each identified label in segmentation network. It 
facilitates to reduce search space for segmentation effectively by 
exploiting class-specific activation maps obtained from bridging layers.</p>
</blockquote>

<p><br></p>
<p align="center">
<span>
<img src="http://meetshah1995.github.io/images/blog/ss/decouplednet.png" alt="arch_decouplednet" width="850px">
</span> &nbsp;&nbsp;&nbsp;
<br>
<small><b>Figure : </b> The DecoupledNet Architecture</small>
</p>

<p>This was perhaps the first semi-supervised approach for semantic segmentation using fully convolutional networks.</p>

<p>Some sailent features of this approach are:</p>

<ul>
  <li><strong>Decouples the classification and the segmentation tasks</strong>, thus enabling pre-trained classification networks to be plugged and played.</li>
  <li><strong>Bridge Layers</strong> between the classification and segmentation networks produces class-sailent feature map (for class <code class="highlighter-rouge">k</code>) which are then used by the segmentation network to produce a binary segmentation map (for class <code class="highlighter-rouge">k</code>)</li>
  <li>This method however <strong>needs k passes to segment k classes</strong> in an image.</li>
</ul>

<hr>
<p><br></p>

<p><a name="sec_gan"></a></p>

<p><strong><a href="https://arxiv.org/abs/1703.09695" target="_blank">GAN Based Approaches</a></strong></p>

<table>
  <tbody>
    <tr>
      <td>2017</td>
      <td>Semi and Weakly Supervised Semantic Segmentation Using Generative Adversarial Network</td>
      <td><a href="https://arxiv.org/abs/1703.09695" target="_blank">Arxiv</a></td>
    </tr>
  </tbody>
</table>

<p><br></p>

<blockquote>
  <p>In particular, we propose a semi-supervised framework ,based on 
Generative Adversarial Networks (GANs), which consists of a generator 
network to provide extra training examples to a multi-class classifier, 
acting as discriminator in the GAN framework, that assigns sample a 
label y from the K possible classes or marks it as a fake sample (extra 
class). To ensure higher quality of generated images for GANs with 
consequent improved pixel classification, we extend the above framework 
by adding weakly annotated data, i.e., we provide class level 
information to the generator.</p>
</blockquote>

<p><br></p>

<p align="center">
<span>
<img src="http://meetshah1995.github.io/images/blog/ss/gan.png" alt="arch_gan" width="550px">
</span> &nbsp;&nbsp;&nbsp;
<br>
<small><b>Figure : </b> Weekly Supervised (Class level labels) GAN</small>
</p>

<p align="center">
<span>
<img src="http://meetshah1995.github.io/images/blog/ss/gan_1.png" alt="arch_gan_1" width="550px">
</span> &nbsp;&nbsp;&nbsp;
<br>
<small><b>Figure : </b> Semi-Supervised GAN</small>
</p>

<hr>
<p><br></p>

<p><a name="sec_datasets"></a></p>
<h4 id="datasets">Datasets</h4>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Dataset</th>
      <th style="text-align: left">Training</th>
      <th style="text-align: left">Testing</th>
      <th style="text-align: left">#Classes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"><a href="http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/" target="_blank">CamVid</a></td>
      <td style="text-align: left">468</td>
      <td style="text-align: left">233</td>
      <td style="text-align: left">11</td>
    </tr>
    <tr>
      <td style="text-align: left"><a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/" target="_blank">PascalVOC 2012</a></td>
      <td style="text-align: left">9963</td>
      <td style="text-align: left">1447</td>
      <td style="text-align: left">20</td>
    </tr>
    <tr>
      <td style="text-align: left"><a href="http://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html" target="_blank">NYUDv2</a></td>
      <td style="text-align: left">795</td>
      <td style="text-align: left">645</td>
      <td style="text-align: left">40</td>
    </tr>
    <tr>
      <td style="text-align: left"><a href="https://www.cityscapes-dataset.com/" target="_blank">Cityscapes</a></td>
      <td style="text-align: left">2975</td>
      <td style="text-align: left">500</td>
      <td style="text-align: left">19</td>
    </tr>
    <tr>
      <td style="text-align: left"><a href="http://rgbd.cs.princeton.edu/" target="_blank">Sun-RGBD</a></td>
      <td style="text-align: left">10355</td>
      <td style="text-align: left">2860</td>
      <td style="text-align: left">37</td>
    </tr>
    <tr>
      <td style="text-align: left"><a href="http://mscoco.org/" target="_blank">MS COCO ‘15</a></td>
      <td style="text-align: left">80000</td>
      <td style="text-align: left">40000</td>
      <td style="text-align: left">80</td>
    </tr>
    <tr>
      <td style="text-align: left"><a href="http://groups.csail.mit.edu/vision/datasets/ADE20K/" target="_blank">ADE20K</a></td>
      <td style="text-align: left">20210</td>
      <td style="text-align: left">2000</td>
      <td style="text-align: left">150</td>
    </tr>
  </tbody>
</table>

<p><br></p>

<hr>

<p><a name="sec_results"></a></p>
<h4 id="results">Results</h4>

<p align="center">
<span>
<img src="http://meetshah1995.github.io/images/blog/ss/i_0.jpg" alt="conv_1_1_gradient" width="20%">
</span> &nbsp;&nbsp;&nbsp;
<span>
<img src="http://meetshah1995.github.io/images/blog/ss/ss0.png" alt="conv_4_1_gradient" width="20%">
</span> &nbsp;&nbsp;&nbsp;
<span>
<img src="http://meetshah1995.github.io/images/blog/ss/i_1.jpg" alt="conv_4_2_gradient" width="20%">
</span> &nbsp;&nbsp;&nbsp;
<span>
<img src="http://meetshah1995.github.io/images/blog/ss/ss1.png" alt="conv_4_3_gradient" width="20%">
</span> &nbsp;&nbsp;&nbsp;
<br>

<span>
<img src="http://meetshah1995.github.io/images/blog/ss/i_2.jpg" alt="conv_1_1_gradient" width="20%">
</span> &nbsp;&nbsp;&nbsp;
<span>
<img src="http://meetshah1995.github.io/images/blog/ss/ss2.png" alt="conv_4_1_gradient" width="20%">
</span> &nbsp;&nbsp;&nbsp;
<span>
<img src="http://meetshah1995.github.io/images/blog/ss/i_3.jpg" alt="conv_4_2_gradient" width="20%">
</span> &nbsp;&nbsp;&nbsp;
<span>
<img src="http://meetshah1995.github.io/images/blog/ss/ss3.png" alt="conv_4_3_gradient" width="20%">
</span> &nbsp;&nbsp;&nbsp;
<br>

<span>
<img src="http://meetshah1995.github.io/images/blog/ss/i_4.jpg" alt="conv_1_1_gradient" width="20%">
</span> &nbsp;&nbsp;&nbsp;
<span>
<img src="http://meetshah1995.github.io/images/blog/ss/ss4.png" alt="conv_4_1_gradient" width="20%">
</span> &nbsp;&nbsp;&nbsp;
<span>
<img src="http://meetshah1995.github.io/images/blog/ss/i_5.jpg" alt="conv_4_2_gradient" width="20%">
</span> &nbsp;&nbsp;&nbsp;
<span>
<img src="http://meetshah1995.github.io/images/blog/ss/ss5.png" alt="conv_4_3_gradient" width="20%">
</span> &nbsp;&nbsp;&nbsp;
<br>

<small><b>Figure : </b>Sample semantic segmentation maps generated by FCN-8s ( trained using <a href="https://github.com/meetshah1995/pytorch-semseg" target="_blank">pytorch-semseg</a> repository) overlayed on the input images from Pascal VOC validation set</small>
</p>

<p><br></p>
<p align="center">
<iframe class="juxtapose" src="Semantic%20Segmentation%20using%20Fully%20Convolutional%20Networks%20over%20the%20years_files/index_002.html" width="50%" height="274" frameborder="0"></iframe>
<br>
<small><b>Figure:</b> The boat and myself segmented, Alongside Neva River</small>
</p>

<hr>

<p><br></p>

<h4 id="debugging">Debugging</h4>

<p>In case this doesn’t work for you, or if there is a mistake/typo, open up an issue in the repo or feel free to shoot a mail at <a href="mailto:meetshah1995@gmail.com?subject=Semantic Segmentation Blog Post&amp;body=Meet,%0D" target="_blank">meetshah1995@ee.iitb.ac.in</a></p>

<script type="text/javascript">

var links = document.links;

for (var i = 0, linksLength = links.length; i < linksLength; i++) {
   if (links[i].hostname != window.location.hostname) {
       links[i].target = '_blank';
   } 
}

</script>


  </div>

</article>
      </div>
    </div>

      <footer class="site-footer">

    <div class="wrapper">

      <h2 class="footer-heading">int const change;</h2>

      <div class="footer-col-wrapper">
        <div class="footer-col footer-col-1">
          <ul class="contact-list">
            <li>int const change;</li>
            <li><a href="mailto:meetshah1995@gmail.com">meetshah1995@gmail.com</a></li>
          </ul>
        </div>

        <div class="footer-col footer-col-2">
          <ul class="social-media-list">
            
            <li>
              <a href="https://github.com/meetshah1995"><span class="icon icon--github"><svg viewBox="0 0 16 16"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"></path></svg>
</span><span class="username">meetshah1995</span></a>

            </li>
            
          </ul>
        </div>

        <div class="footer-col footer-col-3">
          <p>Musings by Me(et)
</p>
        </div>
      </div>

    </div>

  </footer>


  


</body></html>